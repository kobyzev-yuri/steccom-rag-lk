#!/usr/bin/env python3
"""
Comprehensive Knowledge Base Effectiveness Tests
–¢–µ—Å—Ç—ã —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π –¥–ª—è —Å–∏—Å—Ç–µ–º—ã –°–¢–≠–ö–ö–û–ú

–≠—Ç–æ—Ç –º–æ–¥—É–ª—å —Å–æ–¥–µ—Ä–∂–∏—Ç —Ç–µ—Å—Ç—ã –¥–ª—è –æ—Ü–µ–Ω–∫–∏:
- –¢–æ—á–Ω–æ—Å—Ç–∏ –æ—Ç–≤–µ—Ç–æ–≤ KB
- –ü–æ–ª–Ω–æ—Ç—ã –ø–æ–∫—Ä—ã—Ç–∏—è —Ç–µ–º
- –†–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ–∏—Å–∫–∞
- –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —Å–∏—Å—Ç–µ–º—ã
- –ö–∞—á–µ—Å—Ç–≤–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤
"""

import os
import sys
import time
import pytest
import json
from pathlib import Path
from typing import List, Dict, Any, Tuple
from dataclasses import dataclass
from datetime import datetime

# Add project root to path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from modules.rag.multi_kb_rag import MultiKBRAG
from kb_test_protocol import KBTestProtocol, TestQuestion, ModelResponse, RelevanceAssessment, LEGACY_SBD_TEST_QUESTIONS


@dataclass
class KBTestResult:
    """–†–µ–∑—É–ª—å—Ç–∞—Ç —Ç–µ—Å—Ç–∞ KB"""
    question: str
    expected_keywords: List[str]
    actual_answer: str
    response_time: float
    sources_found: int
    accuracy_score: float
    completeness_score: float
    relevance_score: float
    model_used: str
    timestamp: str


class KBEffectivenessTester:
    """–¢–µ—Å—Ç–µ—Ä —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ Knowledge Base"""
    
    def __init__(self):
        self.rag = MultiKBRAG()
        self.test_results: List[KBTestResult] = []
        self.protocol = KBTestProtocol()
        
        # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ —Ç–µ—Å—Ç–æ–≤—ã–µ –≤–æ–ø—Ä–æ—Å—ã —Å –æ–∂–∏–¥–∞–µ–º—ã–º–∏ –∫–ª—é—á–µ–≤—ã–º–∏ —Å–ª–æ–≤–∞–º–∏
        self.test_questions = {
            "billing": [
                {
                    "question": "–ö–∞–∫ —Ä–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç—Å—è –≤–∫–ª—é—á–µ–Ω–Ω—ã–π —Ç—Ä–∞—Ñ–∏–∫ –ø—Ä–∏ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–∏ –≤ —Å–µ—Ä–µ–¥–∏–Ω–µ –º–µ—Å—è—Ü–∞?",
                    "expected_keywords": ["–ø—Ä–æ–ø–æ—Ä—Ü–∏–æ–Ω–∞–ª—å–Ω–æ", "–∞–∫—Ç–∏–≤–Ω—ã—Ö –¥–Ω–µ–π", "—Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–π –ø–µ—Ä–∏–æ–¥", "—Ä–∞—Å—á–µ—Ç"],
                    "category": "billing_calculation"
                },
                {
                    "question": "–ö–∞–∫–∏–µ –ø—Ä–∞–≤–∏–ª–∞ –¥–µ–π—Å—Ç–≤—É—é—Ç –ø—Ä–∏ –¥–µ–∞–∫—Ç–∏–≤–∞—Ü–∏–∏ —Ç–µ—Ä–º–∏–Ω–∞–ª–∞ –≤ —Ç–µ—á–µ–Ω–∏–µ –º–µ—Å—è—Ü–∞?",
                    "expected_keywords": ["–¥–µ–∞–∫—Ç–∏–≤–∞—Ü", "–º–µ—Å—è—Ü", "–ø–ª–∞—Ç–∞", "–ø—Ä–∞–≤–∏–ª–∞"],
                    "category": "deactivation_rules"
                },
                {
                    "question": "–ö–∞–∫ —Ç–∞—Ä–∏—Ñ–∏—Ü–∏—Ä—É–µ—Ç—Å—è —Ç—Ä–∞—Ñ–∏–∫ —ç–ª–µ–∫—Ç—Ä–æ–Ω–Ω—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π –∏ –∫–æ–≥–¥–∞ –æ–Ω –ø–æ–ø–∞–¥–∞–µ—Ç –≤ —Å—á–µ—Ç?",
                    "expected_keywords": ["—Ç–∞—Ä–∏—Ñ–∏—Ü–∏—Ä", "—Å–ª–µ–¥—É—é—â–∏–π –¥–µ–Ω—å", "—Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–π –ø–µ—Ä–∏–æ–¥", "—ç–ª–µ–∫—Ç—Ä–æ–Ω–Ω—ã–µ —Å–æ–æ–±—â–µ–Ω–∏—è"],
                    "category": "traffic_tarification"
                },
                {
                    "question": "–°–∫–æ–ª—å–∫–æ –±–∞–π—Ç –≤ –æ–¥–Ω–æ–º –∫–∏–ª–æ–±–∞–π—Ç–µ —Å–æ–≥–ª–∞—Å–Ω–æ —Ç–∞—Ä–∏—Ñ–∞–º?",
                    "expected_keywords": ["1000", "–±–∞–π—Ç", "–∫–∏–ª–æ–±–∞–π—Ç", "—Ç–∞—Ä–∏—Ñ"],
                    "category": "data_units"
                }
            ],
            "ui_guide": [
                {
                    "question": "–ö–∞–∫–∏–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –µ—Å—Ç—å –≤ –ª–∏—á–Ω–æ–º –∫–∞–±–∏–Ω–µ—Ç–µ?",
                    "expected_keywords": ["–∞–Ω–∞–ª–∏—Ç–∏–∫–∞", "—Ç—Ä–∞—Ñ–∏–∫", "–¥–æ–≥–æ–≤–æ—Ä—ã", "–º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥", "—É—Å—Ç—Ä–æ–π—Å—Ç–≤"],
                    "category": "ui_capabilities"
                },
                {
                    "question": "–ö–∞–∫ —Ä–∞–±–æ—Ç–∞—Ç—å —Å –æ—Ç—á–µ—Ç–∞–º–∏ –≤ —Å–∏—Å—Ç–µ–º–µ?",
                    "expected_keywords": ["–æ—Ç—á–µ—Ç—ã", "—Å–ø–∏—Å–æ–∫", "–ø–æ–∫–∞–∑–∞—Ç—å", "csv", "—Å–∫–∞—á–∞—Ç—å"],
                    "category": "reports_usage"
                },
                {
                    "question": "–ß—Ç–æ —Ç–∞–∫–æ–µ SQL-–∞–≥–µ–Ω—Ç –∏ –∫–∞–∫ –∏–º –ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è?",
                    "expected_keywords": ["sql", "–∞–≥–µ–Ω—Ç", "–≥–µ–Ω–µ—Ä–∞—Ü–∏—è", "–∑–∞–ø—Ä–æ—Å", "–≤–æ–ø—Ä–æ—Å"],
                    "category": "sql_agent"
                }
            ],
            "technical": [
                {
                    "question": "–ö–∞–∫–∏–µ —Ç–∞–±–ª–∏—Ü—ã –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –≤ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö?",
                    "expected_keywords": ["—Ç–∞–±–ª–∏—Ü—ã", "users", "devices", "sessions", "billing_records"],
                    "category": "database_structure"
                },
                {
                    "question": "–ö–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç —É–º–Ω—ã–π –ø–æ–º–æ—â–Ω–∏–∫ RAG?",
                    "expected_keywords": ["rag", "–ø–æ–º–æ—â–Ω–∏–∫", "–¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è", "kb", "–æ—Ç–≤–µ—á–∞–µ—Ç"],
                    "category": "rag_system"
                }
            ]
        }
    
    def run_comprehensive_effectiveness_test(self, model_configs: List[Dict[str, Any]] = None) -> Dict[str, Any]:
        """–ó–∞–ø—É—Å–∫ –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–≥–æ —Ç–µ—Å—Ç–∞ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ KB"""
        if model_configs is None:
            model_configs = [
                {"provider": "proxyapi", "model": "gpt-4o", "temperature": 0.0},
                {"provider": "ollama", "model": "qwen2.5:1.5b", "temperature": 0.0}
            ]
        
        print("üöÄ –ó–∞–ø—É—Å–∫ –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–≥–æ —Ç–µ—Å—Ç–∞ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ Knowledge Base...")
        print(f"üìä –¢–µ—Å—Ç–∏—Ä—É–µ–º {len(model_configs)} –º–æ–¥–µ–ª–µ–π")
        print(f"üìù –í—Å–µ–≥–æ –≤–æ–ø—Ä–æ—Å–æ–≤: {sum(len(questions) for questions in self.test_questions.values())}")
        
        all_results = {}
        
        for config in model_configs:
            print(f"\nü§ñ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏: {config['model']} ({config['provider']})")
            
            # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –º–æ–¥–µ–ª–∏
            self.rag.set_chat_backend(
                provider=config["provider"],
                model=config["model"],
                temperature=config["temperature"]
            )
            
            model_results = self._test_model_effectiveness(config)
            all_results[f"{config['provider']}_{config['model']}"] = model_results
        
        # –ê–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        analysis = self._analyze_results(all_results)
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        self._save_test_results(all_results, analysis)
        
        return {
            "model_results": all_results,
            "analysis": analysis,
            "summary": self._generate_summary(analysis)
        }
    
    def run_protocol_test(self, model_configs: List[Dict[str, Any]] = None) -> str:
        """–ó–∞–ø—É—Å–∫ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –≤ –ø—Ä–æ—Ç–æ–∫–æ–ª"""
        if model_configs is None:
            model_configs = [
                {"provider": "proxyapi", "model": "gpt-4o", "temperature": 0.0},
                {"provider": "ollama", "model": "qwen2.5:1.5b", "temperature": 0.0}
            ]
        
        print("üìã –ó–∞–ø—É—Å–∫ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è —Å –ø—Ä–æ—Ç–æ–∫–æ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ–º...")
        
        for config in model_configs:
            print(f"\nü§ñ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏: {config['model']} ({config['provider']})")
            
            # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –º–æ–¥–µ–ª–∏
            self.rag.set_chat_backend(
                provider=config["provider"],
                model=config["model"],
                temperature=config["temperature"]
            )
            
            # –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–∞–∂–¥–æ–≥–æ –≤–æ–ø—Ä–æ—Å–∞ –∏–∑ –ø—Ä–æ—Ç–æ–∫–æ–ª–∞
            for question in LEGACY_SBD_TEST_QUESTIONS:
                print(f"  ‚ùì {question.question_text[:50]}...")
                
                # –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ —Ç–µ—Å—Ç–∞
                start_time = time.time()
                try:
                    result = self.rag.ask_question(question.question_text)
                    response_time = time.time() - start_time
                    
                    answer = result.get("answer", "")
                    sources = result.get("sources", [])
                    
                    # –°–æ–∑–¥–∞–Ω–∏–µ –æ—Ç–≤–µ—Ç–∞ –º–æ–¥–µ–ª–∏
                    model_response = ModelResponse(
                        model_name=config["model"],
                        provider=config["provider"],
                        response_text=answer,
                        response_time=response_time,
                        sources_found=len(sources),
                        sources_details=[{"source": "legacy_reglament_sbd.json", "relevance": 0.8}],
                        timestamp=datetime.now().isoformat(),
                        # –ú–µ—Ç—Ä–∏–∫–∏ —Ç–æ–∫–µ–Ω–æ–≤ –±—É–¥—É—Ç —Ä–∞—Å—Å—á–∏—Ç–∞–Ω—ã –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏
                        input_tokens=0,
                        output_tokens=0,
                        total_tokens=0,
                        tokens_per_second=0.0,
                        cost_per_token=0.0,
                        estimated_cost=0.0
                    )
                    
                    # –û—Ü–µ–Ω–∫–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
                    accuracy_score = self._calculate_accuracy_score(answer, question.expected_keywords)
                    completeness_score = self._calculate_completeness_score(answer, question.expected_keywords)
                    relevance_score = self._calculate_relevance_score(answer, question.question_text)
                    
                    keywords_found = [kw for kw in question.expected_keywords if kw.lower() in answer.lower()]
                    keywords_missing = [kw for kw in question.expected_keywords if kw.lower() not in answer.lower()]
                    
                    # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –æ–±—â–µ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞
                    avg_score = (accuracy_score + completeness_score + relevance_score) / 3
                    if avg_score >= 0.8:
                        overall_quality = "excellent"
                    elif avg_score >= 0.6:
                        overall_quality = "good"
                    elif avg_score >= 0.4:
                        overall_quality = "fair"
                    else:
                        overall_quality = "poor"
                    
                    relevance_assessment = RelevanceAssessment(
                        accuracy_score=accuracy_score,
                        completeness_score=completeness_score,
                        relevance_score=relevance_score,
                        keywords_found=keywords_found,
                        keywords_missing=keywords_missing,
                        overall_quality=overall_quality,
                        human_notes=f"–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –æ—Ü–µ–Ω–∫–∞ –¥–ª—è {config['model']}",
                        # –ú–µ—Ç—Ä–∏–∫–∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –±—É–¥—É—Ç —Ä–∞—Å—Å—á–∏—Ç–∞–Ω—ã –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏
                        efficiency_score=0.0,
                        cost_effectiveness=0.0
                    )
                    
                    # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –≤ –ø—Ä–æ—Ç–æ–∫–æ–ª
                    test_id = self.protocol.add_test_entry(question, model_response, relevance_assessment)
                    print(f"    ‚úÖ –¢–µ—Å—Ç {test_id} –¥–æ–±–∞–≤–ª–µ–Ω –≤ –ø—Ä–æ—Ç–æ–∫–æ–ª")
                    
                except Exception as e:
                    print(f"    ‚ùå –û—à–∏–±–∫–∞: {e}")
                    # –î–æ–±–∞–≤–ª—è–µ–º –∑–∞–ø–∏—Å—å –æ–± –æ—à–∏–±–∫–µ
                    error_response = ModelResponse(
                        model_name=config["model"],
                        provider=config["provider"],
                        response_text=f"–û—à–∏–±–∫–∞: {str(e)}",
                        response_time=time.time() - start_time,
                        sources_found=0,
                        sources_details=[],
                        timestamp=datetime.now().isoformat(),
                        # –ú–µ—Ç—Ä–∏–∫–∏ —Ç–æ–∫–µ–Ω–æ–≤ –±—É–¥—É—Ç —Ä–∞—Å—Å—á–∏—Ç–∞–Ω—ã –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏
                        input_tokens=0,
                        output_tokens=0,
                        total_tokens=0,
                        tokens_per_second=0.0,
                        cost_per_token=0.0,
                        estimated_cost=0.0
                    )
                    
                    error_assessment = RelevanceAssessment(
                        accuracy_score=0.0,
                        completeness_score=0.0,
                        relevance_score=0.0,
                        keywords_found=[],
                        keywords_missing=question.expected_keywords,
                        overall_quality="poor",
                        human_notes=f"–û—à–∏–±–∫–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è: {str(e)}",
                        # –ú–µ—Ç—Ä–∏–∫–∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –±—É–¥—É—Ç —Ä–∞—Å—Å—á–∏—Ç–∞–Ω—ã –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏
                        efficiency_score=0.0,
                        cost_effectiveness=0.0
                    )
                    
                    self.protocol.add_test_entry(question, error_response, error_assessment)
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ø—Ä–æ—Ç–æ–∫–æ–ª–∞
        protocol_file = self.protocol.save_protocol()
        print(f"\nüíæ –ü—Ä–æ—Ç–æ–∫–æ–ª —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {protocol_file}")
        
        # –í—ã–≤–æ–¥ —Å–≤–æ–¥–∫–∏
        print(self.protocol.get_test_summary())
        
        # –≠–∫—Å–ø–æ—Ä—Ç –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –æ—Ç—á–µ—Ç–∞
        report_file = self.protocol.export_detailed_report()
        print(f"üìä –î–µ—Ç–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç: {report_file}")
        
        return protocol_file
    
    def _test_model_effectiveness(self, config: Dict[str, Any]) -> Dict[str, Any]:
        """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –º–æ–¥–µ–ª–∏"""
        model_results = {
            "config": config,
            "category_results": {},
            "overall_metrics": {},
            "test_results": []
        }
        
        total_questions = 0
        total_accuracy = 0
        total_completeness = 0
        total_relevance = 0
        total_response_time = 0
        
        for category, questions in self.test_questions.items():
            print(f"  üìÇ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏: {category}")
            category_results = []
            
            for question_data in questions:
                question = question_data["question"]
                expected_keywords = question_data["expected_keywords"]
                question_category = question_data["category"]
                
                print(f"    ‚ùì {question[:50]}...")
                
                # –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ —Ç–µ—Å—Ç–∞
                result = self._test_single_question(
                    question, expected_keywords, config["model"]
                )
                
                category_results.append(result)
                model_results["test_results"].append(result)
                
                # –ù–∞–∫–æ–ø–ª–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫
                total_questions += 1
                total_accuracy += result.accuracy_score
                total_completeness += result.completeness_score
                total_relevance += result.relevance_score
                total_response_time += result.response_time
        
        # –†–∞—Å—á–µ—Ç –æ–±—â–∏—Ö –º–µ—Ç—Ä–∏–∫
        model_results["overall_metrics"] = {
            "total_questions": total_questions,
            "average_accuracy": total_accuracy / total_questions if total_questions > 0 else 0,
            "average_completeness": total_completeness / total_questions if total_questions > 0 else 0,
            "average_relevance": total_relevance / total_questions if total_questions > 0 else 0,
            "average_response_time": total_response_time / total_questions if total_questions > 0 else 0,
            "total_response_time": total_response_time
        }
        
        return model_results
    
    def _test_single_question(self, question: str, expected_keywords: List[str], model_name: str) -> KBTestResult:
        """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –æ–¥–Ω–æ–≥–æ –≤–æ–ø—Ä–æ—Å–∞"""
        start_time = time.time()
        
        try:
            result = self.rag.ask_question(question)
            response_time = time.time() - start_time
            
            answer = result.get("answer", "")
            sources = result.get("sources", [])
            
            # –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –æ—Ç–≤–µ—Ç–∞
            accuracy_score = self._calculate_accuracy_score(answer, expected_keywords)
            completeness_score = self._calculate_completeness_score(answer, expected_keywords)
            relevance_score = self._calculate_relevance_score(answer, question)
            
            return KBTestResult(
                question=question,
                expected_keywords=expected_keywords,
                actual_answer=answer,
                response_time=response_time,
                sources_found=len(sources),
                accuracy_score=accuracy_score,
                completeness_score=completeness_score,
                relevance_score=relevance_score,
                model_used=model_name,
                timestamp=datetime.now().isoformat()
            )
            
        except Exception as e:
            print(f"    ‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–∏: {e}")
            return KBTestResult(
                question=question,
                expected_keywords=expected_keywords,
                actual_answer=f"–û—à–∏–±–∫–∞: {str(e)}",
                response_time=time.time() - start_time,
                sources_found=0,
                accuracy_score=0.0,
                completeness_score=0.0,
                relevance_score=0.0,
                model_used=model_name,
                timestamp=datetime.now().isoformat()
            )
    
    def _calculate_accuracy_score(self, answer: str, expected_keywords: List[str]) -> float:
        """–†–∞—Å—á–µ—Ç –æ—Ü–µ–Ω–∫–∏ —Ç–æ—á–Ω–æ—Å—Ç–∏ (–Ω–∞–ª–∏—á–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤)"""
        if not answer:
            return 0.0
        
        answer_lower = answer.lower()
        found_keywords = sum(1 for keyword in expected_keywords if keyword.lower() in answer_lower)
        return found_keywords / len(expected_keywords)
    
    def _calculate_completeness_score(self, answer: str, expected_keywords: List[str]) -> float:
        """–†–∞—Å—á–µ—Ç –æ—Ü–µ–Ω–∫–∏ –ø–æ–ª–Ω–æ—Ç—ã –æ—Ç–≤–µ—Ç–∞"""
        if not answer:
            return 0.0
        
        # –ë–∞–∑–æ–≤–∞—è –æ—Ü–µ–Ω–∫–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–ª–∏–Ω—ã –æ—Ç–≤–µ—Ç–∞
        length_score = min(len(answer) / 200, 1.0)  # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∫ 200 —Å–∏–º–≤–æ–ª–∞–º
        
        # –ë–æ–Ω—É—Å –∑–∞ –Ω–∞–ª–∏—á–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
        keyword_score = self._calculate_accuracy_score(answer, expected_keywords)
        
        # –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –æ—Ü–µ–Ω–∫–∞
        return (length_score * 0.3 + keyword_score * 0.7)
    
    def _calculate_relevance_score(self, answer: str, question: str) -> float:
        """–†–∞—Å—á–µ—Ç –æ—Ü–µ–Ω–∫–∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ –æ—Ç–≤–µ—Ç–∞"""
        if not answer:
            return 0.0
        
        # –ü—Ä–æ—Å—Ç–∞—è —ç–≤—Ä–∏—Å—Ç–∏–∫–∞: –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –Ω–∞–ª–∏—á–∏–µ –æ–±—â–∏—Ö —Å–ª–æ–≤
        question_words = set(question.lower().split())
        answer_words = set(answer.lower().split())
        
        # –ò—Å–∫–ª—é—á–∞–µ–º —Å—Ç–æ–ø-—Å–ª–æ–≤–∞
        stop_words = {"–∫–∞–∫", "—á—Ç–æ", "–≥–¥–µ", "–∫–æ–≥–¥–∞", "–ø–æ—á–µ–º—É", "–∫–æ—Ç–æ—Ä—ã–π", "–∫–∞–∫–æ–π", "–µ—Å—Ç—å", "–≤", "–Ω–∞", "—Å", "–ø–æ", "–¥–ª—è", "–æ—Ç", "–¥–æ", "–∏", "–∏–ª–∏", "–Ω–æ", "–∞", "–∂–µ", "–ª–∏", "–±—ã", "–Ω–µ", "–Ω–∏", "—É–∂–µ", "–µ—â–µ", "—Ç–æ–ª—å–∫–æ", "–¥–∞–∂–µ", "–≤—Å–µ", "–≤—Å–µ–≥–æ", "–≤—Å–µ—Ö", "–≤—Å–µ–π", "–≤—Å–µ–º—É", "–≤—Å–µ–º", "–≤—Å–µ–º–∏", "–≤—Å—é", "–≤—Å–µ—é", "–≤—Å–µ—è", "–≤—Å–µ–µ", "–≤—Å–µ–∏", "–≤—Å–µ—é", "–≤—Å–µ—è", "–≤—Å–µ–µ", "–≤—Å–µ–∏"}
        
        question_words = question_words - stop_words
        answer_words = answer_words - stop_words
        
        if not question_words:
            return 0.5  # –ù–µ–π—Ç—Ä–∞–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –µ—Å–ª–∏ –Ω–µ—Ç –∑–Ω–∞—á–∏–º—ã—Ö —Å–ª–æ–≤ –≤ –≤–æ–ø—Ä–æ—Å–µ
        
        common_words = question_words.intersection(answer_words)
        relevance = len(common_words) / len(question_words)
        
        return min(relevance, 1.0)
    
    def _analyze_results(self, all_results: Dict[str, Any]) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è"""
        analysis = {
            "model_comparison": {},
            "category_analysis": {},
            "performance_analysis": {},
            "quality_analysis": {}
        }
        
        # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π
        for model_name, results in all_results.items():
            metrics = results["overall_metrics"]
            analysis["model_comparison"][model_name] = {
                "accuracy": metrics["average_accuracy"],
                "completeness": metrics["average_completeness"],
                "relevance": metrics["average_relevance"],
                "response_time": metrics["average_response_time"],
                "total_questions": metrics["total_questions"]
            }
        
        # –ê–Ω–∞–ª–∏–∑ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
        categories = set()
        for results in all_results.values():
            for result in results["test_results"]:
                # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏—é –∏–∑ –≤–æ–ø—Ä–æ—Å–∞ (—É–ø—Ä–æ—â–µ–Ω–Ω–æ)
                if "—Ç—Ä–∞—Ñ–∏–∫" in result.question.lower():
                    categories.add("traffic")
                elif "–¥–æ–≥–æ–≤–æ—Ä" in result.question.lower():
                    categories.add("agreements")
                elif "–æ—Ç—á–µ—Ç" in result.question.lower():
                    categories.add("reports")
                else:
                    categories.add("general")
        
        for category in categories:
            analysis["category_analysis"][category] = {
                "questions_count": 0,
                "average_accuracy": 0.0,
                "average_response_time": 0.0
            }
        
        # –ê–Ω–∞–ª–∏–∑ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        response_times = []
        for results in all_results.values():
            for result in results["test_results"]:
                response_times.append(result.response_time)
        
        if response_times:
            analysis["performance_analysis"] = {
                "min_response_time": min(response_times),
                "max_response_time": max(response_times),
                "avg_response_time": sum(response_times) / len(response_times),
                "slow_queries_count": len([t for t in response_times if t > 10.0])
            }
        
        return analysis
    
    def _save_test_results(self, all_results: Dict[str, Any], analysis: Dict[str, Any]):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        results_file = f"/mnt/ai/cnn/steccom/tests/kb_effectiveness_results_{timestamp}.json"
        
        results_data = {
            "timestamp": timestamp,
            "test_results": all_results,
            "analysis": analysis,
            "summary": self._generate_summary(analysis)
        }
        
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(results_data, f, ensure_ascii=False, indent=2)
        
        print(f"üíæ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {results_file}")
    
    def _generate_summary(self, analysis: Dict[str, Any]) -> Dict[str, Any]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫—Ä–∞—Ç–∫–æ–≥–æ —Ä–µ–∑—é–º–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"""
        if not analysis.get("model_comparison"):
            return {"error": "–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞"}
        
        # –ù–∞—Ö–æ–¥–∏–º –ª—É—á—à—É—é –º–æ–¥–µ–ª—å –ø–æ –∫–∞–∂–¥–æ–º—É –∫—Ä–∏—Ç–µ—Ä–∏—é
        best_accuracy = max(analysis["model_comparison"].items(), key=lambda x: x[1]["accuracy"])
        best_speed = min(analysis["model_comparison"].items(), key=lambda x: x[1]["response_time"])
        
        return {
            "best_accuracy_model": best_accuracy[0],
            "best_accuracy_score": best_accuracy[1]["accuracy"],
            "fastest_model": best_speed[0],
            "fastest_response_time": best_speed[1]["response_time"],
            "total_models_tested": len(analysis["model_comparison"]),
            "performance_issues": analysis.get("performance_analysis", {}).get("slow_queries_count", 0)
        }


class TestKBEffectiveness:
    """–û—Å–Ω–æ–≤–Ω—ã–µ —Ç–µ—Å—Ç—ã —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ Knowledge Base"""
    
    @pytest.fixture(autouse=True)
    def setup_tester(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Ç–µ—Å—Ç–µ—Ä–∞"""
        self.tester = KBEffectivenessTester()
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ KB –∑–∞–≥—Ä—É–∂–µ–Ω—ã
        assert len(self.tester.rag.vectorstores) > 0, "No knowledge bases loaded"
    
    def test_kb_accuracy_gpt4o(self):
        """–¢–µ—Å—Ç —Ç–æ—á–Ω–æ—Å—Ç–∏ KB —Å GPT-4o"""
        config = {"provider": "proxyapi", "model": "gpt-4o", "temperature": 0.0}
        results = self.tester._test_model_effectiveness(config)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —Ç–æ—á–Ω–æ—Å—Ç—å –≤—ã—à–µ 70%
        assert results["overall_metrics"]["average_accuracy"] > 0.7, \
            f"Accuracy too low: {results['overall_metrics']['average_accuracy']:.2f}"
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –≤—Ä–µ–º—è –æ—Ç–≤–µ—Ç–∞ —Ä–∞–∑—É–º–Ω–æ–µ
        assert results["overall_metrics"]["average_response_time"] < 30.0, \
            f"Response time too slow: {results['overall_metrics']['average_response_time']:.2f}s"
    
    def test_kb_accuracy_qwen(self):
        """–¢–µ—Å—Ç —Ç–æ—á–Ω–æ—Å—Ç–∏ KB —Å Qwen"""
        config = {"provider": "ollama", "model": "qwen2.5:1.5b", "temperature": 0.0}
        results = self.tester._test_model_effectiveness(config)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —Ç–æ—á–Ω–æ—Å—Ç—å –≤—ã—à–µ 60% (Qwen –º–æ–∂–µ—Ç –±—ã—Ç—å –º–µ–Ω–µ–µ —Ç–æ—á–Ω—ã–º)
        assert results["overall_metrics"]["average_accuracy"] > 0.6, \
            f"Accuracy too low: {results['overall_metrics']['average_accuracy']:.2f}"
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –≤—Ä–µ–º—è –æ—Ç–≤–µ—Ç–∞ —Ä–∞–∑—É–º–Ω–æ–µ
        assert results["overall_metrics"]["average_response_time"] < 15.0, \
            f"Response time too slow: {results['overall_metrics']['average_response_time']:.2f}s"
    
    def test_kb_completeness(self):
        """–¢–µ—Å—Ç –ø–æ–ª–Ω–æ—Ç—ã –ø–æ–∫—Ä—ã—Ç–∏—è KB"""
        config = {"provider": "proxyapi", "model": "gpt-4o", "temperature": 0.0}
        results = self.tester._test_model_effectiveness(config)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –ø–æ–ª–Ω–æ—Ç–∞ –≤—ã—à–µ 60%
        assert results["overall_metrics"]["average_completeness"] > 0.6, \
            f"Completeness too low: {results['overall_metrics']['average_completeness']:.2f}"
    
    def test_kb_relevance(self):
        """–¢–µ—Å—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ –æ—Ç–≤–µ—Ç–æ–≤ KB"""
        config = {"provider": "proxyapi", "model": "gpt-4o", "temperature": 0.0}
        results = self.tester._test_model_effectiveness(config)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å –≤—ã—à–µ 50%
        assert results["overall_metrics"]["average_relevance"] > 0.5, \
            f"Relevance too low: {results['overall_metrics']['average_relevance']:.2f}"
    
    def test_kb_performance_benchmark(self):
        """–ë–µ–Ω—á–º–∞—Ä–∫ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ KB"""
        config = {"provider": "proxyapi", "model": "gpt-4o", "temperature": 0.0}
        results = self.tester._test_model_effectiveness(config)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–æ –∑–∞–ø—Ä–æ—Å–æ–≤ –≤—ã–ø–æ–ª–Ω—è—é—Ç—Å—è –±—ã—Å—Ç—Ä–æ
        slow_queries = 0
        for result in results["test_results"]:
            if result.response_time > 10.0:
                slow_queries += 1
        
        slow_percentage = slow_queries / len(results["test_results"]) * 100
        assert slow_percentage < 20, \
            f"Too many slow queries: {slow_percentage:.1f}% > 20%"
    
    def test_kb_model_comparison(self):
        """–°—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω—ã–π —Ç–µ—Å—Ç –º–æ–¥–µ–ª–µ–π"""
        configs = [
            {"provider": "proxyapi", "model": "gpt-4o", "temperature": 0.0},
            {"provider": "ollama", "model": "qwen2.5:1.5b", "temperature": 0.0}
        ]
        
        results = self.tester.run_comprehensive_effectiveness_test(configs)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –æ–±–µ –º–æ–¥–µ–ª–∏ –¥–∞–ª–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        assert len(results["model_results"]) == 2, "Should test both models"
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ GPT-4o —Ç–æ—á–Ω–µ–µ Qwen
        gpt4o_results = results["model_results"]["proxyapi_gpt-4o"]["overall_metrics"]
        qwen_results = results["model_results"]["ollama_qwen2.5:1.5b"]["overall_metrics"]
        
        assert gpt4o_results["average_accuracy"] >= qwen_results["average_accuracy"], \
            "GPT-4o should be at least as accurate as Qwen"
    
    def test_kb_coverage_analysis(self):
        """–¢–µ—Å—Ç –ø–æ–∫—Ä—ã—Ç–∏—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π –∑–Ω–∞–Ω–∏–π"""
        config = {"provider": "proxyapi", "model": "gpt-4o", "temperature": 0.0}
        results = self.tester._test_model_effectiveness(config)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –≤—Å–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –≤–æ–ø—Ä–æ—Å–æ–≤ –ø–æ–∫—Ä—ã—Ç—ã
        categories_tested = set()
        for result in results["test_results"]:
            if "—Ç—Ä–∞—Ñ–∏–∫" in result.question.lower():
                categories_tested.add("traffic")
            elif "–¥–æ–≥–æ–≤–æ—Ä" in result.question.lower():
                categories_tested.add("agreements")
            elif "–æ—Ç—á–µ—Ç" in result.question.lower():
                categories_tested.add("reports")
            else:
                categories_tested.add("general")
        
        assert len(categories_tested) >= 3, f"Not enough categories covered: {categories_tested}"
    
    def test_kb_quality_consistency(self):
        """–¢–µ—Å—Ç –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –æ—Ç–≤–µ—Ç–æ–≤"""
        config = {"provider": "proxyapi", "model": "gpt-4o", "temperature": 0.0}
        results = self.tester._test_model_effectiveness(config)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –Ω–µ—Ç –æ—Ç–≤–µ—Ç–æ–≤ —Å –Ω—É–ª–µ–≤–æ–π –æ—Ü–µ–Ω–∫–æ–π
        zero_quality_count = 0
        for result in results["test_results"]:
            if result.accuracy_score == 0.0 and result.completeness_score == 0.0:
                zero_quality_count += 1
        
        zero_percentage = zero_quality_count / len(results["test_results"]) * 100
        assert zero_percentage < 10, \
            f"Too many zero-quality responses: {zero_percentage:.1f}% > 10%"
    
    def test_kb_protocol_testing(self):
        """–¢–µ—Å—Ç –ø—Ä–æ—Ç–æ–∫–æ–ª–∏—Ä–æ–≤–∞–Ω–∏—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è KB"""
        # –ó–∞–ø—É—Å–∫ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è —Å –ø—Ä–æ—Ç–æ–∫–æ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ–º
        protocol_file = self.tester.run_protocol_test()
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –ø—Ä–æ—Ç–æ–∫–æ–ª —Å–æ–∑–¥–∞–Ω
        assert os.path.exists(protocol_file), "Protocol file should be created"
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –ø—Ä–æ—Ç–æ–∫–æ–ª–∞
        with open(protocol_file, 'r', encoding='utf-8') as f:
            protocol_data = json.load(f)
        
        assert "protocol_info" in protocol_data, "Protocol should have info section"
        assert "test_entries" in protocol_data, "Protocol should have test entries"
        assert "summary_statistics" in protocol_data, "Protocol should have summary statistics"
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –µ—Å—Ç—å –∑–∞–ø–∏—Å–∏ —Ç–µ—Å—Ç–æ–≤
        assert len(protocol_data["test_entries"]) > 0, "Should have test entries"
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É –∑–∞–ø–∏—Å–µ–π
        for entry in protocol_data["test_entries"]:
            assert "question" in entry, "Entry should have question"
            assert "model_response" in entry, "Entry should have model response"
            assert "relevance_assessment" in entry, "Entry should have relevance assessment"
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –æ—Ü–µ–Ω–∫–∏
            assessment = entry["relevance_assessment"]
            assert 0.0 <= assessment["accuracy_score"] <= 1.0, "Accuracy score should be 0-1"
            assert 0.0 <= assessment["completeness_score"] <= 1.0, "Completeness score should be 0-1"
            assert 0.0 <= assessment["relevance_score"] <= 1.0, "Relevance score should be 0-1"
        
        print(f"‚úÖ –ü—Ä–æ—Ç–æ–∫–æ–ª —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è —Å–æ–∑–¥–∞–Ω: {protocol_file}")


if __name__ == "__main__":
    # –ó–∞–ø—É—Å–∫ —Ç–µ—Å—Ç–æ–≤ —Å –ø–æ–¥—Ä–æ–±–Ω—ã–º –≤—ã–≤–æ–¥–æ–º
    print("üß™ –ó–∞–ø—É—Å–∫ —Ç–µ—Å—Ç–æ–≤ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ Knowledge Base...")
    pytest.main([__file__, "-v", "-s", "--tb=short"])
