"""
Smart Librarian - –£–º–Ω—ã–π –±–∏–±–ª–∏–æ—Ç–µ–∫–∞—Ä—å
–ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–π –ø–æ–º–æ—â–Ω–∏–∫ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Ä–∞–∑–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–æ–≤
"""

import streamlit as st
import os
import mimetypes
from pathlib import Path
from typing import List, Dict, Optional, Tuple, Any
import json
import re
from datetime import datetime
import logging
from .document_manager import DocumentManager
from ..documents.vision_processor import VisionProcessor
from .base_agent import BaseAgent
from .transaction_manager import TransactionManager
from .transaction_decorator import with_transaction, manual_transaction

class SmartLibrarian(BaseAgent):
    """–£–º–Ω—ã–π –±–∏–±–ª–∏–æ—Ç–µ–∫–∞—Ä—å - –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–π –ø–æ–º–æ—â–Ω–∏–∫ –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏
    
    –§—É–Ω–∫—Ü–∏–∏:
    - –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Ä–∞–∑–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–æ–≤
    - –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–∞ –∏ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
    - –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ —Å–æ–∑–¥–∞–Ω–∏—é –±–∞–∑ –∑–Ω–∞–Ω–∏–π
    - –£–º–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –∏ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
    """
    
    def __init__(self, kb_manager, pdf_processor):
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –±–∞–∑–æ–≤—ã–π –∞–≥–µ–Ω—Ç
        super().__init__("SmartLibrarian", "document_analysis")
        
        self.kb_manager = kb_manager
        self.pdf_processor = pdf_processor
        self.document_manager = DocumentManager()
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—É—Ç—å –∏–∑ document_manager
        self.upload_dir = self.document_manager.upload_dir
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π (–∏—Å–ø–æ–ª—å–∑—É–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π VisionProcessor)
        self.vision_processor = VisionProcessor()
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –º–µ–Ω–µ–¥–∂–µ—Ä —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π
        self.transaction_manager = TransactionManager()
        
        # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
        self.images_dir = self.upload_dir.parent / "extracted_images"
        self.images_dir.mkdir(exist_ok=True)
        
        # –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã
        self.supported_formats = {
            '.pdf': 'PDF –¥–æ–∫—É–º–µ–Ω—Ç',
            '.txt': '–¢–µ–∫—Å—Ç–æ–≤—ã–π —Ñ–∞–π–ª',
            '.docx': 'Word –¥–æ–∫—É–º–µ–Ω—Ç',
            '.doc': 'Word –¥–æ–∫—É–º–µ–Ω—Ç (—Å—Ç–∞—Ä—ã–π —Ñ–æ—Ä–º–∞—Ç)',
            '.rtf': 'Rich Text Format',
            '.md': 'Markdown –¥–æ–∫—É–º–µ–Ω—Ç',
            '.html': 'HTML –¥–æ–∫—É–º–µ–Ω—Ç',
            '.xml': 'XML –¥–æ–∫—É–º–µ–Ω—Ç'
        }
        
        # –¢–∏–ø—ã –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü–∏–∏
        self.content_types = {
            'technical_regulations': {
                'keywords': ['—Ä–µ–≥–ª–∞–º–µ–Ω—Ç', '—Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–π', '—Å—Ç–∞–Ω–¥–∞—Ä—Ç', '—Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è', '–ø—Ä–æ—Ü–µ–¥—É—Ä—ã', '–∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è'],
                'category': '–¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —Ä–µ–≥–ª–∞–º–µ–Ω—Ç—ã',
                'description': '–¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —Ä–µ–≥–ª–∞–º–µ–Ω—Ç—ã –∏ —Å—Ç–∞–Ω–¥–∞—Ä—Ç—ã'
            },
            'licenses': {
                'keywords': ['–ª–∏—Ü–µ–Ω–∑–∏—è', '—Ä–∞–∑—Ä–µ—à–µ–Ω–∏–µ', '—Å–µ—Ä—Ç–∏—Ñ–∏–∫–∞—Ç', '–∞—Ç—Ç–µ—Å—Ç–∞—Ü–∏—è', '–¥–æ–ø—É—Å–∫', 'billmaster', '–ø—Ä–æ–≥—Ä–∞–º–º–Ω–æ–µ –æ–±–µ—Å–ø–µ—á–µ–Ω–∏–µ', 'software'],
                'exclude_keywords': ['best practices', 'guide', 'service', 'technical', 'sbd', 'short burst data'],
                'category': '–õ–∏—Ü–µ–Ω–∑–∏–∏ –∏ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è',
                'description': '–õ–∏—Ü–µ–Ω–∑–∏–∏, —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è –∏ —Å–µ—Ä—Ç–∏—Ñ–∏–∫–∞—Ç—ã'
            },
            'manuals': {
                'keywords': ['—Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ', '–∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è', '–º–∞–Ω—É–∞–ª', '–ø–æ—Å–æ–±–∏–µ', '–≥–∞–π–¥', 'how to'],
                'category': '–†—É–∫–æ–≤–æ–¥—Å—Ç–≤–∞',
                'description': '–†—É–∫–æ–≤–æ–¥—Å—Ç–≤–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –∏ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏'
            },
            'technical_guides': {
                'keywords': ['best practices', 'guide', 'service guide', 'technical guide', 'sbd', 'short burst data', 'iridium', 'service documentation'],
                'category': '–¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–∞',
                'description': '–¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–∞ –∏ –≥–∞–π–¥—ã –ø–æ —É—Å–ª—É–≥–∞–º'
            },
            'billing': {
                'keywords': ['–±–∏–ª–ª–∏–Ω–≥', '—Ç–∞—Ä–∏—Ñ', '–æ–ø–ª–∞—Ç–∞', '—Å—á–µ—Ç', '–ø–ª–∞—Ç–µ–∂', 'billing', 'tariff'],
                'category': '–ë–∏–ª–ª–∏–Ω–≥ –∏ —Ç–∞—Ä–∏—Ñ—ã',
                'description': '–î–æ–∫—É–º–µ–Ω—Ç—ã –ø–æ –±–∏–ª–ª–∏–Ω–≥—É –∏ —Ç–∞—Ä–∏—Ñ–∞–º'
            },
            'legal': {
                'keywords': ['–¥–æ–≥–æ–≤–æ—Ä', '—Å–æ–≥–ª–∞—à–µ–Ω–∏–µ', '—É—Å–ª–æ–≤–∏—è', '–ø–æ–ª–∏—Ç–∏–∫–∞', '–ø—Ä–∞–≤–æ–≤–æ–π', 'legal'],
                'category': '–ü—Ä–∞–≤–æ–≤—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã',
                'description': '–ü—Ä–∞–≤–æ–≤—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –∏ —Å–æ–≥–ª–∞—à–µ–Ω–∏—è'
            },
            'technical_docs': {
                'keywords': ['—Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∞—è', '–¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è', 'api', '–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å', '–ø—Ä–æ—Ç–æ–∫–æ–ª', '—Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∞—è'],
                'category': '–¢–µ—Ö–Ω–∏—á–µ—Å–∫–∞—è –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è',
                'description': '–¢–µ—Ö–Ω–∏—á–µ—Å–∫–∞—è –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è –∏ —Å–ø–µ—Ü–∏—Ñ–∏–∫–∞—Ü–∏–∏'
            }
        }
    
    @manual_transaction("document_analysis")
    def analyze_document_transactional(self, file_path: Path, transaction_id: str = None) -> Dict:
        """–ê–Ω–∞–ª–∏–∑ –æ–¥–Ω–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π"""
        return self.analyze_document(file_path)
    
    def analyze_document(self, file_path: Path) -> Dict:
        """–ê–Ω–∞–ª–∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –µ–≥–æ —Ç–∏–ø–∞ –∏ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ"""
        analysis = {
            'file_path': str(file_path),
            'file_name': file_path.name,
            'file_size': file_path.stat().st_size,
            'file_extension': file_path.suffix.lower(),
            'mime_type': mimetypes.guess_type(str(file_path))[0],
            'content_type': None,
            'category': '–î—Ä—É–≥–æ–µ',
            'description': '',
            'keywords': [],
            'confidence': 0.0,
            'recommendations': []
        }
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π –ª–∏ —Ñ–æ—Ä–º–∞—Ç
        if analysis['file_extension'] in self.supported_formats:
            analysis['format_supported'] = True
            analysis['format_description'] = self.supported_formats[analysis['file_extension']]
        else:
            analysis['format_supported'] = False
            analysis['format_description'] = f"–§–æ—Ä–º–∞—Ç {analysis['file_extension']} –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è"
            return analysis
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        try:
            text_content = self._extract_text_content(file_path)
        except Exception:
            text_content = ""
        if not text_content:
            analysis['recommendations'].append("–ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å —Ç–µ–∫—Å—Ç –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞")
            return analysis
        
        analysis['text_length'] = len(text_content)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç
        analysis['raw_ocr_text'] = text_content
        
        # 1) –ë–∞–∑–æ–≤–∞—è –æ—á–∏—Å—Ç–∫–∞ –±–µ–∑ —Å–æ–∫—Ä–∞—â–µ–Ω–∏–π (–ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç)
        base_cleaned_text = self._get_original_cleaned_text(text_content)
        analysis['original_cleaned_text'] = base_cleaned_text
        
        # –í—ã–¥–µ–ª—è–µ–º Q/A –ø–∞—Ä—ã (FAQ) –µ—Å–ª–∏ –µ—Å—Ç—å
        try:
            qa_pairs = self._extract_qa_pairs(base_cleaned_text)
            analysis['qa_pairs'] = qa_pairs
            analysis['qa_count'] = len(qa_pairs)
        except Exception:
            analysis['qa_pairs'] = []
            analysis['qa_count'] = 0
        
        # 2) –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø–æ–ª–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ —Å –ø–æ–º–æ—â—å—é –∞–≥–µ–Ω—Ç–∞ (–Ω–µ —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è)
        # –î–ª—è —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö —Ñ–æ—Ä–º–∞—Ç–æ–≤ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –ù–ï –≤—ã–∑—ã–≤–∞–µ–º –≤–Ω–µ—à–Ω–∏–π –∞–≥–µ–Ω—Ç (ProxyAPI),
        # —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å –ª–∏—à–Ω–∏—Ö —Å–µ—Ç–µ–≤—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤. –ú–æ–∂–Ω–æ –≤–∫–ª—é—á–∏—Ç—å —á–µ—Ä–µ–∑ STEC_ENABLE_AI_CLEANING_TEXT=true
        import os
        text_like_exts = ['.docx', '.doc', '.txt', '.md', '.rtf', '.html', '.xml']
        enable_ai_cleaning_text = os.getenv("STEC_ENABLE_AI_CLEANING_TEXT", "false").lower() == "true"
        if analysis['file_extension'] in text_like_exts and not enable_ai_cleaning_text:
            fully_cleaned_text = base_cleaned_text
        else:
            fully_cleaned_text = self._create_full_cleaned_text(base_cleaned_text)
        
        # 3) –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø—Ä–∞–≤–∏–ª–æ —Ä–∞–∑–º–µ—Ä–∞: –¥–ª—è –æ—á–µ–Ω—å –±–æ–ª—å—à–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –æ—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ abstract
        file_size_mb = analysis['file_size'] / (1024 * 1024)
        is_huge_document = (file_size_mb > 20) or (len(fully_cleaned_text) > 100_000)
        analysis['is_huge_document'] = is_huge_document
        
        # –ï—Å–ª–∏ –¥–æ–∫—É–º–µ–Ω—Ç –Ω–µ –æ–≥—Ä–æ–º–Ω—ã–π, –≤–∫–ª—é—á–∞–µ–º –ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç; –∏–Ω–∞—á–µ –æ–ø—É—Å–∫–∞–µ–º –µ–≥–æ –∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        analysis['full_cleaned_text'] = fully_cleaned_text if not is_huge_document else ""
        analysis['full_included'] = not is_huge_document
        
        # 4) –°–æ–∑–¥–∞–µ–º –æ—Ç–¥–µ–ª—å–Ω—ã–π –∫—Ä–∞—Ç–∫–∏–π abstract
        # –î–ª—è —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö —Ñ–æ—Ä–º–∞—Ç–æ–≤ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –∏—Å–ø–æ–ª—å–∑—É–µ–º –ª–æ–∫–∞–ª—å–Ω—É—é –≤—ã–∂–∏–º–∫—É –±–µ–∑ ProxyAPI
        if analysis['file_extension'] in text_like_exts and not enable_ai_cleaning_text:
            abstract_text = self._create_smart_summary(base_cleaned_text, analysis['file_name'])
        else:
            abstract_text = self._create_ocr_cleaning_agent(base_cleaned_text)
        analysis['smart_summary'] = abstract_text
        
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä–µ–≤—å—é (–±–µ—Ä–µ–º –∏–∑ abstract)
        smart_summary = abstract_text
        
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —É–º–Ω—É—é –≤—ã–∂–∏–º–∫—É –∫–∞–∫ –ø—Ä–µ–≤—å—é
        analysis['text_preview'] = smart_summary[:500] + "..." if len(smart_summary) > 500 else smart_summary
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ (–∏—Å–ø–æ–ª—å–∑—É–µ–º –æ—á–∏—â–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç)
        content_analysis = self._analyze_content(base_cleaned_text, file_path.name)
        analysis.update(content_analysis)
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ
        if analysis['file_extension'] == '.pdf':
            image_analyses = self.analyze_document_images(file_path)
            analysis['images'] = image_analyses
            analysis['image_count'] = len(image_analyses)
            
            # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ –∞–Ω–∞–ª–∏–∑—ã –æ—Ç Gemini –¥–ª—è –ø–µ—Ä–µ–¥–∞—á–∏ –≤ –∞–≥–µ–Ω—Ç –æ—á–∏—Å—Ç–∫–∏
            gemini_analyses = []
            for img_analysis in image_analyses:
                if 'analysis' in img_analysis and img_analysis['analysis']:
                    gemini_analyses.append(img_analysis['analysis'])
            
            # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ –∞–Ω–∞–ª–∏–∑—ã –æ—Ç Gemini
            if gemini_analyses:
                combined_gemini_analysis = "\n\n".join(gemini_analyses)
                analysis['gemini_analysis'] = combined_gemini_analysis
                
                # –ü–µ—Ä–µ—Å–æ–∑–¥–∞–µ–º —É–º–Ω—É—é –≤—ã–∂–∏–º–∫—É —Å —É—á–µ—Ç–æ–º –∞–Ω–∞–ª–∏–∑–∞ –æ—Ç Gemini
                smart_summary_with_gemini = self._create_ocr_cleaning_agent(cleaned_text, combined_gemini_analysis)
                analysis['smart_summary'] = smart_summary_with_gemini
                
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –æ—Ç –∞–≥–µ–Ω—Ç–∞, –µ—Å–ª–∏ –æ–Ω–∏ –µ—Å—Ç—å
                if hasattr(self, '_last_parsed_result') and self._last_parsed_result:
                    agent_keywords = self._last_parsed_result.get('keywords', [])
                    if agent_keywords:
                        analysis['keywords'] = agent_keywords
        elif analysis['file_extension'] in ['.docx', '.doc']:
            image_analyses = self.analyze_docx_images(file_path)
            analysis['images'] = image_analyses
            analysis['image_count'] = len(image_analyses)
            analysis['gemini_analysis'] = None
        else:
            analysis['images'] = []
            analysis['image_count'] = 0
            analysis['gemini_analysis'] = None
            # –Ø–≤–Ω–æ —Ñ–∏–∫—Å–∏—Ä—É–µ–º, —á—Ç–æ Gemini –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è –Ω–µ-–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
            try:
                logger = logging.getLogger(__name__)
                logger.info(f"Gemini –ø—Ä–æ–ø—É—â–µ–Ω –¥–ª—è —Ñ–æ—Ä–º–∞—Ç–∞ {analysis['file_extension']} (—Ä–∞–∑—Ä–µ—à–µ–Ω —Ç–æ–ª—å–∫–æ –¥–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π/–∫–∞—Ä—Ç–∏–Ω–æ–∫ –∏–∑ PDF)")
            except Exception:
                pass

        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –∏ –∑–∞–≤–µ—Ä—à–∞–µ–º –∞–Ω–∞–ª–∏–∑
        analysis['recommendations'] = self._generate_recommendations(analysis)
        kb_suggestion = self._suggest_kb_topic(analysis)
        analysis['suggested_kb'] = kb_suggestion
        return analysis
    
    def generate_relevance_test_questions(self, analysis: Dict) -> List[Dict[str, Any]]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ—Å—Ç–æ–≤—ã—Ö –≤–æ–ø—Ä–æ—Å–æ–≤ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ –ë–ó –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞–Ω–∞–ª–∏–∑–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        try:
            import logging
            import os
            import requests
            
            logger = logging.getLogger(__name__)
            logger.info("–ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ –≤–æ–ø—Ä–æ—Å—ã –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ —É–∂–µ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –≤–æ–ø—Ä–æ—Å—ã –æ—Ç –∞–≥–µ–Ω—Ç–∞
            if hasattr(self, '_last_parsed_result') and self._last_parsed_result:
                agent_questions = self._last_parsed_result.get('test_questions', [])
                if agent_questions:
                    logger.info(f"‚úÖ –ò—Å–ø–æ–ª—å–∑—É–µ–º {len(agent_questions)} –≤–æ–ø—Ä–æ—Å–æ–≤ —Å –æ—Ç–≤–µ—Ç–∞–º–∏, —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∞–≥–µ–Ω—Ç–æ–º")
                    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ –Ω—É–∂–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç
                    formatted_questions = []
                    for i, question in enumerate(agent_questions):
                        formatted_questions.append({
                            "question": question.get('question', ''),
                            "answer": question.get('answer', '–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ'),
                            "expected_keywords": [],  # –ê–≥–µ–Ω—Ç –Ω–µ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –≤–æ–ø—Ä–æ—Å–æ–≤
                            "category": "agent_generated",
                            "difficulty": "medium"
                        })
                    return formatted_questions
            
            # –ü–æ–ª—É—á–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –¥–æ–∫—É–º–µ–Ω—Ç–µ
            doc_content = analysis.get('original_cleaned_text', analysis.get('full_cleaned_text', ''))
            doc_summary = analysis.get('smart_summary', '')
            doc_category = analysis.get('category', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')
            doc_title = analysis.get('file_name', '–î–æ–∫—É–º–µ–Ω—Ç')
            
            # –õ–æ–≥–∏—Ä—É–µ–º —Ä–∞–∑–º–µ—Ä –¥–æ–∫—É–º–µ–Ω—Ç–∞
            logger.info(f"–†–∞–∑–º–µ—Ä –¥–æ–∫—É–º–µ–Ω—Ç–∞ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞: {len(doc_content)} —Å–∏–º–≤–æ–ª–æ–≤")
            
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ª–æ–∫–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å —á–µ—Ä–µ–∑ self.chat_model
            logger.info("–ò—Å–ø–æ–ª—å–∑—É–µ–º –ª–æ–∫–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ—Å—Ç–æ–≤—ã—Ö –≤–æ–ø—Ä–æ—Å–æ–≤")
            logger.info(f"üîç DEBUG: –†–∞–∑–º–µ—Ä –¥–æ–∫—É–º–µ–Ω—Ç–∞: {len(doc_content)} —Å–∏–º–≤–æ–ª–æ–≤")
            logger.info(f"üîç DEBUG: –ù–∞–∑–≤–∞–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞: {doc_title}")
            logger.info(f"üîç DEBUG: –ö–∞—Ç–µ–≥–æ—Ä–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–∞: {doc_category}")
            
            # –°–æ–∑–¥–∞–µ–º –ø—Ä–æ–º–ø—Ç –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ—Å—Ç–æ–≤—ã—Ö –≤–æ–ø—Ä–æ—Å–æ–≤ —Å –æ—Ç–≤–µ—Ç–∞–º–∏
            logger.info(f"üîç DEBUG: –°–æ–∑–¥–∞–µ–º –ø—Ä–æ–º–ø—Ç –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–æ–ø—Ä–æ—Å–æ–≤")
            logger.info(f"üîç DEBUG: –î–ª–∏–Ω–∞ –ø—Ä–æ–º–ø—Ç–∞ –±—É–¥–µ—Ç: ~{len(doc_content) + 1000} —Å–∏–º–≤–æ–ª–æ–≤")
            
            prompt = f"""
–¢—ã - —ç–∫—Å–ø–µ—Ä—Ç –ø–æ —Å–æ–∑–¥–∞–Ω–∏—é —Ç–µ—Å—Ç–æ–≤—ã—Ö –≤–æ–ø—Ä–æ—Å–æ–≤ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π. –¢–≤–æ—è –∑–∞–¥–∞—á–∞ - —Å–æ–∑–¥–∞—Ç—å 5-7 –≤–æ–ø—Ä–æ—Å–æ–≤ —Å –æ—Ç–≤–µ—Ç–∞–º–∏ –¢–û–õ–¨–ö–û –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏, –Ω–∞–π–¥–µ–Ω–Ω–æ–π –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ.

–ò–ù–§–û–†–ú–ê–¶–ò–Ø –û –î–û–ö–£–ú–ï–ù–¢–ï:
- –ù–∞–∑–≤–∞–Ω–∏–µ —Ñ–∞–π–ª–∞: {doc_title}
- –ö–∞—Ç–µ–≥–æ—Ä–∏—è: {doc_category}
- –ö—Ä–∞—Ç–∫–æ–µ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ: {doc_summary[:500]}...

–°–û–î–ï–†–ñ–ò–ú–û–ï –î–û–ö–£–ú–ï–ù–¢–ê (–ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç, {len(doc_content)} —Å–∏–º–≤–æ–ª–æ–≤):
{doc_content}

–ó–ê–î–ê–ß–ê:
–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –†–ï–ê–õ–¨–ù–´–ô —Ç–µ–∫—Å—Ç –¥–æ–∫—É–º–µ–Ω—Ç–∞ –∏ —Å–æ–∑–¥–∞–π 3-5 —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –≤–æ–ø—Ä–æ—Å–æ–≤ —Å –æ—Ç–≤–µ—Ç–∞–º–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ù–ê–ô–î–ï–ù–ù–û–ô –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏.

–ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–û:
- –ù–ï –∏—Å–ø–æ–ª—å–∑—É–π —à–∞–±–ª–æ–Ω–Ω—ã–µ –≤–æ–ø—Ä–æ—Å—ã!
- –ù–ï –ø—Ä–∏–¥—É–º—ã–≤–∞–π –≤–æ–ø—Ä–æ—Å—ã "–≤ –æ–±—â–µ–º"!
- –ê–Ω–∞–ª–∏–∑–∏—Ä—É–π –ö–û–ù–ö–†–ï–¢–ù–´–ô —Ç–µ–∫—Å—Ç –¥–æ–∫—É–º–µ–Ω—Ç–∞!
- –ù–∞–π–¥–∏ –†–ï–ê–õ–¨–ù–´–ï —Ñ–∞–∫—Ç—ã, –Ω–∞–∑–≤–∞–Ω–∏—è, –¥–∞—Ç—ã, –ø—Ä–æ—Ü–µ–¥—É—Ä—ã –≤ —Ç–µ–∫—Å—Ç–µ!
- –°–æ–∑–¥–∞–π –≤–æ–ø—Ä–æ—Å—ã –æ–± —ç—Ç–∏—Ö –†–ï–ê–õ–¨–ù–´–• —Ñ–∞–∫—Ç–∞—Ö!

–ü–†–û–¶–ï–°–°:
1. –ü—Ä–æ—á–∏—Ç–∞–π –í–ï–°–¨ –¥–æ–∫—É–º–µ–Ω—Ç –≤–Ω–∏–º–∞—Ç–µ–ª—å–Ω–æ
2. –ù–∞–π–¥–∏ 3-5 –ö–û–ù–ö–†–ï–¢–ù–´–• —Ñ–∞–∫—Ç–æ–≤ –∏–∑ —Ç–µ–∫—Å—Ç–∞
3. –°–æ–∑–¥–∞–π –≤–æ–ø—Ä–æ—Å—ã –æ–± —ç—Ç–∏—Ö –ö–û–ù–ö–†–ï–¢–ù–´–• —Ñ–∞–∫—Ç–∞—Ö
4. –î–∞–π –æ—Ç–≤–µ—Ç—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ –ù–ê–ô–î–ï–ù–ù–û–ô –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏

–í–ê–ñ–ù–û:
- –û—Ç–≤–µ—Ç—ã –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞
- –ï—Å–ª–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –Ω–µ—Ç, —É–∫–∞–∂–∏ "–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ"
- –í–æ–ø—Ä–æ—Å—ã –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –ø—Ä–æ—Å—Ç—ã–º–∏ –∏ –ø–æ–Ω—è—Ç–Ω—ã–º–∏
- –ò—Å–ø–æ–ª—å–∑—É–π –ª—é–±—É—é –ø–æ–ª–µ–∑–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞

–¢–†–ï–ë–û–í–ê–ù–ò–Ø:
1. –í–æ–ø—Ä–æ—Å—ã –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ
2. –û—Ç–≤–µ—Ç—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞
3. –ü—Ä–æ—Å—Ç—ã–µ –∏ –ø–æ–Ω—è—Ç–Ω—ã–µ –≤–æ–ø—Ä–æ—Å—ã
4. –ö–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ —Ñ–∞–∫—Ç—ã –≤ –æ—Ç–≤–µ—Ç–∞—Ö

–§–û–†–ú–ê–¢ –û–¢–í–ï–¢–ê (JSON):
{{{{"questions": [
        {{{{"question": "–í–æ–ø—Ä–æ—Å –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ",
            "answer": "–û—Ç–≤–µ—Ç –¢–û–õ–¨–ö–û –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞ —Å —Ü–∏—Ç–∞—Ç–∞–º–∏",
            "expected_keywords": ["–∫–ª—é—á–µ–≤–æ–µ", "—Å–ª–æ–≤–æ1", "—Å–ª–æ–≤–æ2"],
            "category": "–∫–∞—Ç–µ–≥–æ—Ä–∏—è_–≤–æ–ø—Ä–æ—Å–∞",
            "difficulty": "easy/medium/hard",
            "source_info": "–ö–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π —Ñ—Ä–∞–≥–º–µ–Ω—Ç –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞, –Ω–∞ –∫–æ—Ç–æ—Ä–æ–º –æ—Å–Ω–æ–≤–∞–Ω –æ—Ç–≤–µ—Ç"
        }}}}
    ]
}}}}

–ü–†–ò–ú–ï–†–´ –í–û–ü–†–û–°–û–í:
- –í–æ–ø—Ä–æ—Å: "–ö–∞–∫–∞—è –æ—Å–Ω–æ–≤–Ω–∞—è —Ç–µ–º–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞?"
  –û—Ç–≤–µ—Ç: "–û—Å–Ω–æ–≤–Ω–∞—è —Ç–µ–º–∞: [–Ω–∞–π–¥–µ–Ω–Ω–∞—è —Ç–µ–º–∞]"
- –í–æ–ø—Ä–æ—Å: "–ö–∞–∫–∏–µ –∫–æ–º–ø–∞–Ω–∏–∏ —É–ø–æ–º–∏–Ω–∞—é—Ç—Å—è –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ?"
  –û—Ç–≤–µ—Ç: "–í –¥–æ–∫—É–º–µ–Ω—Ç–µ —É–ø–æ–º–∏–Ω–∞—é—Ç—Å—è: [–Ω–∞–π–¥–µ–Ω–Ω—ã–µ –∫–æ–º–ø–∞–Ω–∏–∏]"
- –í–æ–ø—Ä–æ—Å: "–ö–∞–∫–∏–µ –¥–∞—Ç—ã —É–∫–∞–∑–∞–Ω—ã –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ?"
  –û—Ç–≤–µ—Ç: "–í –¥–æ–∫—É–º–µ–Ω—Ç–µ —É–∫–∞–∑–∞–Ω—ã –¥–∞—Ç—ã: [–Ω–∞–π–¥–µ–Ω–Ω—ã–µ –¥–∞—Ç—ã]"
- –í–æ–ø—Ä–æ—Å: "–ö–∞–∫–∏–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è –æ–ø–∏—Å–∞–Ω—ã –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ?"
  –û—Ç–≤–µ—Ç: "–í –¥–æ–∫—É–º–µ–Ω—Ç–µ –æ–ø–∏—Å–∞–Ω—ã —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è: [–Ω–∞–π–¥–µ–Ω–Ω—ã–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è]"

–í–ï–†–ù–ò –¢–û–õ–¨–ö–û JSON –ë–ï–ó –î–û–ü–û–õ–ù–ò–¢–ï–õ–¨–ù–´–• –ö–û–ú–ú–ï–ù–¢–ê–†–ò–ï–í.
"""
            
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ª–æ–∫–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å
            try:
                from langchain.prompts import ChatPromptTemplate
                
                # –°–æ–∑–¥–∞–µ–º –ø—Ä–æ–º–ø—Ç –¥–ª—è LangChain
                system_prompt = "–¢—ã - —ç–∫—Å–ø–µ—Ä—Ç –ø–æ —Å–æ–∑–¥–∞–Ω–∏—é —Ç–µ—Å—Ç–æ–≤—ã—Ö –≤–æ–ø—Ä–æ—Å–æ–≤ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π. –¢–≤–æ—è –∑–∞–¥–∞—á–∞ - —Å–æ–∑–¥–∞—Ç—å –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –≤–æ–ø—Ä–æ—Å—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞–Ω–∞–ª–∏–∑–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞."
                
                template = ChatPromptTemplate.from_messages([
                    ("system", system_prompt),
                    ("user", prompt)
                ])
                
                # –í—ã–∑—ã–≤–∞–µ–º –º–æ–¥–µ–ª—å
                logger.info(f"üîç DEBUG: –í—ã–∑—ã–≤–∞–µ–º –º–æ–¥–µ–ª—å –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–æ–ø—Ä–æ—Å–æ–≤")
                logger.info(f"üîç DEBUG: –¢–∏–ø –º–æ–¥–µ–ª–∏: {type(self.chat_model)}")
                logger.info(f"üîç DEBUG: –ú–æ–¥–µ–ª—å: {getattr(self.chat_model, 'model_name', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')}")
                
                chain = template | self.chat_model
                response = chain.invoke({})
                
                logger.info(f"üîç DEBUG: –ü–æ–ª—É—á–µ–Ω –æ—Ç–≤–µ—Ç –æ—Ç –º–æ–¥–µ–ª–∏")
                logger.info(f"üîç DEBUG: –¢–∏–ø –æ—Ç–≤–µ—Ç–∞: {type(response)}")
                
                # –ü–æ–ª—É—á–∞–µ–º —Ç–µ–∫—Å—Ç –æ—Ç–≤–µ—Ç–∞
                response_text = getattr(response, 'content', None) or str(response)
                logger.info(f"üîç DEBUG: –û—Ç–≤–µ—Ç –º–æ–¥–µ–ª–∏: {response_text[:500]}...")
                
                # –û—á–∏—â–∞–µ–º –æ—Ç–≤–µ—Ç –æ—Ç markdown –±–ª–æ–∫–æ–≤
                cleaned_response = response_text.strip()
                if cleaned_response.startswith('```json'):
                    cleaned_response = cleaned_response[7:]  # –£–±–∏—Ä–∞–µ–º ```json
                if cleaned_response.startswith('```'):
                    cleaned_response = cleaned_response[3:]   # –£–±–∏—Ä–∞–µ–º ```
                if cleaned_response.endswith('```'):
                    cleaned_response = cleaned_response[:-3]  # –£–±–∏—Ä–∞–µ–º ``` –≤ –∫–æ–Ω—Ü–µ
                cleaned_response = cleaned_response.strip()
                
                # –ü–∞—Ä—Å–∏–º JSON –æ—Ç–≤–µ—Ç
                try:
                    import json
                    questions_data = json.loads(cleaned_response)
                    questions = questions_data.get('questions', [])
                    logger.info(f"‚úÖ –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ {len(questions)} —Ç–µ—Å—Ç–æ–≤—ã—Ö –≤–æ–ø—Ä–æ—Å–æ–≤ —á–µ—Ä–µ–∑ LLM")
                    return questions
                except json.JSONDecodeError as e:
                    logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ JSON –æ—Ç–≤–µ—Ç–∞: {e}")
                    logger.error(f"–û—á–∏—â–µ–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç –º–æ–¥–µ–ª–∏: {cleaned_response[:500]}...")
                    logger.error("‚ùå –ù–ï –ò–°–ü–û–õ–¨–ó–£–ï–ú —à–∞–±–ª–æ–Ω–Ω—ã–µ –≤–æ–ø—Ä–æ—Å—ã! –¢—Ä–µ–±—É–µ—Ç—Å—è —Ä–µ–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞!")
                    # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫ –≤–º–µ—Å—Ç–æ —à–∞–±–ª–æ–Ω–Ω—ã—Ö –≤–æ–ø—Ä–æ—Å–æ–≤
                    return []
            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã–∑–æ–≤–µ –ª–æ–∫–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏: {e}")
                logger.error(f"üîç DEBUG: –¢–∏–ø –æ—à–∏–±–∫–∏: {type(e)}")
                import traceback
                logger.error(f"üîç DEBUG: –¢—Ä–∞—Å—Å–∏—Ä–æ–≤–∫–∞ –æ—à–∏–±–∫–∏: {traceback.format_exc()}")
                logger.error("‚ùå –ù–ï –ò–°–ü–û–õ–¨–ó–£–ï–ú —à–∞–±–ª–æ–Ω–Ω—ã–µ –≤–æ–ø—Ä–æ—Å—ã! –¢—Ä–µ–±—É–µ—Ç—Å—è —Ä–µ–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞!")
                # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫ –≤–º–µ—Å—Ç–æ —à–∞–±–ª–æ–Ω–Ω—ã—Ö –≤–æ–ø—Ä–æ—Å–æ–≤
                return []
                
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ—Å—Ç–æ–≤—ã—Ö –≤–æ–ø—Ä–æ—Å–æ–≤: {e}")
            logger.error(f"üîç DEBUG: –¢–∏–ø –æ—à–∏–±–∫–∏: {type(e)}")
            import traceback
            logger.error(f"üîç DEBUG: –¢—Ä–∞—Å—Å–∏—Ä–æ–≤–∫–∞ –æ—à–∏–±–∫–∏: {traceback.format_exc()}")
            logger.error("‚ùå –ù–ï –ò–°–ü–û–õ–¨–ó–£–ï–ú —à–∞–±–ª–æ–Ω–Ω—ã–µ –≤–æ–ø—Ä–æ—Å—ã! –¢—Ä–µ–±—É–µ—Ç—Å—è —Ä–µ–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞!")
            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫ –≤–º–µ—Å—Ç–æ —à–∞–±–ª–æ–Ω–Ω—ã—Ö –≤–æ–ø—Ä–æ—Å–æ–≤
            return []
    
    def _get_basic_test_questions(self, category: str) -> List[Dict[str, Any]]:
        """–ë–∞–∑–æ–≤—ã–µ —Ç–µ—Å—Ç–æ–≤—ã–µ –≤–æ–ø—Ä–æ—Å—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏"""
        import logging
        logger = logging.getLogger(__name__)
        logger.warning(f"‚ö†Ô∏è –ò—Å–ø–æ–ª—å–∑—É–µ–º –±–∞–∑–æ–≤—ã–µ –≤–æ–ø—Ä–æ—Å—ã –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∏: {category}")
        basic_questions = {
            '–õ–∏—Ü–µ–Ω–∑–∏–∏ –∏ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è': [
                {
                    "question": "–ö–∞–∫–∏–µ –ª–∏—Ü–µ–Ω–∑–∏–∏ —É–ø–æ–º–∏–Ω–∞—é—Ç—Å—è –≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö?",
                    "expected_keywords": ["–ª–∏—Ü–µ–Ω–∑–∏—è", "—Ä–∞–∑—Ä–µ—à–µ–Ω–∏–µ", "–ø—Ä–æ–≥—Ä–∞–º–º–Ω–æ–µ –æ–±–µ—Å–ø–µ—á–µ–Ω–∏–µ"],
                    "category": "licenses",
                    "difficulty": "easy"
                },
                {
                    "question": "–ö–∞–∫–∏–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è –∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é –ª–∏—Ü–µ–Ω–∑–∏–π?",
                    "expected_keywords": ["—Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è", "—É—Å–ª–æ–≤–∏—è", "–∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ"],
                    "category": "license_requirements",
                    "difficulty": "medium"
                }
            ],
            '–ë–∏–ª–ª–∏–Ω–≥ –∏ —Ç–∞—Ä–∏—Ñ—ã': [
                {
                    "question": "–ö–∞–∫ —Ä–∞—Å—Å—á–∏—Ç—ã–≤–∞—é—Ç—Å—è —Ç–∞—Ä–∏—Ñ—ã?",
                    "expected_keywords": ["—Ç–∞—Ä–∏—Ñ", "—Ä–∞—Å—á–µ—Ç", "—Å—Ç–æ–∏–º–æ—Å—Ç—å"],
                    "category": "billing",
                    "difficulty": "medium"
                },
                {
                    "question": "–ö–∞–∫–∏–µ —É—Å–ª–æ–≤–∏—è –æ–ø–ª–∞—Ç—ã —É–∫–∞–∑–∞–Ω—ã?",
                    "expected_keywords": ["–æ–ø–ª–∞—Ç–∞", "—É—Å–ª–æ–≤–∏—è", "–ø–ª–∞—Ç–µ–∂"],
                    "category": "payment_terms",
                    "difficulty": "easy"
                }
            ],
            '–ü—Ä–∞–≤–æ–≤—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã': [
                {
                    "question": "–ö–∞–∫–∏–µ –æ—Å–Ω–æ–≤–Ω—ã–µ —É—Å–ª–æ–≤–∏—è –¥–æ–≥–æ–≤–æ—Ä–∞?",
                    "expected_keywords": ["–¥–æ–≥–æ–≤–æ—Ä", "—É—Å–ª–æ–≤–∏—è", "—Å—Ç–æ—Ä–æ–Ω—ã"],
                    "category": "contract_terms",
                    "difficulty": "medium"
                },
                {
                    "question": "–ö–∞–∫–∏–µ –ø—Ä–∞–≤–∞ –∏ –æ–±—è–∑–∞–Ω–Ω–æ—Å—Ç–∏ —Å—Ç–æ—Ä–æ–Ω?",
                    "expected_keywords": ["–ø—Ä–∞–≤–∞", "–æ–±—è–∑–∞–Ω–Ω–æ—Å—Ç–∏", "—Å—Ç–æ—Ä–æ–Ω—ã"],
                    "category": "rights_obligations",
                    "difficulty": "hard"
                }
            ],
            '–¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–∞': [
                {
                    "question": "–ö–∞–∫–∏–µ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è –æ–ø–∏—Å–∞–Ω—ã –≤ —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–µ?",
                    "answer": "–í —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–µ –æ–ø–∏—Å–∞–Ω—ã —Å–ª–µ–¥—É—é—â–∏–µ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è: [–Ω–∞–π—Ç–∏ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ]",
                    "expected_keywords": ["—Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è", "—Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ", "—Å–ø–µ—Ü–∏—Ñ–∏–∫–∞—Ü–∏–∏"],
                    "category": "technical_guides",
                    "difficulty": "medium"
                },
                {
                    "question": "–ö–∞–∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –æ–ø–∏—Å–∞–Ω–Ω—É—é —É—Å–ª—É–≥—É –∏–ª–∏ —Å–µ—Ä–≤–∏—Å?",
                    "answer": "–î–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —É—Å–ª—É–≥–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ: [–Ω–∞–π—Ç–∏ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ]",
                    "expected_keywords": ["–∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ", "–ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ", "—Ä–∞–±–æ—Ç–∞"],
                    "category": "technical_guides",
                    "difficulty": "easy"
                },
                {
                    "question": "–ö–∞–∫–∏–µ –ª—É—á—à–∏–µ –ø—Ä–∞–∫—Ç–∏–∫–∏ —Ä–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–Ω—ã?",
                    "answer": "–í –¥–æ–∫—É–º–µ–Ω—Ç–µ —Ä–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–Ω—ã —Å–ª–µ–¥—É—é—â–∏–µ –ª—É—á—à–∏–µ –ø—Ä–∞–∫—Ç–∏–∫–∏: [–Ω–∞–π—Ç–∏ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ]",
                    "expected_keywords": ["best practices", "—Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏", "—Å–æ–≤–µ—Ç—ã"],
                    "category": "technical_guides",
                    "difficulty": "medium"
                },
                {
                    "question": "–ö–∞–∫–∏–µ —É—Å–ª—É–≥–∏ –∏–ª–∏ —Å–µ—Ä–≤–∏—Å—ã —É–ø–æ–º–∏–Ω–∞—é—Ç—Å—è –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ?",
                    "answer": "–í –¥–æ–∫—É–º–µ–Ω—Ç–µ —É–ø–æ–º–∏–Ω–∞—é—Ç—Å—è —Å–ª–µ–¥—É—é—â–∏–µ —É—Å–ª—É–≥–∏: [–Ω–∞–π—Ç–∏ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ]",
                    "expected_keywords": ["—É—Å–ª—É–≥–∏", "—Å–µ—Ä–≤–∏—Å—ã", "services"],
                    "category": "technical_guides",
                    "difficulty": "easy"
                },
                {
                    "question": "–ö–∞–∫–∏–µ –∫–æ–º–ø–∞–Ω–∏–∏ –∏–ª–∏ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏ —É–ø–æ–º–∏–Ω–∞—é—Ç—Å—è –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ?",
                    "answer": "–í –¥–æ–∫—É–º–µ–Ω—Ç–µ —É–ø–æ–º–∏–Ω–∞—é—Ç—Å—è —Å–ª–µ–¥—É—é—â–∏–µ –∫–æ–º–ø–∞–Ω–∏–∏: [–Ω–∞–π—Ç–∏ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ]",
                    "expected_keywords": ["–∫–æ–º–ø–∞–Ω–∏–∏", "–æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏", "companies"],
                    "category": "technical_guides",
                    "difficulty": "easy"
                }
            ]
        }
        
        questions = basic_questions.get(category, [
            {
                "question": "–ö–∞–∫–∞—è –æ—Å–Ω–æ–≤–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è —Å–æ–¥–µ—Ä–∂–∏—Ç—Å—è –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ?",
                "expected_keywords": ["–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è", "–¥–æ–∫—É–º–µ–Ω—Ç", "—Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ"],
                "category": "general",
                "difficulty": "easy"
            }
        ])
        
        logger.warning(f"‚ö†Ô∏è –í–æ–∑–≤—Ä–∞—â–∞–µ–º {len(questions)} –±–∞–∑–æ–≤—ã—Ö –≤–æ–ø—Ä–æ—Å–æ–≤ –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∏: {category}")
        return questions
    
    def _test_single_question(self, question: Dict[str, Any]):
        """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –æ–¥–Ω–æ–≥–æ –≤–æ–ø—Ä–æ—Å–∞ —á–µ—Ä–µ–∑ RAG —Å–∏—Å—Ç–µ–º—É"""
        try:
            import streamlit as st
            import time
            
            question_text = question["question"]
            expected_keywords = question.get("expected_keywords", [])
            
            st.info(f"üîç –¢–µ—Å—Ç–∏—Ä—É–µ–º –≤–æ–ø—Ä–æ—Å: **{question_text}**")
            
            # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º RAG —Å–∏—Å—Ç–µ–º—É
            from ..rag.multi_kb_rag import MultiKBRAG
            rag = MultiKBRAG()
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –≤—Å–µ –∞–∫—Ç–∏–≤–Ω—ã–µ –ë–ó
            with st.spinner("–ó–∞–≥—Ä—É–∂–∞–µ–º –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π..."):
                rag.load_all_active_kbs()
            
            # –í—ã–ø–æ–ª–Ω—è–µ–º –ø–æ–∏—Å–∫
            with st.spinner("–ò—â–µ–º –æ—Ç–≤–µ—Ç –≤ –±–∞–∑–∞—Ö –∑–Ω–∞–Ω–∏–π..."):
                start_time = time.time()
                try:
                    search_result = rag.ask_question(question_text)
                    response_time = time.time() - start_time
                    
                    answer = search_result.get("answer", "")
                    sources = search_result.get("sources", [])
                    
                except Exception as e:
                    response_time = time.time() - start_time
                    answer = f"–û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞: {str(e)}"
                    sources = []
            
            # –û—Ç–æ–±—Ä–∞–∂–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            st.subheader("üìã –†–µ–∑—É–ª—å—Ç–∞—Ç —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è")
            
            col1, col2 = st.columns(2)
            
            with col1:
                st.metric("‚è±Ô∏è –í—Ä–µ–º—è –æ—Ç–≤–µ—Ç–∞", f"{response_time:.2f} —Å–µ–∫")
                st.metric("üìö –ù–∞–π–¥–µ–Ω–æ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤", len(sources))
            
            with col2:
                # –ü—Ä–æ—Å—Ç–∞—è –æ—Ü–µ–Ω–∫–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
                if expected_keywords:
                    found_keywords = sum(1 for keyword in expected_keywords 
                                       if keyword.lower() in answer.lower())
                    relevance_score = (found_keywords / len(expected_keywords)) * 100
                    st.metric("üéØ –†–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å", f"{relevance_score:.1f}%")
                else:
                    st.metric("üéØ –†–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å", "–ù–µ –æ—Ü–µ–Ω–µ–Ω–∞")
            
            # –û—Ç–≤–µ—Ç
            st.subheader("üí¨ –û—Ç–≤–µ—Ç —Å–∏—Å—Ç–µ–º—ã")
            if answer:
                st.write(answer)
            else:
                st.warning("–û—Ç–≤–µ—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω")
            
            # –ò—Å—Ç–æ—á–Ω–∏–∫–∏
            if sources:
                st.subheader("üìñ –ò—Å—Ç–æ—á–Ω–∏–∫–∏")
                for i, source in enumerate(sources[:3], 1):  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–µ 3
                    with st.expander(f"–ò—Å—Ç–æ—á–Ω–∏–∫ {i}: {source.get('source', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')}"):
                        st.write(f"**–°–æ–¥–µ—Ä–∂–∞–Ω–∏–µ:** {source.get('content', '')[:300]}...")
                        st.write(f"**–†–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å:** {source.get('score', 0):.3f}")
            
            # –û–∂–∏–¥–∞–µ–º—ã–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
            if expected_keywords:
                st.subheader("üîë –û–∂–∏–¥–∞–µ–º—ã–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞")
                found_keywords = []
                missing_keywords = []
                
                for keyword in expected_keywords:
                    if keyword.lower() in answer.lower():
                        found_keywords.append(f"‚úÖ {keyword}")
                    else:
                        missing_keywords.append(f"‚ùå {keyword}")
                
                if found_keywords:
                    st.success("–ù–∞–π–¥–µ–Ω–Ω—ã–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞:")
                    for keyword in found_keywords:
                        st.write(keyword)
                
                if missing_keywords:
                    st.error("–û—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞:")
                    for keyword in missing_keywords:
                        st.write(keyword)
            
        except Exception as e:
            st.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–∏ –≤–æ–ø—Ä–æ—Å–∞: {e}")
    
    def _show_relevance_testing_after_creation(self, kb_id: int, test_questions: List[Dict[str, Any]]):
        """–ü–æ–∫–∞–∑–∞—Ç—å —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ –ø–æ—Å–ª–µ —Å–æ–∑–¥–∞–Ω–∏—è/–æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –ë–ó"""
        st.markdown("---")
        st.subheader("üß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ —Å–æ–∑–¥–∞–Ω–Ω–æ–π –ë–ó")
        st.info(f"–¢–µ—Å—Ç–∏—Ä—É–µ–º –ë–ó ID {kb_id} —Å –ø–æ–º–æ—â—å—é {len(test_questions)} —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –≤–æ–ø—Ä–æ—Å–æ–≤")
        
        # –û—Ç–æ–±—Ä–∞–∂–∞–µ–º –≤–æ–ø—Ä–æ—Å—ã —Å –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å—é —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
        for i, question in enumerate(test_questions, 1):
            with st.expander(f"‚ùì –í–æ–ø—Ä–æ—Å {i}: {question['question']}"):
                st.write(f"**–ö–∞—Ç–µ–≥–æ—Ä–∏—è:** {question.get('category', '–ù–µ —É–∫–∞–∑–∞–Ω–∞')}")
                st.write(f"**–°–ª–æ–∂–Ω–æ—Å—Ç—å:** {question.get('difficulty', '–ù–µ —É–∫–∞–∑–∞–Ω–∞')}")
                st.write(f"**–û–∂–∏–¥–∞–µ–º—ã–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞:** {', '.join(question.get('expected_keywords', []))}")
                
                # –ö–Ω–æ–ø–∫–∞ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è —ç—Ç–æ–≥–æ –≤–æ–ø—Ä–æ—Å–∞ –Ω–∞ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –ë–ó
                if st.button(f"üîç –ü—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å –Ω–∞ –ë–ó ID {kb_id}", key=f"test_question_kb_{kb_id}_{i}"):
                    self._test_single_question_on_kb(question, kb_id)
    
    def _test_single_question_on_kb(self, question: Dict[str, Any], kb_id: int):
        """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –æ–¥–Ω–æ–≥–æ –≤–æ–ø—Ä–æ—Å–∞ –Ω–∞ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –ë–ó"""
        try:
            import streamlit as st
            import time
            
            question_text = question["question"]
            expected_keywords = question.get("expected_keywords", [])
            
            st.info(f"üîç –¢–µ—Å—Ç–∏—Ä—É–µ–º –≤–æ–ø—Ä–æ—Å –Ω–∞ –ë–ó ID {kb_id}: **{question_text}**")
            
            # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º RAG —Å–∏—Å—Ç–µ–º—É
            from ..rag.multi_kb_rag import MultiKBRAG
            rag = MultiKBRAG()
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–æ–ª—å–∫–æ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—É—é –ë–ó
            with st.spinner(f"–ó–∞–≥—Ä—É–∂–∞–µ–º –ë–ó ID {kb_id}..."):
                try:
                    # –ó–∞–≥—Ä—É–∂–∞–µ–º –≤—Å–µ –ë–ó, –Ω–æ –±—É–¥–µ–º –∏—Å–∫–∞—Ç—å —Ç–æ–ª—å–∫–æ –≤ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π
                    rag.load_all_active_kbs()
                    
                    # –ü–æ–ª—É—á–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ë–ó
                    kb_info = self.kb_manager.get_knowledge_base(kb_id)
                    if not kb_info:
                        st.error(f"–ë–ó ID {kb_id} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
                        return
                        
                except Exception as e:
                    st.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –ë–ó: {e}")
                    return
            
            # –í—ã–ø–æ–ª–Ω—è–µ–º –ø–æ–∏—Å–∫
            with st.spinner("–ò—â–µ–º –æ—Ç–≤–µ—Ç –≤ –ë–ó..."):
                start_time = time.time()
                try:
                    search_result = rag.ask_question(question_text)
                    response_time = time.time() - start_time
                    
                    answer = search_result.get("answer", "")
                    sources = search_result.get("sources", [])
                    
                    # –§–∏–ª—å—Ç—Ä—É–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫–∏ —Ç–æ–ª—å–∫–æ –ø–æ –Ω–∞—à–µ–π –ë–ó
                    kb_sources = [s for s in sources if s.get('kb_id') == kb_id]
                    
                except Exception as e:
                    response_time = time.time() - start_time
                    answer = f"–û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞: {str(e)}"
                    kb_sources = []
            
            # –û—Ç–æ–±—Ä–∞–∂–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            st.subheader("üìã –†–µ–∑—É–ª—å—Ç–∞—Ç —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è")
            
            col1, col2 = st.columns(2)
            
            with col1:
                st.metric("‚è±Ô∏è –í—Ä–µ–º—è –æ—Ç–≤–µ—Ç–∞", f"{response_time:.2f} —Å–µ–∫")
                st.metric("üìö –ù–∞–π–¥–µ–Ω–æ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –≤ –ë–ó", len(kb_sources))
            
            with col2:
                # –ü—Ä–æ—Å—Ç–∞—è –æ—Ü–µ–Ω–∫–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
                if expected_keywords:
                    found_keywords = sum(1 for keyword in expected_keywords 
                                       if keyword.lower() in answer.lower())
                    relevance_score = (found_keywords / len(expected_keywords)) * 100
                    st.metric("üéØ –†–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å", f"{relevance_score:.1f}%")
                else:
                    st.metric("üéØ –†–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å", "–ù–µ –æ—Ü–µ–Ω–µ–Ω–∞")
            
            # –û—Ç–≤–µ—Ç
            st.subheader("üí¨ –û—Ç–≤–µ—Ç —Å–∏—Å—Ç–µ–º—ã")
            if answer:
                st.write(answer)
            else:
                st.warning("–û—Ç–≤–µ—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ –¥–∞–Ω–Ω–æ–π –ë–ó")
            
            # –ò—Å—Ç–æ—á–Ω–∏–∫–∏ –∏–∑ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –ë–ó
            if kb_sources:
                st.subheader(f"üìñ –ò—Å—Ç–æ—á–Ω–∏–∫–∏ –∏–∑ –ë–ó ID {kb_id}")
                for i, source in enumerate(kb_sources[:3], 1):  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–µ 3
                    with st.expander(f"–ò—Å—Ç–æ—á–Ω–∏–∫ {i}: {source.get('source', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')}"):
                        st.write(f"**–°–æ–¥–µ—Ä–∂–∞–Ω–∏–µ:** {source.get('content', '')[:300]}...")
                        st.write(f"**–†–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å:** {source.get('score', 0):.3f}")
            else:
                st.warning(f"–í –ë–ó ID {kb_id} –Ω–µ –Ω–∞–π–¥–µ–Ω–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –¥–ª—è —ç—Ç–æ–≥–æ –≤–æ–ø—Ä–æ—Å–∞")
            
            # –û–∂–∏–¥–∞–µ–º—ã–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
            if expected_keywords:
                st.subheader("üîë –û–∂–∏–¥–∞–µ–º—ã–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞")
                found_keywords = []
                missing_keywords = []
                
                for keyword in expected_keywords:
                    if keyword.lower() in answer.lower():
                        found_keywords.append(f"‚úÖ {keyword}")
                    else:
                        missing_keywords.append(f"‚ùå {keyword}")
                
                if found_keywords:
                    st.success("–ù–∞–π–¥–µ–Ω–Ω—ã–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞:")
                    for keyword in found_keywords:
                        st.write(keyword)
                
                if missing_keywords:
                    st.error("–û—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞:")
                    for keyword in missing_keywords:
                        st.write(keyword)
            
        except Exception as e:
            st.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–∏ –≤–æ–ø—Ä–æ—Å–∞: {e}")
    
    def _save_test_questions_to_kb(self, kb_id: int, test_questions: List[Dict[str, Any]]):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ç–µ—Å—Ç–æ–≤—ã—Ö –≤–æ–ø—Ä–æ—Å–æ–≤ –≤ –ë–ó –∫–∞–∫ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ"""
        try:
            import json
            import logging
            
            logger = logging.getLogger(__name__)
            logger.info(f"–°–æ—Ö—Ä–∞–Ω—è–µ–º {len(test_questions)} —Ç–µ—Å—Ç–æ–≤—ã—Ö –≤–æ–ø—Ä–æ—Å–æ–≤ –≤ –ë–ó ID {kb_id}")
            
            # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
            test_questions_data = {
                "questions": test_questions,
                "created_at": datetime.now().isoformat(),
                "created_by": "smart_librarian",
                "version": "1.0",
                "description": "–¢–µ—Å—Ç–æ–≤—ã–µ –≤–æ–ø—Ä–æ—Å—ã –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ –ë–ó"
            }
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –ë–ó –∫–∞–∫ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
            success = self.kb_manager.update_knowledge_base_metadata(
                kb_id=kb_id,
                metadata_key="relevance_test_questions",
                metadata_value=json.dumps(test_questions_data, ensure_ascii=False, indent=2)
            )
            
            if success:
                logger.info(f"‚úÖ –¢–µ—Å—Ç–æ–≤—ã–µ –≤–æ–ø—Ä–æ—Å—ã —É—Å–ø–µ—à–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ –ë–ó ID {kb_id}")
                st.success(f"üíæ –¢–µ—Å—Ç–æ–≤—ã–µ –≤–æ–ø—Ä–æ—Å—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ –ë–ó ID {kb_id} –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –¥—Ä—É–≥–∏–º–∏ —Å–∏—Å—Ç–µ–º–∞–º–∏")
            else:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ç–µ—Å—Ç–æ–≤—ã—Ö –≤–æ–ø—Ä–æ—Å–æ–≤ –≤ –ë–ó ID {kb_id}")
                st.warning("‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å —Ç–µ—Å—Ç–æ–≤—ã–µ –≤–æ–ø—Ä–æ—Å—ã –≤ –ë–ó")
                
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ —Ç–µ—Å—Ç–æ–≤—ã—Ö –≤–æ–ø—Ä–æ—Å–æ–≤: {e}")
            st.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ —Ç–µ—Å—Ç–æ–≤—ã—Ö –≤–æ–ø—Ä–æ—Å–æ–≤: {e}")
    
    def get_test_questions_from_kb(self, kb_id: int) -> List[Dict[str, Any]]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Ç–µ—Å—Ç–æ–≤—ã—Ö –≤–æ–ø—Ä–æ—Å–æ–≤ –∏–∑ –ë–ó"""
        try:
            import json
            import logging
            
            logger = logging.getLogger(__name__)
            
            # –ü–æ–ª—É—á–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –ë–ó
            metadata = self.kb_manager.get_knowledge_base_metadata(kb_id, "relevance_test_questions")
            
            if "relevance_test_questions" in metadata:
                test_questions_data = json.loads(metadata["relevance_test_questions"])
                questions = test_questions_data.get("questions", [])
                logger.info(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(questions)} —Ç–µ—Å—Ç–æ–≤—ã—Ö –≤–æ–ø—Ä–æ—Å–æ–≤ –∏–∑ –ë–ó ID {kb_id}")
                return questions
            else:
                logger.info(f"–í –ë–ó ID {kb_id} –Ω–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã—Ö —Ç–µ—Å—Ç–æ–≤—ã—Ö –≤–æ–ø—Ä–æ—Å–æ–≤")
                return []
                
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ —Ç–µ—Å—Ç–æ–≤—ã—Ö –≤–æ–ø—Ä–æ—Å–æ–≤ –∏–∑ –ë–ó: {e}")
            return []

    def _is_image_noise(self, image_path: Path) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –Ω–∞ –º—É—Å–æ—Ä/–ø—É—Å—Ç–æ—Ç—É.
        –ö—Ä–∏—Ç–µ—Ä–∏–∏ (–ª—é–±–æ–π –∏–∑):
        - –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä —Å—Ç–æ—Ä–æ–Ω—ã < 200 px
        - –†–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞ < 8 KB
        - > 80% –æ—á–µ–Ω—å —Ç–µ–º–Ω—ã—Ö –ø–∏–∫—Å–µ–ª–µ–π –∏–ª–∏ > 95% –ø–æ—á—Ç–∏ –±–µ–ª—ã—Ö –ø–∏–∫—Å–µ–ª–µ–π
        - –ù–∏–∑–∫–∞—è –¥–∏—Å–ø–µ—Ä—Å–∏—è —è—Ä–∫–æ—Å—Ç–∏ (stddev < 10)
        - –ò–º—è —Ñ–∞–π–ª–∞ —Å–æ–¥–µ—Ä–∂–∏—Ç logo/icon
        - –û—á–µ–Ω—å –º–∞–ª–µ–Ω—å–∫–∞—è –ø–ª–æ—â–∞–¥—å (< 40k px) –∏–ª–∏ —ç–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω—ã–µ –ø—Ä–æ–ø–æ—Ä—Ü–∏–∏ (AR > 5:1 –∏–ª–∏ < 1:5)
        - –ù–∏–∑–∫–∞—è —Ü–≤–µ—Ç–æ–≤–∞—è –≤–∞—Ä–∏–∞—Ç–∏–≤–Ω–æ—Å—Ç—å (–ø–æ—Å–ª–µ –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏—è <= 16 —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ü–≤–µ—Ç–æ–≤)
        """
        try:
            from PIL import Image, ImageStat
            stat = image_path.stat()
            if stat.st_size < 8 * 1024:
                return True
            # –Ø–≤–Ω—ã–µ –ª–æ–≥–æ—Ç–∏–ø—ã –ø–æ –∏–º–µ–Ω–∏
            name_l = image_path.name.lower()
            if any(tag in name_l for tag in ("logo", "icon", "favicon", "brand")):
                return True
            img = Image.open(image_path).convert('L')
            width, height = img.size
            # –ü–æ—Ä–æ–≥ –ø–æ –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–π —Å—Ç–æ—Ä–æ–Ω–µ –∏ –ø–ª–æ—â–∞–¥–∏
            if min(width, height) < 200:
                return True
            if (width * height) < (200 * 200):
                return True
            # –≠–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω—ã–µ –ø—Ä–æ–ø–æ—Ä—Ü–∏–∏ (—á–∞—Å—Ç–æ –ø–æ–ª–æ—Å–∫–∏/–±–∞–Ω–Ω–µ—Ä—ã –±–µ–∑ —Ç–µ–∫—Å—Ç–∞)
            ar = width / height if height else 9999
            if ar > 5.0 or ar < 0.2:
                return True
            histogram = img.histogram()
            total = sum(histogram)
            white_pct = (sum(histogram[245:256]) / total * 100) if total else 0.0
            black_pct = (sum(histogram[0:10]) / total * 100) if total else 0.0
            stat_b = ImageStat.Stat(img)
            stddev = stat_b.stddev[0]
            if black_pct > 80.0 or white_pct > 95.0 or stddev < 10.0:
                return True
            # –ù–∏–∑–∫–∞—è —Ü–≤–µ—Ç–æ–≤–∞—è –≤–∞—Ä–∏–∞—Ç–∏–≤–Ω–æ—Å—Ç—å (–ø–æ –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏—é –¥–æ 64 —Ü–≤–µ—Ç–æ–≤)
            try:
                img_rgb = Image.open(image_path).convert('RGB')
                pal = img_rgb.convert('P', palette=Image.ADAPTIVE, colors=64)
                colors = pal.getcolors(1_000_000) or []
                unique_colors = len(colors)
                if unique_colors <= 16:
                    return True
            except Exception:
                pass
            return False
        except Exception:
            # –í —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏ –Ω–µ —Ñ–∏–ª—å—Ç—Ä—É–µ–º
            return False
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
        analysis['recommendations'] = self._generate_recommendations(analysis)
        
        # –ü—Ä–µ–¥–ª–∞–≥–∞–µ–º —Ç–µ–º—É KB —Å —É—á–µ—Ç–æ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö
        kb_suggestion = self._suggest_kb_topic(analysis)
        analysis['suggested_kb'] = kb_suggestion
        
        return analysis

    def _fallback_analysis(self, file_path: Path) -> Dict:
        """–†–µ–∑–µ—Ä–≤–Ω—ã–π –∞–Ω–∞–ª–∏–∑, –∫–æ–≥–¥–∞ –æ—Å–Ω–æ–≤–Ω–æ–π —É–ø–∞–ª: –∏–∑–≤–ª–µ–∫–∞–µ—Ç —Ç–µ–∫—Å—Ç –∏ —Å—Ç—Ä–æ–∏—Ç –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π —Å–ª–æ–≤–∞—Ä—å."""
        try:
            text = self._extract_text_content(file_path)
        except Exception:
            text = ""
        qa_pairs = []
        try:
            qa_pairs = self._extract_qa_pairs(text)
        except Exception:
            qa_pairs = []
        return {
            'file_path': str(file_path),
            'file_name': file_path.name,
            'file_size': file_path.stat().st_size if file_path.exists() else 0,
            'file_extension': file_path.suffix.lower(),
            'mime_type': mimetypes.guess_type(str(file_path))[0],
            'content_type': 'unknown',
            'category': '–î—Ä—É–≥–æ–µ',
            'description': '',
            'keywords': [],
            'confidence': 0.0,
            'format_supported': True,
            'format_description': self.supported_formats.get(file_path.suffix.lower(), '–î–æ–∫—É–º–µ–Ω—Ç'),
            'raw_ocr_text': text,
            'original_cleaned_text': self._get_original_cleaned_text(text),
            'full_cleaned_text': self._get_original_cleaned_text(text),
            'smart_summary': self._create_smart_summary(text, file_path.name),
            'text_preview': (text[:500] + '...') if len(text) > 500 else text,
            'text_length': len(text),
            'images': [],
            'image_count': 0,
            'gemini_analysis': None,
            'recommendations': [],
            'qa_pairs': qa_pairs,
            'qa_count': len(qa_pairs)
        }
    
    def _extract_text_content(self, file_path: Path) -> str:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π OCR"""
        try:
            if file_path.suffix.lower() == '.pdf':
                # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–±—É–µ–º –æ–±—ã—á–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ
                text = self.pdf_processor.extract_text(str(file_path))
                
                # –ï—Å–ª–∏ —Ç–µ–∫—Å—Ç–∞ –º–∞–ª–æ, –ø—Ä–æ–±—É–µ–º OCR
                if len(text.strip()) < 100:
                    try:
                        from modules.documents.ocr_processor import OCRProcessor
                        ocr_processor = OCRProcessor()
                        ocr_result = ocr_processor.process_document(str(file_path))
                        if ocr_result['success'] and ocr_result.get('text_content'):
                            text = ocr_result['text_content']
                            st.info(f"üîç –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω OCR –¥–ª—è {file_path.name}")
                    except Exception as ocr_e:
                        st.warning(f"OCR –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω: {ocr_e}")
                
                return text
                
            elif file_path.suffix.lower() in ['.png', '.jpg', '.jpeg', '.tiff', '.bmp']:
                # –î–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏—Å–ø–æ–ª—å–∑—É–µ–º OCR
                try:
                    from modules.documents.ocr_processor import OCRProcessor
                    ocr_processor = OCRProcessor()
                    ocr_result = ocr_processor.process_document(str(file_path))
                    if ocr_result['success'] and ocr_result.get('text_content'):
                        return ocr_result['text_content']
                    else:
                        return ""
                except Exception as ocr_e:
                    st.warning(f"OCR –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω –¥–ª—è {file_path.name}: {ocr_e}")
                    return ""
                    
            elif file_path.suffix.lower() in ['.docx', '.doc']:
                # –£–ª—É—á—à–µ–Ω–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ Word –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
                try:
                    from docx import Document
                    logger = logging.getLogger(__name__)
                    logger.info(f"DOCX: –æ—Ç–∫—Ä—ã–≤–∞–µ–º —Ñ–∞–π–ª {file_path}")
                    doc = Document(file_path)
                    try:
                        logger.info(f"DOCX: sections={len(getattr(doc, 'sections', []))}, paragraphs={len(getattr(doc, 'paragraphs', []))}, tables={len(getattr(doc, 'tables', []))}")
                    except Exception:
                        pass
                    text_parts = []
                    
                    def _norm_text(s: str) -> str:
                        import re
                        s = re.sub(r'\s+', ' ', s or '').strip()
                        return s
                    
                    # –ó–∞–≥–æ–ª–æ–≤–∫–∏/–∫–æ–ª–æ–Ω—Ç–∏—Ç—É–ª—ã (–µ—Å–ª–∏ –µ—Å—Ç—å)
                    try:
                        for section in doc.sections:
                            header_text = _norm_text(getattr(section.header, 'text', '') if hasattr(section, 'header') else '')
                            footer_text = _norm_text(getattr(section.footer, 'text', '') if hasattr(section, 'footer') else '')
                            if header_text:
                                text_parts.append(header_text)
                            if footer_text:
                                text_parts.append(footer_text)
                    except Exception:
                        pass
                    
                    # –ü–∞—Ä–∞–≥—Ä–∞—Ñ—ã (—É—á–µ—Ç —Å–ø–∏—Å–∫–∞ –ø–æ bullet/numbering —á–µ—Ä–µ–∑ tabs)
                    for paragraph in getattr(doc, 'paragraphs', []) or []:
                        try:
                            p = _norm_text(getattr(paragraph, 'text', '') or '')
                            if not p:
                                continue
                            # –ü—Ä–∏–∑–Ω–∞–∫–∏ —Å–ø–∏—Å–∫–∞: —É—Ä–æ–≤–µ–Ω—å –æ—Ç—Å—Ç—É–ø–∞/–Ω—É–º–µ—Ä–∞—Ü–∏–∏ –Ω–µ —Ç—Ä–æ–≥–∞–µ–º, –ø—Ä–æ—Å—Ç–æ –¥–æ–±–∞–≤–∏–º –º–∞—Ä–∫–µ—Ä
                            try:
                                style = getattr(paragraph, 'style', None)
                                style_name = getattr(style, 'name', '') if style is not None else ''
                                if style_name and 'List' in str(style_name):
                                    p = f"- {p}"
                            except Exception:
                                pass
                            text_parts.append(p)
                        except Exception as para_e:
                            try:
                                logger.warning(f"DOCX: –æ—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è –ø–∞—Ä–∞–≥—Ä–∞—Ñ–∞: {para_e}")
                            except Exception:
                                pass
                    
                    # –¢–∞–±–ª–∏—Ü—ã
                    for table in getattr(doc, 'tables', []) or []:
                        try:
                            for row in getattr(table, 'rows', []) or []:
                                row_text = []
                                for cell in getattr(row, 'cells', []) or []:
                                    try:
                                        cell_text = _norm_text(getattr(cell, 'text', '') or '')
                                        if cell_text:
                                            row_text.append(cell_text)
                                    except Exception:
                                        continue
                                if row_text:
                                    text_parts.append(' | '.join(row_text))
                        except Exception as tbl_e:
                            try:
                                logger.warning(f"DOCX: –æ—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è —Ç–∞–±–ª–∏—Ü—ã: {tbl_e}")
                            except Exception:
                                pass
                    
                    # –ò—Ç–æ–≥–æ–≤–∞—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
                    content = '\n'.join([t for t in text_parts if t])
                    logger.info(f"DOCX: –∏–∑–≤–ª–µ—á–µ–Ω–æ {len(content)} —Å–∏–º–≤–æ–ª–æ–≤ —Ç–µ–∫—Å—Ç–∞ –∏–∑ {file_path.name}")
                    return content
                except Exception as docx_e:
                    try:
                        logger = logging.getLogger(__name__)
                        logger.error(f"DOCX: –æ—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ {file_path.name}: {docx_e}")
                    except Exception:
                        pass
                    st.warning(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ DOCX {file_path.name}: {docx_e}")
                    return ""
                    
            elif file_path.suffix.lower() == '.txt':
                # TXT —Å —Ä–µ–∑–µ—Ä–≤–Ω—ã–º–∏ –∫–æ–¥–∏—Ä–æ–≤–∫–∞–º–∏
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        return f.read()
                except Exception:
                    for enc in ['cp1251', 'latin-1']:
                        try:
                            with open(file_path, 'r', encoding=enc, errors='ignore') as f:
                                return f.read()
                        except Exception:
                            continue
                    return ""
            elif file_path.suffix.lower() == '.md':
                with open(file_path, 'r', encoding='utf-8') as f:
                    return f.read()
            elif file_path.suffix.lower() == '.rtf':
                # –û–±—Ä–∞–±–æ—Ç–∫–∞ RTF —Ñ–∞–π–ª–æ–≤
                try:
                    import re
                    with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                        content = f.read()
                        # –ü—Ä–æ—Å—Ç–∞—è –æ—á–∏—Å—Ç–∫–∞ RTF —Ä–∞–∑–º–µ—Ç–∫–∏
                        content = re.sub(r'\\[a-z]+\d*\s?', '', content)
                        content = re.sub(r'[{}]', '', content)
                        return content.strip()
                except Exception as rtf_e:
                    st.warning(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ RTF {file_path.name}: {rtf_e}")
                    return ""
            elif file_path.suffix.lower() == '.html':
                # –û–±—Ä–∞–±–æ—Ç–∫–∞ HTML —Ñ–∞–π–ª–æ–≤
                try:
                    from bs4 import BeautifulSoup
                    with open(file_path, 'r', encoding='utf-8') as f:
                        content = f.read()
                        soup = BeautifulSoup(content, 'html.parser')
                        return soup.get_text()
                except Exception as html_e:
                    st.warning(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ HTML {file_path.name}: {html_e}")
                    # Fallback - –ø—Ä–æ—Å—Ç–æ–µ —É–¥–∞–ª–µ–Ω–∏–µ —Ç–µ–≥–æ–≤
                    try:
                        import re
                        with open(file_path, 'r', encoding='utf-8') as f:
                            content = f.read()
                            content = re.sub(r'<[^>]+>', '', content)
                            return content.strip()
                    except:
                        return ""
            elif file_path.suffix.lower() == '.xml':
                # –û–±—Ä–∞–±–æ—Ç–∫–∞ XML —Ñ–∞–π–ª–æ–≤
                try:
                    from bs4 import BeautifulSoup
                    with open(file_path, 'r', encoding='utf-8') as f:
                        content = f.read()
                        soup = BeautifulSoup(content, 'xml')
                        return soup.get_text()
                except Exception as xml_e:
                    st.warning(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ XML {file_path.name}: {xml_e}")
                    return ""
            else:
                # –î–ª—è –¥—Ä—É–≥–∏—Ö —Ñ–æ—Ä–º–∞—Ç–æ–≤ –ø–æ–∫–∞ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—É—Å—Ç—É—é —Å—Ç—Ä–æ–∫—É
                return ""
        except Exception as e:
            st.warning(f"–û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ –∏–∑ {file_path.name}: {e}")
            return ""
    
    def _clean_ocr_text(self, text: str) -> str:
        """–û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ –æ—Ç –º—É—Å–æ—Ä–∞ OCR —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –∞–≥–µ–Ω—Ç–∞ –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏"""
        if not text:
            return ""
        
        import re
        import logging
        
        logger = logging.getLogger(__name__)
        logger.info(f"–ù–∞—á–∏–Ω–∞–µ–º –æ—á–∏—Å—Ç–∫—É OCR —Ç–µ–∫—Å—Ç–∞ –≤ SmartLibrarian, –¥–ª–∏–Ω–∞: {len(text)}")
        
        # –£–¥–∞–ª—è–µ–º –Ω–æ–º–µ—Ä–∞ —Å—Ç—Ä–∞–Ω–∏—Ü
        text = re.sub(r'---\s*–°—Ç—Ä–∞–Ω–∏—Ü–∞\s+\d+\s*---', '', text)
        
        # –£–¥–∞–ª—è–µ–º –ø–æ–≤—Ç–æ—Ä—è—é—â–∏–µ—Å—è —Å–∏–º–≤–æ–ª—ã (–±–æ–ª–µ–µ 3 –ø–æ–¥—Ä—è–¥)
        text = re.sub(r'(.)\1{3,}', r'\1', text)
        
        # –£–¥–∞–ª—è–µ–º –∫—Ä–∏–≤—ã–µ —Å–∏–º–≤–æ–ª—ã –∏ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç—ã OCR - –±–æ–ª–µ–µ –∞–≥—Ä–µ—Å—Å–∏–≤–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞
        # –£–¥–∞–ª—è–µ–º —Å—Ç—Ä–æ–∫–∏ —Å –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–º–∏ –ø–æ–≤—Ç–æ—Ä—è—é—â–∏–º–∏—Å—è —Å–∏–º–≤–æ–ª–∞–º–∏
        text = re.sub(r'^[^\w\s]{5,}$', '', text, flags=re.MULTILINE)
        
        # –£–¥–∞–ª—è–µ–º —Å—Ç—Ä–æ–∫–∏ —Å–æ—Å—Ç–æ—è—â–∏–µ —Ç–æ–ª—å–∫–æ –∏–∑ —Å–∏–º–≤–æ–ª–æ–≤ =, -, _, +, *, ~, #, $, %, ^, &, *, (, ), [, ], {, }, |, \, :, ;, ", ', <, >, ?, /, ., ,, !, @
        text = re.sub(r'^[=\-_+*~#$%^&*()\[\]{}|\\:;"\'<>,.?!/@\s]{3,}$', '', text, flags=re.MULTILINE)
        
        # –£–¥–∞–ª—è–µ–º –æ–¥–∏–Ω–æ—á–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã –≤ –Ω–∞—á–∞–ª–µ —Å—Ç—Ä–æ–∫–∏ (–∞—Ä—Ç–µ—Ñ–∞–∫—Ç—ã OCR)
        text = re.sub(r'^\s*[^\w\s]\s*$', '', text, flags=re.MULTILINE)
        
        # –£–¥–∞–ª—è–µ–º —Å—Ç—Ä–æ–∫–∏ —Å –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–º–∏ –ø—Ä–æ–±–µ–ª–∞–º–∏ –∏ —Å–∏–º–≤–æ–ª–∞–º–∏
        text = re.sub(r'^\s*[\s\W]{15,}\s*$', '', text, flags=re.MULTILINE)
        
        # –£–¥–∞–ª—è–µ–º –∫—Ä–∞–∫–æ–∑—è–±—Ä—ã –≤ —Å–µ—Ä–µ–¥–∏–Ω–µ —Ç–µ–∫—Å—Ç–∞ (–ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∏–∑ 3+ –Ω–µ–±—É–∫–≤–µ–Ω–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤)
        text = re.sub(r'[^\w\s]{3,}', ' ', text)
        
        # –£–¥–∞–ª—è–µ–º –∫—Ä–∏–≤—ã–µ —Å–∏–º–≤–æ–ª—ã —Ç–∏–ø–∞ "oe ee ee eS a" - –∫–æ—Ä–æ—Ç–∫–∏–µ –±–µ—Å—Å–º—ã—Å–ª–µ–Ω–Ω—ã–µ –∫–æ–º–±–∏–Ω–∞—Ü–∏–∏
        # –£–¥–∞–ª—è–µ–º —Å—Ç—Ä–æ–∫–∏ –∏–∑ 2-3 —Å–∏–º–≤–æ–ª–æ–≤, —Å–æ—Å—Ç–æ—è—â–∏–µ —Ç–æ–ª—å–∫–æ –∏–∑ –±—É–∫–≤ –±–µ–∑ –ø—Ä–æ–±–µ–ª–æ–≤
        text = re.sub(r'^\s*[a-zA-Z–∞-—è–ê-–Ø]{1,3}\s*$', '', text, flags=re.MULTILINE)
        
        # –£–¥–∞–ª—è–µ–º —Å—Ç—Ä–æ–∫–∏ —Ç–∏–ø–∞ "eS a", "oe ee" - –∫–æ—Ä–æ—Ç–∫–∏–µ –∫–æ–º–±–∏–Ω–∞—Ü–∏–∏ –±—É–∫–≤ —Å –ø—Ä–æ–±–µ–ª–∞–º–∏
        text = re.sub(r'^\s*[a-zA-Z–∞-—è–ê-–Ø]{1,2}\s+[a-zA-Z–∞-—è–ê-–Ø]{1,2}\s*$', '', text, flags=re.MULTILINE)
        
        # –£–¥–∞–ª—è–µ–º —Å—Ç—Ä–æ–∫–∏ –∏–∑ –ø–æ–≤—Ç–æ—Ä—è—é—â–∏—Ö—Å—è –∫–æ—Ä–æ—Ç–∫–∏—Ö –±—É–∫–≤–µ–Ω–Ω—ã—Ö –∫–æ–º–±–∏–Ω–∞—Ü–∏–π
        text = re.sub(r'^\s*([a-zA-Z–∞-—è–ê-–Ø]{1,2}\s+){2,}[a-zA-Z–∞-—è–ê-–Ø]{1,2}\s*$', '', text, flags=re.MULTILINE)
        
        # –£–¥–∞–ª—è–µ–º —Å—Ç—Ä–æ–∫–∏ —Å–æ—Å—Ç–æ—è—â–∏–µ —Ç–æ–ª—å–∫–æ –∏–∑ –æ–¥–∏–Ω–æ—á–Ω—ã—Ö –±—É–∫–≤ –∏ –ø—Ä–æ–±–µ–ª–æ–≤
        text = re.sub(r'^\s*([a-zA-Z–∞-—è–ê-–Ø]\s+){3,}[a-zA-Z–∞-—è–ê-–Ø]\s*$', '', text, flags=re.MULTILINE)
        
        # –£–¥–∞–ª—è–µ–º –∫—Ä–∞–∫–æ–∑—è–±—Ä—ã —Ç–∏–ø–∞ "epee na —Å–≤—è–∑–∏. pepe –∏—è" - –∫–æ—Ä–æ—Ç–∫–∏–µ —Å–ª–æ–≤–∞ —Å —Ç–æ—á–∫–∞–º–∏
        text = re.sub(r'\b[a-zA-Z–∞-—è–ê-–Ø]{2,4}\s+[a-zA-Z–∞-—è–ê-–Ø]{2,4}\s*\.\s*[a-zA-Z–∞-—è–ê-–Ø]{2,4}\s+[a-zA-Z–∞-—è–ê-–Ø]{2,4}\b', '', text)
        
        # –£–¥–∞–ª—è–µ–º –∫—Ä–∞–∫–æ–∑—è–±—Ä—ã —Ç–∏–ø–∞ "–û –±—Ä –û–û–ù es 5 –ï–ï –ï–ï–ï–í–ï–ï–ï–†–´–ï–ï–†–ï–ï–ï" - —Å–º–µ—à–∞–Ω–Ω—ã–µ –±—É–∫–≤—ã –∏ —Ü–∏—Ñ—Ä—ã
        text = re.sub(r'\b[–∞-—è–ê-–Ø]{1,2}\s+[–∞-—è–ê-–Ø]{1,3}\s+[–∞-—è–ê-–Ø]{1,3}\s+[a-zA-Z]{1,2}\s+\d+\s+[–∞-—è–ê-–Ø]{1,2}\s+[–∞-—è–ê-–Ø]{10,}\b', '', text)
        
        # –£–¥–∞–ª—è–µ–º –∫—Ä–∞–∫–æ–∑—è–±—Ä—ã —Ç–∏–ø–∞ "ee ae oe ae oe eo se ee Se SSS Sa" - –ø–æ–≤—Ç–æ—Ä—è—é—â–∏–µ—Å—è –∫–æ—Ä–æ—Ç–∫–∏–µ –±—É–∫–≤—ã
        text = re.sub(r'\b([a-zA-Z]{1,2}\s+){5,}[a-zA-Z]{1,2}\b', '', text)
        
        # –£–¥–∞–ª—è–µ–º –∫—Ä–∞–∫–æ–∑—è–±—Ä—ã —Ç–∏–ø–∞ "–µ–µ–µ –°–û–û–¢–í–ï–¢–°–¢–í–£–Æ–¢" - –∫–æ—Ä–æ—Ç–∫–∏–µ –±—É–∫–≤—ã –ø–µ—Ä–µ–¥ –Ω–æ—Ä–º–∞–ª—å–Ω—ã–º–∏ —Å–ª–æ–≤–∞–º–∏
        text = re.sub(r'\b[–∞-—è–ê-–Ø]{1,3}\s+([–ê-–Ø]{5,})\b', r'\1', text)
        
        # –£–¥–∞–ª—è–µ–º –∫—Ä–∞–∫–æ–∑—è–±—Ä—ã —Ç–∏–ø–∞ "–í–∞—é–º–µ–Ω–æ–≤—è–Ω–∏–µ –∏–æ–≥–æ—Ç–æ–≤–∏—Ç–µ–ª—è" - –∏—Å–∫–∞–∂–µ–Ω–Ω—ã–µ —Å–ª–æ–≤–∞
        text = re.sub(r'\b[–∞-—è–ê-–Ø]{8,}–∏[–∞-—è–ê-–Ø]{8,}\b', '', text)
        
        # –£–¥–∞–ª—è–µ–º –∫—Ä–∞–∫–æ–∑—è–±—Ä—ã —Ç–∏–ø–∞ "–∑–∞–≤–æ–ª–∞] - –∏–∑–≥–æ—Ç–æ–≤–∏—Ç–µ–ª—è" - —Å–ª–æ–≤–∞ —Å –ª–∏—à–Ω–∏–º–∏ —Å–∏–º–≤–æ–ª–∞–º–∏
        text = re.sub(r'\b[–∞-—è–ê-–Ø]{5,}[\]\)]\s*-\s*[–∞-—è–ê-–Ø]{5,}\b', '', text)
        
        # –î–û–ü–û–õ–ù–ò–¢–ï–õ–¨–ù–´–ï –ü–†–ê–í–ò–õ–ê –î–õ–Ø –ö–û–ù–ö–†–ï–¢–ù–´–• –ö–†–ê–ö–û–ó–Ø–ë–†:
        
        # –£–¥–∞–ª—è–µ–º "epee na —Å–≤—è–∑–∏. pepe –∏—è" - –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π –ø–∞—Ç—Ç–µ—Ä–Ω
        text = re.sub(r'\bepee\s+na\s+—Å–≤—è–∑–∏\.\s+pepe\s+–∏—è\b', '', text)
        
        # –£–¥–∞–ª—è–µ–º "–û –±—Ä –û–û–ù es 5 –ï–ï –ï–ï–ï–í–ï–ï–ï–†–´–ï–ï–†–ï–ï–ï" - –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π –ø–∞—Ç—Ç–µ—Ä–Ω
        text = re.sub(r'\b–û\s+–±—Ä\s+–û–û–ù\s+es\s+5\s+–ï–ï\s+–ï–ï–ï–í–ï–ï–ï–†–´–ï–ï–†–ï–ï–ï\b', '', text)
        
        # –£–¥–∞–ª—è–µ–º "ee ae oe ae oe eo se ee Se SSS Sa" - –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π –ø–∞—Ç—Ç–µ—Ä–Ω
        text = re.sub(r'\bee\s+ae\s+oe\s+ae\s+oe\s+eo\s+se\s+ee\s+Se\s+SSS\s+Sa\b', '', text)
        
        # –£–¥–∞–ª—è–µ–º "–í–∞—é–º–µ–Ω–æ–≤—è–Ω–∏–µ –∏–æ–≥–æ—Ç–æ–≤–∏—Ç–µ–ª—è" - –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π –ø–∞—Ç—Ç–µ—Ä–Ω
        text = re.sub(r'\b–í–∞—é–º–µ–Ω–æ–≤—è–Ω–∏–µ\s+–∏–æ–≥–æ—Ç–æ–≤–∏—Ç–µ–ª—è\b', '', text)
        
        # –£–¥–∞–ª—è–µ–º "–∑–∞–≤–æ–ª–∞] - –∏–∑–≥–æ—Ç–æ–≤–∏—Ç–µ–ª—è" - –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π –ø–∞—Ç—Ç–µ—Ä–Ω
        text = re.sub(r'\b–∑–∞–≤–æ–ª–∞\]\s*-\s*–∏–∑–≥–æ—Ç–æ–≤–∏—Ç–µ–ª—è\b', '', text)
        
        # –£–¥–∞–ª—è–µ–º "–µ–µ–µ –°–û–û–¢–í–ï–¢–°–¢–í–£–Æ–¢" - –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π –ø–∞—Ç—Ç–µ—Ä–Ω
        text = re.sub(r'\b–µ–µ–µ\s+–°–û–û–¢–í–ï–¢–°–¢–í–£–Æ–¢\b', '–°–û–û–¢–í–ï–¢–°–¢–í–£–Æ–¢', text)
        
        # –£–¥–∞–ª—è–µ–º "–Ω–µ—Ö —É –∂–µ =: :" - –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π –ø–∞—Ç—Ç–µ—Ä–Ω
        text = re.sub(r'\b–Ω–µ—Ö\s+—É\s+–∂–µ\s*=\s*:\s*:\s*', '', text)
        
        # –£–¥–∞–ª—è–µ–º "SSS Sa" - –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π –ø–∞—Ç—Ç–µ—Ä–Ω
        text = re.sub(r'\bSSS\s+Sa\b', '', text)
        
        # –£–±–∏—Ä–∞–µ–º –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –ø—Ä–æ–±–µ–ª—ã
        text = re.sub(r'\s+', ' ', text)
        
        # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –ø–µ—Ä–µ–Ω–æ—Å—ã —Å—Ç—Ä–æ–∫
        text = re.sub(r'\n\s*\n', '\n', text)
        
        # –£–¥–∞–ª—è–µ–º –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏
        text = re.sub(r'\n\s*\n', '\n', text)
        
        cleaned_text = text.strip()
        logger.info(f"–ü–æ—Å–ª–µ –±–∞–∑–æ–≤–æ–π –æ—á–∏—Å—Ç–∫–∏, –¥–ª–∏–Ω–∞: {len(cleaned_text)}")
        
        # –í—Å–µ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑—É–µ–º –∞–≥–µ–Ω—Ç–∞ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —á–∏—Å—Ç–æ–≥–æ abstract –∏–∑ OCR —Ç–µ–∫—Å—Ç–∞
        if len(cleaned_text) > 50:  # –¢–æ–ª—å–∫–æ –¥–ª—è –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–ª–∏–Ω–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤
            logger.info("–ò—Å–ø–æ–ª—å–∑—É–µ–º –∞–≥–µ–Ω—Ç–∞ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —á–∏—Å—Ç–æ–≥–æ abstract –∏–∑ OCR —Ç–µ–∫—Å—Ç–∞")
            try:
                abstract = self._create_ocr_cleaning_agent(cleaned_text)
                logger.info(f"–ê–≥–µ–Ω—Ç –æ—á–∏—Å—Ç–∫–∏ —Å–æ–∑–¥–∞–ª abstract, –¥–ª–∏–Ω–∞: {len(abstract)}")
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏—Å—Ö–æ–¥–Ω—ã–π –æ—á–∏—â–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç –≤ session_state –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–µ
                if 'original_cleaned_text' not in st.session_state:
                    st.session_state['original_cleaned_text'] = cleaned_text
                
                return abstract
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –∞–≥–µ–Ω—Ç–∞ –æ—á–∏—Å—Ç–∫–∏: {e}")
                return cleaned_text
        else:
            logger.info("–¢–µ–∫—Å—Ç —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –∞–≥–µ–Ω—Ç–∞")
        
        return cleaned_text
    
    def _get_original_cleaned_text(self, text: str) -> str:
        """–ü–æ–ª—É—á–∏—Ç—å —Ç–æ–ª—å–∫–æ –æ—á–∏—â–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç –±–µ–∑ —Å–æ–∑–¥–∞–Ω–∏—è –∞–±—Å—Ç—Ä–∞–∫—Ç–∞"""
        if not text:
            return ""
        
        import re
        import logging
        
        logger = logging.getLogger(__name__)
        logger.info(f"–°–æ–∑–¥–∞–µ–º –∏—Å—Ö–æ–¥–Ω—ã–π –æ—á–∏—â–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç, –¥–ª–∏–Ω–∞: {len(text)}")
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º —Ç—É –∂–µ –±–∞–∑–æ–≤—É—é –æ—á–∏—Å—Ç–∫—É, —á—Ç–æ –∏ –≤ _clean_ocr_text, –Ω–æ –ë–ï–ó –∞–≥–µ–Ω—Ç–∞
        # –£–¥–∞–ª—è–µ–º –Ω–æ–º–µ—Ä–∞ —Å—Ç—Ä–∞–Ω–∏—Ü
        text = re.sub(r'---\s*–°—Ç—Ä–∞–Ω–∏—Ü–∞\s+\d+\s*---', '', text)
        
        # –£–¥–∞–ª—è–µ–º –ø–æ–≤—Ç–æ—Ä—è—é—â–∏–µ—Å—è —Å–∏–º–≤–æ–ª—ã (–±–æ–ª–µ–µ 3 –ø–æ–¥—Ä—è–¥)
        text = re.sub(r'(.)\1{3,}', r'\1', text)
        
        # –£–¥–∞–ª—è–µ–º –∫—Ä–∏–≤—ã–µ —Å–∏–º–≤–æ–ª—ã –∏ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç—ã OCR - –±–æ–ª–µ–µ –∞–≥—Ä–µ—Å—Å–∏–≤–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞
        # –£–¥–∞–ª—è–µ–º —Å—Ç—Ä–æ–∫–∏ —Å –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–º–∏ –ø–æ–≤—Ç–æ—Ä—è—é—â–∏–º–∏—Å—è —Å–∏–º–≤–æ–ª–∞–º–∏
        text = re.sub(r'^[^\w\s]{5,}$', '', text, flags=re.MULTILINE)
        
        # –£–¥–∞–ª—è–µ–º —Å—Ç—Ä–æ–∫–∏ —Å–æ—Å—Ç–æ—è—â–∏–µ —Ç–æ–ª—å–∫–æ –∏–∑ —Å–∏–º–≤–æ–ª–æ–≤
        text = re.sub(r'^[=\-_+*~#$%^&*()\[\]{}|\\:;"\'<>,.?!/@\s]{3,}$', '', text, flags=re.MULTILINE)
        
        # –£–¥–∞–ª—è–µ–º –æ–¥–∏–Ω–æ—á–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã –≤ –Ω–∞—á–∞–ª–µ —Å—Ç—Ä–æ–∫–∏ (–∞—Ä—Ç–µ—Ñ–∞–∫—Ç—ã OCR)
        text = re.sub(r'^\s*[^\w\s]\s*$', '', text, flags=re.MULTILINE)
        
        # –£–¥–∞–ª—è–µ–º —Å—Ç—Ä–æ–∫–∏ —Å –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–º–∏ –ø—Ä–æ–±–µ–ª–∞–º–∏ –∏ —Å–∏–º–≤–æ–ª–∞–º–∏
        text = re.sub(r'^\s*[\s\W]{15,}\s*$', '', text, flags=re.MULTILINE)
        
        # –£–¥–∞–ª—è–µ–º –∫—Ä–∞–∫–æ–∑—è–±—Ä—ã –≤ —Å–µ—Ä–µ–¥–∏–Ω–µ —Ç–µ–∫—Å—Ç–∞ (–ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∏–∑ 3+ –Ω–µ–±—É–∫–≤–µ–Ω–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤)
        text = re.sub(r'[^\w\s]{3,}', ' ', text)
        
        # –£–¥–∞–ª—è–µ–º –∫—Ä–∏–≤—ã–µ —Å–∏–º–≤–æ–ª—ã —Ç–∏–ø–∞ "oe ee ee eS a" - –∫–æ—Ä–æ—Ç–∫–∏–µ –±–µ—Å—Å–º—ã—Å–ª–µ–Ω–Ω—ã–µ –∫–æ–º–±–∏–Ω–∞—Ü–∏–∏
        text = re.sub(r'^\s*[a-zA-Z–∞-—è–ê-–Ø]{1,3}\s*$', '', text, flags=re.MULTILINE)
        
        # –£–¥–∞–ª—è–µ–º —Å—Ç—Ä–æ–∫–∏ —Ç–∏–ø–∞ "eS a", "oe ee" - –∫–æ—Ä–æ—Ç–∫–∏–µ –∫–æ–º–±–∏–Ω–∞—Ü–∏–∏ –±—É–∫–≤ —Å –ø—Ä–æ–±–µ–ª–∞–º–∏
        text = re.sub(r'^\s*[a-zA-Z–∞-—è–ê-–Ø]{1,2}\s+[a-zA-Z–∞-—è–ê-–Ø]{1,2}\s*$', '', text, flags=re.MULTILINE)
        
        # –£–¥–∞–ª—è–µ–º —Å—Ç—Ä–æ–∫–∏ –∏–∑ –ø–æ–≤—Ç–æ—Ä—è—é—â–∏—Ö—Å—è –∫–æ—Ä–æ—Ç–∫–∏—Ö –±—É–∫–≤–µ–Ω–Ω—ã—Ö –∫–æ–º–±–∏–Ω–∞—Ü–∏–π
        text = re.sub(r'^\s*([a-zA-Z–∞-—è–ê-–Ø]{1,2}\s+){2,}[a-zA-Z–∞-—è–ê-–Ø]{1,2}\s*$', '', text, flags=re.MULTILINE)
        
        # –£–¥–∞–ª—è–µ–º —Å—Ç—Ä–æ–∫–∏ —Å–æ—Å—Ç–æ—è—â–∏–µ —Ç–æ–ª—å–∫–æ –∏–∑ –æ–¥–∏–Ω–æ—á–Ω—ã—Ö –±—É–∫–≤ –∏ –ø—Ä–æ–±–µ–ª–æ–≤
        text = re.sub(r'^\s*([a-zA-Z–∞-—è–ê-–Ø]\s+){3,}[a-zA-Z–∞-—è–ê-–Ø]\s*$', '', text, flags=re.MULTILINE)
        
        # –î–û–ü–û–õ–ù–ò–¢–ï–õ–¨–ù–´–ï –ü–†–ê–í–ò–õ–ê –î–õ–Ø –ö–û–ù–ö–†–ï–¢–ù–´–• –ö–†–ê–ö–û–ó–Ø–ë–†:
        
        # –£–¥–∞–ª—è–µ–º "epee na —Å–≤—è–∑–∏. pepe –∏—è" - –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π –ø–∞—Ç—Ç–µ—Ä–Ω
        text = re.sub(r'\bepee\s+na\s+—Å–≤—è–∑–∏\.\s+pepe\s+–∏—è\b', '', text)
        
        # –£–¥–∞–ª—è–µ–º "–û –±—Ä –û–û–ù es 5 –ï–ï –ï–ï–ï–í–ï–ï–ï–†–´–ï–ï–†–ï–ï–ï" - –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π –ø–∞—Ç—Ç–µ—Ä–Ω
        text = re.sub(r'\b–û\s+–±—Ä\s+–û–û–ù\s+es\s+5\s+–ï–ï\s+–ï–ï–ï–í–ï–ï–ï–†–´–ï–ï–†–ï–ï–ï\b', '', text)
        
        # –£–¥–∞–ª—è–µ–º "ee ae oe ae oe eo se ee Se SSS Sa" - –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π –ø–∞—Ç—Ç–µ—Ä–Ω
        text = re.sub(r'\bee\s+ae\s+oe\s+ae\s+oe\s+eo\s+se\s+ee\s+Se\s+SSS\s+Sa\b', '', text)
        
        # –£–¥–∞–ª—è–µ–º "–í–∞—é–º–µ–Ω–æ–≤—è–Ω–∏–µ –∏–æ–≥–æ—Ç–æ–≤–∏—Ç–µ–ª—è" - –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π –ø–∞—Ç—Ç–µ—Ä–Ω
        text = re.sub(r'\b–í–∞—é–º–µ–Ω–æ–≤—è–Ω–∏–µ\s+–∏–æ–≥–æ—Ç–æ–≤–∏—Ç–µ–ª—è\b', '', text)
        
        # –£–¥–∞–ª—è–µ–º "–∑–∞–≤–æ–ª–∞] - –∏–∑–≥–æ—Ç–æ–≤–∏—Ç–µ–ª—è" - –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π –ø–∞—Ç—Ç–µ—Ä–Ω
        text = re.sub(r'\b–∑–∞–≤–æ–ª–∞\]\s*-\s*–∏–∑–≥–æ—Ç–æ–≤–∏—Ç–µ–ª—è\b', '', text)
        
        # –£–¥–∞–ª—è–µ–º "–µ–µ–µ –°–û–û–¢–í–ï–¢–°–¢–í–£–Æ–¢" - –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π –ø–∞—Ç—Ç–µ—Ä–Ω
        text = re.sub(r'\b–µ–µ–µ\s+–°–û–û–¢–í–ï–¢–°–¢–í–£–Æ–¢\b', '–°–û–û–¢–í–ï–¢–°–¢–í–£–Æ–¢', text)
        
        # –£–¥–∞–ª—è–µ–º "–Ω–µ—Ö —É –∂–µ =: :" - –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π –ø–∞—Ç—Ç–µ—Ä–Ω
        text = re.sub(r'\b–Ω–µ—Ö\s+—É\s+–∂–µ\s*=\s*:\s*:\s*', '', text)
        
        # –£–¥–∞–ª—è–µ–º "SSS Sa" - –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π –ø–∞—Ç—Ç–µ—Ä–Ω
        text = re.sub(r'\bSSS\s+Sa\b', '', text)
        
        # –£–±–∏—Ä–∞–µ–º –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –ø—Ä–æ–±–µ–ª—ã
        text = re.sub(r'\s+', ' ', text)
        
        # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –ø–µ—Ä–µ–Ω–æ—Å—ã —Å—Ç—Ä–æ–∫
        text = re.sub(r'\n\s*\n', '\n', text)
        
        # –£–¥–∞–ª—è–µ–º –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏
        text = re.sub(r'\n\s*\n', '\n', text)
        
        cleaned_text = text.strip()
        logger.info(f"–ò—Å—Ö–æ–¥–Ω—ã–π –æ—á–∏—â–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç —Å–æ–∑–¥–∞–Ω, –¥–ª–∏–Ω–∞: {len(cleaned_text)}")
        
        return cleaned_text

    def _extract_qa_pairs(self, text: str) -> List[Dict[str, str]]:
        """–í—ã–¥–µ–ª–µ–Ω–∏–µ Q/A –ø–∞—Ä –∏–∑ —Ç–µ–∫—Å—Ç–∞ (FAQ): –ø–æ–¥–¥–µ—Ä–∂–∫–∞ RU/EN –º–∞—Ä–∫–µ—Ä–æ–≤."""
        if not text:
            return []
        import re
        pairs: List[Dict[str, str]] = []
        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –ø–µ—Ä–µ–Ω–æ—Å—ã –∞–±–∑–∞—Ü–µ–≤
        blocks = [b.strip() for b in re.split(r"\n{2,}", text) if b.strip()]
        # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –≤–æ–ø—Ä–æ—Å/–æ—Ç–≤–µ—Ç
        question_patterns = [
            r"^Q\s*[:\-]\s*(.+)$",
            r"^Question\s*[:\-]\s*(.+)$",
            r"^–í–æ–ø—Ä–æ—Å\s*[:\-]\s*(.+)$",
            r"^–ü\.?\s*[:\-]\s*(.+)$",
        ]
        answer_patterns = [
            r"^A\s*[:\-]\s*(.+)$",
            r"^Answer\s*[:\-]\s*(.+)$",
            r"^–û—Ç–≤–µ—Ç\s*[:\-]\s*(.+)$",
            r"^–û\.?\s*[:\-]\s*(.+)$",
        ]
        q_re = re.compile("|".join(f"({p})" for p in question_patterns), re.IGNORECASE)
        a_re = re.compile("|".join(f"({p})" for p in answer_patterns), re.IGNORECASE)
        current_q = None
        for block in blocks:
            qm = q_re.match(block)
            if qm:
                # –í–æ–∑—å–º–µ–º –ø–µ—Ä–≤—É—é –Ω–µ-–ø—É—Å—Ç—É—é –≥—Ä—É–ø–ø—É
                for g in qm.groups():
                    if g and not g.strip().startswith(('Q', 'Question', '–í–æ–ø—Ä–æ—Å', '–ü', 'A', 'Answer', '–û—Ç–≤–µ—Ç', '–û')):
                        current_q = g.strip()
                        break
                continue
            am = a_re.match(block)
            if am and current_q:
                for g in am.groups():
                    if g and not g.strip().startswith(('Q', 'Question', '–í–æ–ø—Ä–æ—Å', '–ü', 'A', 'Answer', '–û—Ç–≤–µ—Ç', '–û')):
                        pairs.append({"question": current_q, "answer": g.strip()})
                        current_q = None
                        break
        # –î–æ–ø. —ç–≤—Ä–∏—Å—Ç–∏–∫–∞: ¬´–í–æ–ø—Ä–æ—Å ‚Ä¶?\n–û—Ç–≤–µ—Ç ‚Ä¶¬ª –≤ –æ–¥–Ω–æ–º –±–ª–æ–∫–µ
        if not pairs:
            inline = re.findall(r"(?:–í–æ–ø—Ä–æ—Å|Question|Q)\s*[:\-]?\s*(.+?)\?\s*(?:\n|\s){1,3}(?:–û—Ç–≤–µ—Ç|Answer|A)\s*[:\-]?\s*(.+?)(?=\n\n|\Z)", text, re.IGNORECASE | re.DOTALL)
            for q, a in inline:
                qn = q.strip()
                an = a.strip()
                if qn and an:
                    pairs.append({"question": qn, "answer": an})
        # –≠–≤—Ä–∏—Å—Ç–∏–∫–∞ –ø–æ —Å—Ç—Ä–æ–∫–∞–º: –≤–æ–ø—Ä–æ—Å ‚Äî —Å—Ç—Ä–æ–∫–∞, –∑–∞–∫–∞–Ω—á–∏–≤–∞—é—â–∞—è—Å—è '?', –æ—Ç–≤–µ—Ç ‚Äî —Å–ª–µ–¥—É—é—â–∏–π –∞–±–∑–∞—Ü
        if not pairs:
            lines = [ln.rstrip() for ln in text.splitlines()]
            i = 0
            while i < len(lines):
                line = lines[i].strip()
                # –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –ø—É—Å—Ç—ã–µ
                if not line:
                    i += 1
                    continue
                # –≤–æ–ø—Ä–æ—Å, –µ—Å–ª–∏ –∑–∞–∫–∞–Ω—á–∏–≤–∞–µ—Ç—Å—è –Ω–∞ '?'
                if line.endswith('?') and 3 <= len(line) <= 300:
                    question = line
                    # —Å–æ–±—Ä–∞—Ç—å –æ—Ç–≤–µ—Ç –∏–∑ —Å–ª–µ–¥—É—é—â–∏—Ö 1-8 —Å—Ç—Ä–æ–∫ –¥–æ –ø—É—Å—Ç–æ–π —Å—Ç—Ä–æ–∫–∏ –∏–ª–∏ —Å–ª–µ–¥—É—é—â–µ–≥–æ –≤–æ–ø—Ä–æ—Å–∞
                    answer_lines = []
                    j = i + 1
                    while j < len(lines) and len(answer_lines) < 8:
                        nxt = lines[j].strip()
                        if not nxt:
                            # –∫–æ–Ω–µ—Ü –æ—Ç–≤–µ—Ç–∞
                            break
                        if nxt.endswith('?') and len(nxt) <= 300:
                            # —Å–ª–µ–¥—É—é—â–∞—è —Å—Ç—Ä–æ–∫–∞ –ø–æ—Ö–æ–∂–∞ –Ω–∞ –≤–æ–ø—Ä–æ—Å ‚Äî –ø—Ä–µ—Ä—ã–≤–∞–µ–º
                            break
                        answer_lines.append(nxt)
                        j += 1
                    answer = ' '.join(answer_lines).strip()
                    if answer:
                        pairs.append({"question": question, "answer": answer})
                        i = j
                        continue
                i += 1
        # –ò–Ω–ª–∞–π–Ω–æ–≤—ã–µ bullets –ø–æ—Å–ª–µ –≤–æ–ø—Ä–æ—Å–∞ –≤ –æ–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–µ: "What is X? ‚Ä¢ ... ‚Ä¢ ..."
        try:
            if not pairs:
                import re as _re
                pattern = _re.compile(r"(?s)(?P<q>[^\n]{3,300}?\?)\s*(?P<a>.+?)(?=(?:\n\s*[^\n]{0,300}?\?|\Z))")
                for m in pattern.finditer(text):
                    q = (m.group('q') or '').strip()
                    a = (m.group('a') or '').strip()
                    if not q or not a:
                        continue
                    # –ï—Å–ª–∏ –≤ –æ—Ç–≤–µ—Ç–µ –µ—Å—Ç—å –º–∞—Ä–∫–µ—Ä—ã '‚Ä¢', —Å–æ–±–µ—Ä–µ–º –∫–∞–∫ —Å–≤—è–∑–Ω—ã–π –æ—Ç–≤–µ—Ç
                    if '‚Ä¢' in a:
                        bullets = [seg.strip() for seg in a.split('‚Ä¢') if seg.strip()]
                        a_norm = ' '.join(bullets)
                    else:
                        a_norm = a
                    # –û–≥—Ä–∞–Ω–∏—á–∏–º —á—Ä–µ–∑–º–µ—Ä–Ω—ã–µ —Ö–≤–æ—Å—Ç—ã
                    a_norm = a_norm.strip()
                    if a_norm:
                        pairs.append({"question": q, "answer": a_norm})
        except Exception:
            pass
        return pairs[:100]
    
    def _create_full_cleaned_text(self, ocr_text: str) -> str:
        """–°–æ–∑–¥–∞—Ç—å –ø–æ–ª–Ω—ã–π –æ—á–∏—â–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç —Å –ø–æ–º–æ—â—å—é AI –∞–≥–µ–Ω—Ç–∞ (–±–µ–∑ —Å–æ–∫—Ä–∞—â–µ–Ω–∏–π)"""
        try:
            import os
            import requests
            import logging
            
            logger = logging.getLogger(__name__)
            logger.info("–°–æ–∑–¥–∞–µ–º –ø–æ–ª–Ω—ã–π –æ—á–∏—â–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç —Å –ø–æ–º–æ—â—å—é AI –∞–≥–µ–Ω—Ç–∞")
            
            # –¢–µ–∫—Å—Ç —É–∂–µ –æ—á–∏—â–µ–Ω –≤ analyze_document, –∏—Å–ø–æ–ª—å–∑—É–µ–º –µ–≥–æ –∫–∞–∫ –µ—Å—Ç—å
            base_cleaned = ocr_text
            
            # –ï—Å–ª–∏ —Ç–µ–∫—Å—Ç —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –∫–∞–∫ –µ—Å—Ç—å
            if len(base_cleaned) < 100:
                return base_cleaned
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ API –∫–ª—é—á–∞
            api_key = os.getenv("PROXYAPI_KEY") or os.getenv("PROXYAPI_API_KEY") or os.getenv("OPEN_AI_API_KEY")
            if not api_key:
                logger.warning("API –∫–ª—é—á –Ω–µ –Ω–∞–π–¥–µ–Ω, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –±–∞–∑–æ–≤—É—é –æ—á–∏—Å—Ç–∫—É")
                return base_cleaned
            
            logger.info("API –∫–ª—é—á –Ω–∞–π–¥–µ–Ω, –æ—Ç–ø—Ä–∞–≤–ª—è–µ–º –∑–∞–ø—Ä–æ—Å –∫ –∞–≥–µ–Ω—Ç—É –¥–ª—è –ø–æ–ª–Ω–æ–π –æ—á–∏—Å—Ç–∫–∏")
            
            # –°–æ–∑–¥–∞–µ–º –ø—Ä–æ–º–ø—Ç –¥–ª—è –ø–æ–ª–Ω–æ–π –æ—á–∏—Å—Ç–∫–∏ –±–µ–∑ —Å–æ–∫—Ä–∞—â–µ–Ω–∏–π
            prompt = f"""
–¢—ã - —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –æ—á–∏—Å—Ç–∫–µ OCR —Ç–µ–∫—Å—Ç–∞. –¢–≤–æ—è –∑–∞–¥–∞—á–∞ - –∏—Å–ø—Ä–∞–≤–∏—Ç—å –∏—Å–∫–∞–∂–µ–Ω–Ω—ã–µ —Å–ª–æ–≤–∞ –∏ —Å–∏–º–≤–æ–ª—ã –≤ —Ç–µ–∫—Å—Ç–µ, –Ω–æ –°–û–•–†–ê–ù–ò–¢–¨ –í–°–ï –°–û–î–ï–†–ñ–ê–ù–ò–ï –ü–û–õ–ù–û–°–¢–¨–Æ.

–ò–°–•–û–î–ù–´–ô OCR –¢–ï–ö–°–¢:
{base_cleaned}

–ó–ê–î–ê–ß–ê:
1. –ò—Å–ø—Ä–∞–≤—å –≤—Å–µ –∏—Å–∫–∞–∂–µ–Ω–Ω—ã–µ —Å–ª–æ–≤–∞ (–∫—Ä–∞–∫–æ–∑—è–±—Ä—ã) –Ω–∞ –ø—Ä–∞–≤–∏–ª—å–Ω—ã–µ
2. –í–æ—Å—Å—Ç–∞–Ω–æ–≤–∏ —Å—Ç—Ä—É–∫—Ç—É—Ä—É –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π
3. –ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–û: –°–æ—Ö—Ä–∞–Ω–∏ –í–°–ï —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ - –ù–ï —Å–æ–∫—Ä–∞—â–∞–π, –ù–ï —É–¥–∞–ª—è–π –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è, –ù–ï —Å–æ–∑–¥–∞–≤–∞–π –∫—Ä–∞—Ç–∫–æ–µ –∏–∑–ª–æ–∂–µ–Ω–∏–µ
4. –£–¥–∞–ª–∏ —Ç–æ–ª—å–∫–æ —è–≤–Ω—ã–π –º—É—Å–æ—Ä (–ø–æ–≤—Ç–æ—Ä—è—é—â–∏–µ—Å—è —Å–∏–º–≤–æ–ª—ã, –∞—Ä—Ç–µ—Ñ–∞–∫—Ç—ã OCR)
5. –°–æ—Ö—Ä–∞–Ω–∏ –≤—Å–µ —á–∏—Å–ª–∞, –¥–∞—Ç—ã, –Ω–∞–∑–≤–∞–Ω–∏—è, —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —Ç–µ—Ä–º–∏–Ω—ã, –ø–æ–ª–Ω—ã–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
6. –†–µ–∑—É–ª—å—Ç–∞—Ç –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –î–õ–ò–ù–ù–ï–ï –∏–ª–∏ –†–ê–í–ï–ù –∏—Å—Ö–æ–¥–Ω–æ–º—É —Ç–µ–∫—Å—Ç—É

–ü–†–ò–ú–ï–†–´ –ò–°–ü–†–ê–í–õ–ï–ù–ò–ô:
- "–í–∞—é–º–µ–Ω–æ–≤—è–Ω–∏–µ" ‚Üí "–ù–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ"
- "–∏–æ–≥–æ—Ç–æ–≤–∏—Ç–µ–ª—è" ‚Üí "–∏–∑–≥–æ—Ç–æ–≤–∏—Ç–µ–ª—è" 
- "–µ–µ–µ –°–û–û–¢–í–ï–¢–°–¢–í–£–Æ–¢" ‚Üí "–°–û–û–¢–í–ï–¢–°–¢–í–£–Æ–¢"
- "–∑–∞–≤–æ–ª–∞] - –∏–∑–≥–æ—Ç–æ–≤–∏—Ç–µ–ª—è" ‚Üí "–∑–∞–≤–æ–¥–∞ - –∏–∑–≥–æ—Ç–æ–≤–∏—Ç–µ–ª—è"

–í–ï–†–ù–ò –¢–û–õ–¨–ö–û –û–ß–ò–©–ï–ù–ù–´–ô –ü–û–õ–ù–´–ô –¢–ï–ö–°–¢ –ë–ï–ó –î–û–ü–û–õ–ù–ò–¢–ï–õ–¨–ù–´–• –ö–û–ú–ú–ï–ù–¢–ê–†–ò–ï–í.
"""
            
            headers = {
                "Authorization": f"Bearer {api_key}",
                "Content-Type": "application/json"
            }
            
            payload = {
                "model": "gpt-4o",
                "messages": [
                    {
                        "role": "system",
                        "content": "–¢—ã - —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –æ—á–∏—Å—Ç–∫–µ OCR —Ç–µ–∫—Å—Ç–∞. –¢–≤–æ—è –∑–∞–¥–∞—á–∞ - –∏—Å–ø—Ä–∞–≤–∏—Ç—å –∏—Å–∫–∞–∂–µ–Ω–Ω—ã–µ —Å–ª–æ–≤–∞, —Å–æ—Ö—Ä–∞–Ω–∏–≤ –≤—Å–µ —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –±–µ–∑ —Å–æ–∫—Ä–∞—â–µ–Ω–∏–π."
                    },
                    {
                        "role": "user", 
                        "content": prompt
                    }
                ],
                "max_tokens": 4000,
                "temperature": 0.1
            }
            
            response = requests.post(
                "https://api.proxyapi.ru/openai/v1/chat/completions",
                headers=headers,
                json=payload,
                timeout=60
            )
            
            if response.status_code == 200:
                result = response.json()
                cleaned_text = result['choices'][0]['message']['content'].strip()
                logger.info(f"‚úÖ –ê–≥–µ–Ω—Ç –ø–æ–ª–Ω–æ–π –æ—á–∏—Å—Ç–∫–∏ –∑–∞–≤–µ—Ä—à–∏–ª —Ä–∞–±–æ—Ç—É, –¥–ª–∏–Ω–∞: {len(cleaned_text)}")
                return cleaned_text
            else:
                logger.error(f"–û—à–∏–±–∫–∞ API –ø—Ä–∏ –ø–æ–ª–Ω–æ–π –æ—á–∏—Å—Ç–∫–µ: {response.status_code} - {response.text}")
                return base_cleaned
                
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ –ø–æ–ª–Ω–æ–≥–æ –æ—á–∏—â–µ–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞: {e}")
            return self._get_original_cleaned_text(ocr_text)
    
    def _smart_kb_suggestion(self, analysis: Dict) -> Dict:
        """–£–º–Ω–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ KB —Å –ø—Ä–æ–≤–µ—Ä–∫–æ–π –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –∏ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å—é –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è"""
        try:
            import os
            import requests
            import logging
            
            logger = logging.getLogger(__name__)
            logger.info("–£–º–Ω–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ KB —Å –ø—Ä–æ–≤–µ—Ä–∫–æ–π –¥—É–±–ª–∏–∫–∞—Ç–æ–≤")
            
            # –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö KB
            existing_kbs = self.kb_manager.get_knowledge_bases(active_only=True)
            existing_categories = self.kb_manager.get_categories()
            
            # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –¥–æ–∫—É–º–µ–Ω—Ç–µ
            doc_content = analysis.get('original_cleaned_text', analysis.get('full_cleaned_text', ''))
            doc_summary = analysis.get('smart_summary', '')
            doc_category = analysis.get('category', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')
            doc_title = analysis.get('file_name', '–î–æ–∫—É–º–µ–Ω—Ç')
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ API –∫–ª—é—á–∞
            api_key = os.getenv("PROXYAPI_KEY") or os.getenv("PROXYAPI_API_KEY") or os.getenv("OPEN_AI_API_KEY")
            if not api_key:
                logger.warning("API –∫–ª—é—á –Ω–µ –Ω–∞–π–¥–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º –±–∞–∑–æ–≤–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ")
                return self._create_basic_kb_suggestion(doc_category, existing_categories)
            
            logger.info("API –∫–ª—é—á –Ω–∞–π–¥–µ–Ω, –æ—Ç–ø—Ä–∞–≤–ª—è–µ–º –∑–∞–ø—Ä–æ—Å –∫ –∞–≥–µ–Ω—Ç—É –¥–ª—è —É–º–Ω–æ–≥–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è KB")
            
            # –°–æ–∑–¥–∞–µ–º –ø—Ä–æ–º–ø—Ç –¥–ª—è —É–º–Ω–æ–≥–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è —Ç–µ–º—ã KB
            existing_kb_info = ""
            if existing_kbs:
                existing_kb_info = "–°–£–©–ï–°–¢–í–£–Æ–©–ò–ï –ë–ê–ó–´ –ó–ù–ê–ù–ò–ô:\n"
                for kb in existing_kbs[:15]:  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 15
                    existing_kb_info += f"- ID {kb['id']}: {kb['name']} (–∫–∞—Ç–µ–≥–æ—Ä–∏—è: {kb['category']})\n"
                    if kb.get('description'):
                        existing_kb_info += f"  –û–ø–∏—Å–∞–Ω–∏–µ: {kb['description'][:100]}...\n"
            
            prompt = f"""
–¢—ã - —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏ –±–∞–∑ –∑–Ω–∞–Ω–∏–π –¥–ª—è –æ–ø–µ—Ä–∞—Ç–æ—Ä–∞ —Å–ø—É—Ç–Ω–∏–∫–æ–≤–æ–π —Å–≤—è–∑–∏. –¢–≤–æ—è –∑–∞–¥–∞—á–∞ - –ø—Ä–µ–¥–ª–æ–∂–∏—Ç—å –æ–ø—Ç–∏–º–∞–ª—å–Ω—É—é —Ç–µ–º—É/–Ω–∞–∑–≤–∞–Ω–∏–µ –¥–ª—è –Ω–æ–≤–æ–π –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π —Å —É—á–µ—Ç–æ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –∏ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è.

–ò–ù–§–û–†–ú–ê–¶–ò–Ø –û –î–û–ö–£–ú–ï–ù–¢–ï:
- –ù–∞–∑–≤–∞–Ω–∏–µ —Ñ–∞–π–ª–∞: {doc_title}
- –ö–∞—Ç–µ–≥–æ—Ä–∏—è: {doc_category}
- –ö—Ä–∞—Ç–∫–æ–µ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ: {doc_summary[:500]}...

{existing_kb_info}

–°–£–©–ï–°–¢–í–£–Æ–©–ò–ï –ö–ê–¢–ï–ì–û–†–ò–ò: {', '.join(existing_categories)}

–ó–ê–î–ê–ß–ê:
1. –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞
2. –ü—Ä–æ–≤–µ—Ä—å, –µ—Å—Ç—å –ª–∏ –ø–æ—Ö–æ–∂–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –≤ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö KB
3. –ï—Å–ª–∏ –µ—Å—Ç—å –ø–æ—Ö–æ–∂–∏–µ - –ø—Ä–µ–¥–ª–æ–∂–∏ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–π KB
4. –ï—Å–ª–∏ –Ω–µ—Ç –ø–æ—Ö–æ–∂–∏—Ö - –ø—Ä–µ–¥–ª–æ–∂–∏ –Ω–æ–≤—É—é KB —Å —É–Ω–∏–∫–∞–ª—å–Ω—ã–º –Ω–∞–∑–≤–∞–Ω–∏–µ–º
5. –ü—Ä–µ–¥–ª–æ–∂–∏ –∫–∞—Ç–µ–≥–æ—Ä–∏—é –∏–∑ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –∏–ª–∏ –Ω–æ–≤—É—é
6. –ò–∑–±–µ–≥–∞–π –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏—è –Ω–∞–∑–≤–∞–Ω–∏–π

–û–¢–í–ï–¢ –í –§–û–†–ú–ê–¢–ï JSON:
{{
    "suggested_name": "–ù–∞–∑–≤–∞–Ω–∏–µ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π",
    "suggested_category": "–ö–∞—Ç–µ–≥–æ—Ä–∏—è",
    "description": "–ö—Ä–∞—Ç–∫–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ (1-2 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è)",
    "merge_with_existing": {{
        "can_merge": true/false,
        "existing_kb_id": null –∏–ª–∏ ID,
        "existing_kb_name": null –∏–ª–∏ –Ω–∞–∑–≤–∞–Ω–∏–µ,
        "merge_reason": "–ü—Ä–∏—á–∏–Ω–∞ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è"
    }},
    "duplicate_check": {{
        "has_duplicates": true/false,
        "duplicate_kbs": [—Å–ø–∏—Å–æ–∫ ID –¥—É–±–ª–∏–∫–∞—Ç–æ–≤],
        "duplicate_reason": "–ü—Ä–∏—á–∏–Ω–∞ –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏—è"
    }},
    "confidence": 0.85,
    "reasoning": "–û–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è"
}}
"""
            
            headers = {
                "Authorization": f"Bearer {api_key}",
                "Content-Type": "application/json"
            }
            
            payload = {
                "model": "gpt-4o",
                "messages": [
                    {
                        "role": "system",
                        "content": "–¢—ã - —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏ –±–∞–∑ –∑–Ω–∞–Ω–∏–π. –¢–≤–æ—è –∑–∞–¥–∞—á–∞ - –ø—Ä–µ–¥–ª–æ–∂–∏—Ç—å –æ–ø—Ç–∏–º–∞–ª—å–Ω—É—é —Ç–µ–º—É KB —Å —É—á–µ—Ç–æ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö, –∏–∑–±–µ–≥–∞—è –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏—è –∏ –ø—Ä–µ–¥–ª–∞–≥–∞—è –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –∫–æ–≥–¥–∞ —ç—Ç–æ —É–º–µ—Å—Ç–Ω–æ."
                    },
                    {
                        "role": "user", 
                        "content": prompt
                    }
                ],
                "max_tokens": 1000,
                "temperature": 0.3
            }
            
            response = requests.post(
                "https://api.proxyapi.ru/openai/v1/chat/completions",
                headers=headers,
                json=payload,
                timeout=30
            )
            
            if response.status_code == 200:
                result = response.json()
                response_text = result['choices'][0]['message']['content'].strip()
                
                # –ü–∞—Ä—Å–∏–º JSON –æ—Ç–≤–µ—Ç
                try:
                    import json
                    suggestion = json.loads(response_text)
                    logger.info(f"‚úÖ –£–º–Ω–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ KB —Å–æ–∑–¥–∞–Ω–æ: {suggestion.get('suggested_name', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')}")
                    return suggestion
                except json.JSONDecodeError:
                    logger.warning("–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ JSON –æ—Ç–≤–µ—Ç–∞, –∏—Å–ø–æ–ª—å–∑—É–µ–º –±–∞–∑–æ–≤–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ")
                    return self._create_basic_kb_suggestion(doc_category, existing_categories)
            else:
                logger.error(f"–û—à–∏–±–∫–∞ API –ø—Ä–∏ —É–º–Ω–æ–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–∏ KB: {response.status_code} - {response.text}")
                return self._create_basic_kb_suggestion(doc_category, existing_categories)
                
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —É–º–Ω–æ–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–∏ —Ç–µ–º—ã KB: {e}")
            return self._create_basic_kb_suggestion(analysis.get('category', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ'), [])

    def _suggest_kb_topic(self, analysis: Dict) -> Dict:
        """–ü—Ä–µ–¥–ª–æ–∂–∏—Ç—å —Ç–µ–º—É KB —Å —É—á–µ—Ç–æ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –∏ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è"""
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –Ω–æ–≤—ã–π —É–º–Ω—ã–π –º–µ—Ç–æ–¥
        return self._smart_kb_suggestion(analysis)
    
    def _create_basic_kb_suggestion(self, doc_category: str, existing_categories: List[str]) -> Dict:
        """–°–æ–∑–¥–∞—Ç—å –±–∞–∑–æ–≤–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ KB –±–µ–∑ AI"""
        # –ü—Ä–æ—Å—Ç–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞
        suggested_name = f"–ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π: {doc_category}"
        suggested_category = doc_category if doc_category in existing_categories else "–û–±—â–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã"
        
        return {
            "suggested_name": suggested_name,
            "suggested_category": suggested_category,
            "description": f"–ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π –¥–ª—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ {doc_category}",
            "merge_with_existing": {
                "can_merge": False,
                "existing_kb_id": None,
                "existing_kb_name": None,
                "merge_reason": "–ù–µ –Ω–∞–π–¥–µ–Ω–æ –ø–æ–¥—Ö–æ–¥—è—â–∏—Ö –¥–ª—è –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è"
            },
            "confidence": 0.5,
            "reasoning": "–ë–∞–∑–æ–≤–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ –±–µ–∑ AI –∞–Ω–∞–ª–∏–∑–∞"
        }
    
    def _create_ocr_cleaning_agent(self, ocr_text: str, gemini_analysis: str = None) -> str:
        """–°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–π –∞–≥–µ–Ω—Ç –¥–ª—è –æ—á–∏—Å—Ç–∫–∏ OCR —Ç–µ–∫—Å—Ç–∞ –∏ —Å–æ–∑–¥–∞–Ω–∏—è abstract –¥–ª—è KB"""
        try:
            import logging
            
            logger = logging.getLogger(__name__)
            logger.info("–ó–∞–ø—É—Å–∫–∞–µ–º –∞–≥–µ–Ω—Ç–∞ –æ—á–∏—Å—Ç–∫–∏ OCR —Ç–µ–∫—Å—Ç–∞ –¥–ª—è KB (–ª–æ–∫–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å)")
            
            # –ü–æ–ª—É—á–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –±–∞–∑–∞—Ö –∑–Ω–∞–Ω–∏–π
            existing_kbs = self._get_existing_knowledge_bases()
            logger.info(f"–ù–∞–π–¥–µ–Ω–æ {len(existing_kbs)} —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –ë–ó")
            
            # –õ–æ–≥–∏—Ä—É–µ–º –Ω–∞–ª–∏—á–∏–µ –∞–Ω–∞–ª–∏–∑–∞ –æ—Ç Gemini
            if gemini_analysis:
                logger.info(f"‚úÖ –ü–æ–ª—É—á–µ–Ω –∞–Ω–∞–ª–∏–∑ –æ—Ç Gemini ({len(gemini_analysis)} —Å–∏–º–≤–æ–ª–æ–≤) - –±—É–¥–µ—Ç –æ–±—ä–µ–¥–∏–Ω–µ–Ω —Å OCR")
            else:
                logger.info("‚ÑπÔ∏è –ê–Ω–∞–ª–∏–∑ –æ—Ç Gemini –Ω–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω - —Ä–∞–±–æ—Ç–∞–µ–º —Ç–æ–ª—å–∫–æ —Å OCR")
            
            # –£–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç –¥–ª—è –∞–≥–µ–Ω—Ç–∞ –æ—á–∏—Å—Ç–∫–∏ OCR —Å –∞–Ω–∞–ª–∏–∑–æ–º –∫–∞—á–µ—Å—Ç–≤–∞
            prompt = f"""
–¢—ã - —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –∞–Ω–∞–ª–∏–∑—É –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π. –¢–≤–æ—è –∑–∞–¥–∞—á–∞:

1. –û–ß–ò–°–¢–ò–¢–¨ OCR —Ç–µ–∫—Å—Ç –æ—Ç –∫—Ä–∞–∫–æ–∑—è–±—Ä –∏ –º—É—Å–æ—Ä–∞
2. –í–û–°–°–¢–ê–ù–û–í–ò–¢–¨ –æ—Å–º—ã—Å–ª–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ
3. –û–¶–ï–ù–ò–¢–¨ –∫–∞—á–µ—Å—Ç–≤–æ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
4. –ü–†–ï–î–õ–û–ñ–ò–¢–¨ –∫–∞—Ç–µ–≥–æ—Ä–∏—é –¥–ª—è –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π
5. –í–´–î–ï–õ–ò–¢–¨ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
6. –°–§–û–†–ú–£–õ–ò–†–û–í–ê–¢–¨ —Ç–µ—Å—Ç–æ–≤—ã–µ –≤–æ–ø—Ä–æ—Å—ã –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏

–ò—Å—Ö–æ–¥–Ω—ã–π OCR —Ç–µ–∫—Å—Ç (–ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç, {len(ocr_text)} —Å–∏–º–≤–æ–ª–æ–≤):
{ocr_text}

–°–£–©–ï–°–¢–í–£–Æ–©–ò–ï –ë–ê–ó–´ –ó–ù–ê–ù–ò–ô:
{existing_kbs}

–ü–†–ê–í–ò–õ–ê –û–ß–ò–°–¢–ö–ò:
- –£–¥–∞–ª–∏ –∫—Ä–∞–∫–æ–∑—è–±—Ä—ã: "epee na —Å–≤—è–∑–∏. pepe –∏—è", "–û –±—Ä –û–û–ù es 5 –ï–ï –ï–ï–ï–í–ï–ï–ï–†–´–ï–ï–†–ï–ï–ï"
- –£–¥–∞–ª–∏ –º—É—Å–æ—Ä: "ee ae oe ae oe eo se ee Se SSS Sa", "–í–∞—é–º–µ–Ω–æ–≤—è–Ω–∏–µ –∏–æ–≥–æ—Ç–æ–≤–∏—Ç–µ–ª—è"
- –í–æ—Å—Å—Ç–∞–Ω–æ–≤–∏ –Ω–æ—Ä–º–∞–ª—å–Ω—ã–µ —Å–ª–æ–≤–∞ –∏–∑ –∏—Å–∫–∞–∂–µ–Ω–∏–π
- –°–æ—Ö—Ä–∞–Ω–∏ —Ç–æ–ª—å–∫–æ –æ—Å–º—ã—Å–ª–µ–Ω–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
- –°–æ–∑–¥–∞–π –∫—Ä–∞—Ç–∫–∏–π, —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π abstract –¢–û–õ–¨–ö–û –∏–∑ OCR —Ç–µ–∫—Å—Ç–∞

–§–û–†–ú–ê–¢ –û–¢–í–ï–¢–ê:
```
–ö–ê–ß–ï–°–¢–í–û_–¢–ï–ö–°–¢–ê: [–û–¢–õ–ò–ß–ù–û–ï/–•–û–†–û–®–ï–ï/–£–î–û–í–õ–ï–¢–í–û–†–ò–¢–ï–õ–¨–ù–û–ï/–ü–õ–û–•–û–ï]
–û–°–ú–´–°–õ–ï–ù–ù–û–°–¢–¨: [–í–´–°–û–ö–ê–Ø/–°–†–ï–î–ù–Ø–Ø/–ù–ò–ó–ö–ê–Ø]
–ì–û–¢–û–í–ù–û–°–¢–¨_–î–õ–Ø_KB: [–î–ê/–ù–ï–¢]

–ö–ê–¢–ï–ì–û–†–ò–Ø_–ë–ó: [–≤—ã–±–µ—Ä–∏ –∏–∑ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –ë–ó –∏–ª–∏ –ø—Ä–µ–¥–ª–æ–∂–∏ –Ω–æ–≤—É—é]

–ö–õ–Æ–ß–ï–í–´–ï_–°–õ–û–í–ê: [—Å–ø–∏—Å–æ–∫ –∏–∑ 5-10 –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ —á–µ—Ä–µ–∑ –∑–∞–ø—è—Ç—É—é]

–ß–ò–°–¢–´–ô_ABSTRACT:
[–∑–¥–µ—Å—å –ö–†–ê–¢–ö–ò–ô –∞–±—Å—Ç—Ä–∞–∫—Ç (–º–∞–∫—Å–∏–º—É–º 200-300 —Å–ª–æ–≤) —Å –∫–ª—é—á–µ–≤–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –¥–ª—è –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π]

–¢–ï–°–¢–û–í–´–ï_–í–û–ü–†–û–°–´:
1. [–≤–æ–ø—Ä–æ—Å –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ]
   –û—Ç–≤–µ—Ç: [–æ—Ç–≤–µ—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞]
2. [–≤–æ–ø—Ä–æ—Å –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ]
   –û—Ç–≤–µ—Ç: [–æ—Ç–≤–µ—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞]
3. [–≤–æ–ø—Ä–æ—Å –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ]
   –û—Ç–≤–µ—Ç: [–æ—Ç–≤–µ—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞]
4. [–≤–æ–ø—Ä–æ—Å –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ]
   –û—Ç–≤–µ—Ç: [–æ—Ç–≤–µ—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞]
5. [–≤–æ–ø—Ä–æ—Å –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ]
   –û—Ç–≤–µ—Ç: [–æ—Ç–≤–µ—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞]
```

–°–û–ó–î–ê–ù–ò–ï –ê–ë–°–¢–†–ê–ö–¢–ê:
- –°–æ–∑–¥–∞–π –ö–†–ê–¢–ö–ò–ô –∞–±—Å—Ç—Ä–∞–∫—Ç (200-300 —Å–ª–æ–≤ –º–∞–∫—Å–∏–º—É–º)
- –í–∫–ª—é—á–∏ —Ç–æ–ª—å–∫–æ –ö–õ–Æ–ß–ï–í–£–Æ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é: –Ω–∞–∑–≤–∞–Ω–∏–µ, –Ω–æ–º–µ—Ä, –¥–∞—Ç—ã, –æ—Å–Ω–æ–≤–Ω—ã–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏
- –£–±–µ—Ä–∏ –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏—è –∏ –∏–∑–±—ã—Ç–æ—á–Ω—ã–µ –¥–µ—Ç–∞–ª–∏
- –°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä—É–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –≤ –ª–æ–≥–∏—á–µ—Å–∫–æ–º –ø–æ—Ä—è–¥–∫–µ
- –ò—Å–ø–æ–ª—å–∑—É–π —á–µ—Ç–∫–∏–µ, –ø–æ–Ω—è—Ç–Ω—ã–µ —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∏

–ö–õ–Æ–ß–ï–í–´–ï –°–õ–û–í–ê:
- –í—ã–¥–µ–ª–∏ 5-10 —Å–∞–º—ã—Ö –≤–∞–∂–Ω—ã—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤, –ø–æ–Ω—è—Ç–∏–π, –Ω–∞–∑–≤–∞–Ω–∏–π
- –í–∫–ª—é—á–∏ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —Ç–µ—Ä–º–∏–Ω—ã, –Ω–∞–∑–≤–∞–Ω–∏—è —É—Å–ª—É–≥, –∫–ª—é—á–µ–≤—ã–µ –ø–æ–Ω—è—Ç–∏—è
- –ò—Å–ø–æ–ª—å–∑—É–π –∫–∞–∫ —Ä—É—Å—Å–∫–∏–µ, —Ç–∞–∫ –∏ –∞–Ω–≥–ª–∏–π—Å–∫–∏–µ —Ç–µ—Ä–º–∏–Ω—ã –µ—Å–ª–∏ –Ω—É–∂–Ω–æ

–¢–ï–°–¢–û–í–´–ï –í–û–ü–†–û–°–´:
- –ù–∞–π–¥–∏ 3-5 –∏–Ω—Ç–µ—Ä–µ—Å–Ω—ã—Ö —Ñ–∞–∫—Ç–æ–≤ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ
- –°–æ–∑–¥–∞–π –ø—Ä–æ—Å—Ç—ã–µ –≤–æ–ø—Ä–æ—Å—ã –æ–± —ç—Ç–∏—Ö —Ñ–∞–∫—Ç–∞—Ö
- –û—Ç–≤–µ—Ç—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞
- –í–æ–ø—Ä–æ—Å—ã –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ
"""
            
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ª–æ–∫–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å —á–µ—Ä–µ–∑ BaseAgent
            from langchain.prompts import ChatPromptTemplate
            from langchain.schema import StrOutputParser
            
            template = ChatPromptTemplate.from_template(prompt)
            chain = template | self.chat_model | StrOutputParser()
            
            agent_response = chain.invoke({})
            
            # –ü–∞—Ä—Å–∏–º —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç –∞–≥–µ–Ω—Ç–∞
            parsed_result = self._parse_agent_response(agent_response)
            
            logger.info(f"‚úÖ –ê–≥–µ–Ω—Ç –æ—á–∏—Å—Ç–∫–∏ –∑–∞–≤–µ—Ä—à–∏–ª —Ä–∞–±–æ—Ç—É (–ª–æ–∫–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å)")
            logger.info(f"–ö–∞—á–µ—Å—Ç–≤–æ —Ç–µ–∫—Å—Ç–∞: {parsed_result.get('quality', '–ù–ï–ò–ó–í–ï–°–¢–ù–û')}")
            logger.info(f"–û—Å–º—ã—Å–ª–µ–Ω–Ω–æ—Å—Ç—å: {parsed_result.get('meaningfulness', '–ù–ï–ò–ó–í–ï–°–¢–ù–û')}")
            logger.info(f"–ì–æ—Ç–æ–≤–Ω–æ—Å—Ç—å –¥–ª—è KB: {parsed_result.get('kb_ready', '–ù–ï–ò–ó–í–ï–°–¢–ù–û')}")
            logger.info(f"–ö–∞—Ç–µ–≥–æ—Ä–∏—è –ë–ó: {parsed_result.get('category', '–ù–ï–ò–ó–í–ï–°–¢–ù–û')}")
            logger.info(f"–ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞: {parsed_result.get('keywords', [])}")
            logger.info(f"–¢–µ—Å—Ç–æ–≤—ã–µ –≤–æ–ø—Ä–æ—Å—ã: {len(parsed_result.get('test_questions', []))}")
            logger.info(f"–î–ª–∏–Ω–∞ abstract: {len(parsed_result.get('abstract', ''))}")
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ session_state –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ –¥—Ä—É–≥–∏—Ö –º–µ—Ç–æ–¥–∞—Ö
            if hasattr(self, '_last_parsed_result'):
                self._last_parsed_result = parsed_result
            
            return parsed_result.get('abstract', agent_response)
                
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∞–≥–µ–Ω—Ç–∞ –æ—á–∏—Å—Ç–∫–∏: {e}")
            return ocr_text
    
    def _get_existing_knowledge_bases(self) -> str:
        """–ü–æ–ª—É—á–∏—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –±–∞–∑–∞—Ö –∑–Ω–∞–Ω–∏–π"""
        try:
            # –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –ë–ó
            kbs = self.kb_manager.get_knowledge_bases()
            
            if not kbs:
                return "–°—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –±–∞–∑ –∑–Ω–∞–Ω–∏–π –Ω–µ –Ω–∞–π–¥–µ–Ω–æ."
            
            kb_info = "–°—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π:\n"
            for kb in kbs:
                kb_info += f"- ID: {kb['id']}, –ù–∞–∑–≤–∞–Ω–∏–µ: '{kb['name']}', –ö–∞—Ç–µ–≥–æ—Ä–∏—è: '{kb['category']}', –û–ø–∏—Å–∞–Ω–∏–µ: '{kb['description']}'{chr(10)}"
            
            return kb_info
            
        except Exception as e:
            import logging
            logger = logging.getLogger(__name__)
            logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Å–ø–∏—Å–∫–∞ –ë–ó: {e}")
            return "–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Å–ø–∏—Å–∫–∞ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –ë–ó."
    
    def _parse_agent_response(self, response: str) -> dict:
        """–ü–∞—Ä—Å–∏–Ω–≥ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞ –∞–≥–µ–Ω—Ç–∞ –æ—á–∏—Å—Ç–∫–∏ OCR"""
        try:
            import re
            
            result = {
                'quality': '–ù–ï–ò–ó–í–ï–°–¢–ù–û',
                'meaningfulness': '–ù–ï–ò–ó–í–ï–°–¢–ù–û', 
                'kb_ready': '–ù–ï–ò–ó–í–ï–°–¢–ù–û',
                'category': '–ù–ï–ò–ó–í–ï–°–¢–ù–û',
                'keywords': [],
                'test_questions': [],
                'abstract': response  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é –≤–µ—Å—å –æ—Ç–≤–µ—Ç
            }
            
            # –ò—â–µ–º –∫–∞—á–µ—Å—Ç–≤–æ —Ç–µ–∫—Å—Ç–∞
            quality_match = re.search(r'–ö–ê–ß–ï–°–¢–í–û_–¢–ï–ö–°–¢–ê:\s*([–ê-–Ø]+)', response)
            if quality_match:
                result['quality'] = quality_match.group(1)
            
            # –ò—â–µ–º –æ—Å–º—ã—Å–ª–µ–Ω–Ω–æ—Å—Ç—å
            meaningfulness_match = re.search(r'–û–°–ú–´–°–õ–ï–ù–ù–û–°–¢–¨:\s*([–ê-–Ø]+)', response)
            if meaningfulness_match:
                result['meaningfulness'] = meaningfulness_match.group(1)
            
            # –ò—â–µ–º –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç—å –¥–ª—è KB
            kb_ready_match = re.search(r'–ì–û–¢–û–í–ù–û–°–¢–¨_–î–õ–Ø_KB:\s*([–ê-–Ø]+)', response)
            if kb_ready_match:
                result['kb_ready'] = kb_ready_match.group(1)
            
            # –ò—â–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏—é –ë–ó
            category_match = re.search(r'–ö–ê–¢–ï–ì–û–†–ò–Ø_–ë–ó:\s*([^\n]+)', response)
            if category_match:
                result['category'] = category_match.group(1).strip()
            
            # –ò—â–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
            keywords_match = re.search(r'–ö–õ–Æ–ß–ï–í–´–ï_–°–õ–û–í–ê:\s*([^\n]+)', response)
            if keywords_match:
                keywords_text = keywords_match.group(1).strip()
                result['keywords'] = [kw.strip() for kw in keywords_text.split(',') if kw.strip()]
            
            # –ò—â–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ –≤–æ–ø—Ä–æ—Å—ã —Å –æ—Ç–≤–µ—Ç–∞–º–∏
            questions_match = re.search(r'–¢–ï–°–¢–û–í–´–ï_–í–û–ü–†–û–°–´:\s*\n(.*?)(?:\n\n|\Z)', response, re.DOTALL)
            if questions_match:
                questions_text = questions_match.group(1).strip()
                # –ü–∞—Ä—Å–∏–º –Ω—É–º–µ—Ä–æ–≤–∞–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ –≤–æ–ø—Ä–æ—Å–æ–≤ —Å –æ—Ç–≤–µ—Ç–∞–º–∏
                questions = []
                current_question = None
                current_answer = None
                
                for line in questions_text.split('\n'):
                    line = line.strip()
                    if not line:
                        continue
                    
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Å—Ç—Ä–æ–∫–∞ –≤–æ–ø—Ä–æ—Å–æ–º (–Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è —Å —Ü–∏—Ñ—Ä—ã)
                    if re.match(r'^\d+\.', line):
                        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–µ–¥—ã–¥—É—â–∏–π –≤–æ–ø—Ä–æ—Å, –µ—Å–ª–∏ –µ—Å—Ç—å
                        if current_question:
                            questions.append({
                                'question': current_question,
                                'answer': current_answer or '–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ'
                            })
                        
                        # –ù–∞—á–∏–Ω–∞–µ–º –Ω–æ–≤—ã–π –≤–æ–ø—Ä–æ—Å
                        current_question = re.sub(r'^\d+\.\s*', '', line)
                        current_answer = None
                    
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Å—Ç—Ä–æ–∫–∞ –æ—Ç–≤–µ—Ç–æ–º
                    elif line.startswith('–û—Ç–≤–µ—Ç:'):
                        current_answer = line.replace('–û—Ç–≤–µ—Ç:', '').strip()
                
                # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π –≤–æ–ø—Ä–æ—Å
                if current_question:
                    questions.append({
                        'question': current_question,
                        'answer': current_answer or '–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ'
                    })
                
                result['test_questions'] = questions
            
            # –ò—â–µ–º —á–∏—Å—Ç—ã–π abstract
            abstract_match = re.search(r'–ß–ò–°–¢–´–ô_ABSTRACT:\s*\n(.*?)(?:\n\n|\Z)', response, re.DOTALL)
            if abstract_match:
                result['abstract'] = abstract_match.group(1).strip()
            
            return result
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –æ—Ç–≤–µ—Ç–∞ –∞–≥–µ–Ω—Ç–∞: {e}")
            return {
                'quality': '–û–®–ò–ë–ö–ê',
                'meaningfulness': '–û–®–ò–ë–ö–ê',
                'kb_ready': '–û–®–ò–ë–ö–ê', 
                'category': '–û–®–ò–ë–ö–ê',
                'abstract': response
            }
    
    def _create_abstract_with_agent(self, ocr_text: str) -> str:
        """–°–æ–∑–¥–∞–Ω–∏–µ abstract –∏–∑ OCR —Ç–µ–∫—Å—Ç–∞ —Å –ø–æ–º–æ—â—å—é –∞–≥–µ–Ω—Ç–∞"""
        try:
            import os
            import requests
            import logging
            
            logger = logging.getLogger(__name__)
            logger.info("–ó–∞–ø—É—Å–∫–∞–µ–º –∞–≥–µ–Ω—Ç–∞ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è abstract –∏–∑ OCR —Ç–µ–∫—Å—Ç–∞ –≤ SmartLibrarian")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ API –∫–ª—é—á –¥–ª—è –∞–≥–µ–Ω—Ç–∞
            api_key = os.getenv("PROXYAPI_KEY") or os.getenv("PROXYAPI_API_KEY") or os.getenv("OPEN_AI_API_KEY")
            if not api_key:
                logger.warning("API –∫–ª—é—á –Ω–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –∏—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç")
                return ocr_text
            
            logger.info("API –∫–ª—é—á –Ω–∞–π–¥–µ–Ω, –æ—Ç–ø—Ä–∞–≤–ª—è–µ–º –∑–∞–ø—Ä–æ—Å –∫ –∞–≥–µ–Ω—Ç—É")
            
            headers = {
                "Authorization": f"Bearer {api_key}",
                "Content-Type": "application/json"
            }
            
            # –ü—Ä–æ–º–ø—Ç –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è abstract
            prompt = f"""
–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —Å–ª–µ–¥—É—é—â–∏–π —Ç–µ–∫—Å—Ç, –∏–∑–≤–ª–µ—á–µ–Ω–Ω—ã–π —Å –ø–æ–º–æ—â—å—é OCR –∏–∑ PDF –¥–æ–∫—É–º–µ–Ω—Ç–∞.
–°–æ–∑–¥–∞–π –∫—Ä–∞—Ç–∫–∏–π –∏ –ø–æ–Ω—è—Ç–Ω—ã–π abstract (—Ä–µ–∑—é–º–µ) –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ.

–ò—Å—Ö–æ–¥–Ω—ã–π OCR —Ç–µ–∫—Å—Ç:
{ocr_text[:2000]}  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É –¥–ª—è API

–ó–∞–¥–∞—á–∞:
1. –û–ø—Ä–µ–¥–µ–ª–∏ –æ—Å–Ω–æ–≤–Ω—É—é —Ç–µ–º—É –¥–æ–∫—É–º–µ–Ω—Ç–∞
2. –í—ã–¥–µ–ª–∏ –∫–ª—é—á–µ–≤—ã–µ –º–æ–º–µ–Ω—Ç—ã –∏ –≤–∞–∂–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
3. –°–æ–∑–¥–∞–π —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π abstract
4. –ï—Å–ª–∏ —Ç–µ–∫—Å—Ç —Å–æ–¥–µ—Ä–∂–∏—Ç –º–Ω–æ–≥–æ –º—É—Å–æ—Ä–∞ OCR, –ø–æ–ø—Ä–æ–±—É–π –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–∏—Ç—å —Å–º—ã—Å–ª

–û—Ç–≤–µ—Ç—å —Ç–æ–ª—å–∫–æ abstract –±–µ–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤.
"""
            
            payload = {
                "model": "gpt-4o-mini",
                "messages": [
                    {
                        "role": "user",
                        "content": prompt
                    }
                ],
                "max_tokens": 1000,
                "temperature": 0.1
            }
            
            response = requests.post(
                "https://api.proxyapi.ru/openai/v1/chat/completions",
                headers=headers,
                json=payload,
                timeout=30
            )
            
            if response.status_code == 200:
                result = response.json()
                abstract = result['choices'][0]['message']['content']
                logger.info(f"‚úÖ Abstract —Å–æ–∑–¥–∞–Ω —Å –ø–æ–º–æ—â—å—é –∞–≥–µ–Ω—Ç–∞ –≤ SmartLibrarian, –¥–ª–∏–Ω–∞: {len(abstract)}")
                logger.info(f"–ü–µ—Ä–≤—ã–µ 200 —Å–∏–º–≤–æ–ª–æ–≤ abstract: {abstract[:200]}...")
                return abstract
            else:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ API –∞–≥–µ–Ω—Ç–∞: {response.status_code} - {response.text}")
                return ocr_text
                
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è abstract: {e}")
            return ocr_text
    
    def _create_smart_summary(self, text: str, file_name: str = None) -> str:
        """–°–æ–∑–¥–∞–Ω–∏–µ —É–º–Ω–æ–π –≤—ã–∂–∏–º–∫–∏ –∏–∑ —Ç–µ–∫—Å—Ç–∞ —Å —É—á–µ—Ç–æ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        if not text:
            return ""
        
        # –°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ –ø—Ä–∞–≤–∏–ª–∞ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –∞–±—Å—Ç—Ä–∞–∫—Ç–∞ –ø–æ —Ç–∏–ø—É –¥–æ–∫—É–º–µ–Ω—Ç–∞
        if file_name:
            if 'iridium' in file_name.lower() and 'sbd' in file_name.lower():
                # –î–ª—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ Iridium SBD —Å–æ–∑–¥–∞–µ–º –±–æ–ª–µ–µ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∞–±—Å—Ç—Ä–∞–∫—Ç
                lines = text.split('\n')
                important_lines = []
                for line in lines[:20]:  # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ 20 —Å—Ç—Ä–æ–∫
                    line = line.strip()
                    if line and len(line) > 10:  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –∫–æ—Ä–æ—Ç–∫–∏–µ —Å—Ç—Ä–æ–∫–∏
                        important_lines.append(line)
                
                if important_lines:
                    return '\n'.join(important_lines[:10])  # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –ø–µ—Ä–≤—ã–µ 10 –≤–∞–∂–Ω—ã—Ö —Å—Ç—Ä–æ–∫
            
            elif 'billmaster' in file_name.lower():
                # –î–ª—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ BillMaster
                return text[:800] + "..." if len(text) > 800 else text
        
        # –û–±—â–∏–π —Å–ª—É—á–∞–π - –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É —Ç–µ–∫—Å—Ç–∞
        if len(text) > 1000:
            return text[:1000] + "..."
        
        return text
    
    def analyze_document_images(self, file_path: Path) -> List[Dict]:
        """–ê–Ω–∞–ª–∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ —Å –ø–æ–º–æ—â—å—é VisionProcessor"""
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏
        model_status = self.vision_processor.check_model_availability()
        if not model_status.get('available', False):
            st.warning(f"Vision –º–æ–¥–µ–ª—å –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞: {model_status.get('message', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –æ—à–∏–±–∫–∞')}")
            return []
        
        try:
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏–∑ PDF
            extracted_images = self._extract_images_from_pdf(file_path)
            
            if not extracted_images:
                return []
            
            # –§–∏–ª—å—Ç—Ä—É–µ–º –º—É—Å–æ—Ä–Ω—ã–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
            filtered_images = [p for p in extracted_images if not self._is_image_noise(p)]
            
            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–∞–∂–¥–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
            image_analyses = []
            for image_path in filtered_images:
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º VisionProcessor –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
                analysis_result = self.vision_processor.analyze_image_with_gemini(image_path)
                
                if analysis_result['success']:
                    # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç –∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
                    text_content = self.vision_processor.extract_text_from_image_gemini(image_path)
                    
                    # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É –¥–æ–∫—É–º–µ–Ω—Ç–∞
                    structure_result = self.vision_processor.analyze_document_structure(image_path)
                    
                    image_analyses.append({
                        'image_path': str(image_path),
                        'image_name': image_path.name,
                        'description': analysis_result['analysis'][:200] + "..." if len(analysis_result['analysis']) > 200 else analysis_result['analysis'],
                        'analysis': analysis_result['analysis'],
                        'text_content': text_content,
                        'structure': structure_result.get('structure') if structure_result['success'] else None,
                        'model_used': analysis_result['model'],
                        'provider': analysis_result.get('provider', 'gemini')
                    })
                else:
                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ–± –æ—à–∏–±–∫–µ
                    image_analyses.append({
                        'image_path': str(image_path),
                        'image_name': image_path.name,
                        'description': '',
                        'analysis': '',
                        'error': analysis_result['error']
                    })
            
            return image_analyses
            
        except Exception as e:
            st.warning(f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –≤ {file_path.name}: {e}")
            return []

    def analyze_docx_images(self, docx_path: Path) -> List[Dict]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏–∑ DOCX –∏ –∏—Ö –∞–Ω–∞–ª–∏–∑ —Å –ø–æ–º–æ—â—å—é VisionProcessor"""
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏
        model_status = self.vision_processor.check_model_availability()
        if not model_status.get('available', False):
            st.warning(f"Vision –º–æ–¥–µ–ª—å –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞: {model_status.get('message', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –æ—à–∏–±–∫–∞')}")
            return []
        
        try:
            from docx import Document
            import shutil
            
            doc = Document(docx_path)
            extracted_images: List[Path] = []
            
            # 1) –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∫–∞–∫ –≤—Å—Ç—Ä–æ–µ–Ω–Ω—ã–µ –æ–±—ä–µ–∫—Ç—ã (inline shapes)
            for idx, shape in enumerate(getattr(doc.inline_shapes, '_inline_shapes', doc.inline_shapes)):
                try:
                    # –ü–æ–ª—É—á–∞–µ–º –±–∏–Ω–∞—Ä–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —á–µ—Ä–µ–∑ —Å–≤—è–∑–∞–Ω–Ω—ã–π —Ä–∏—Å—É–Ω–æ–∫
                    blip = shape._inline.graphic.graphicData.pic.blipFill.blip
                    rId = blip.embed
                    image_part = doc.part.related_parts[rId]
                    image_bytes = image_part.blob
                    
                    image_name = f"{docx_path.stem}_img_{idx + 1}.png"
                    image_path = self.images_dir / image_name
                    with open(image_path, 'wb') as f:
                        f.write(image_bytes)
                    extracted_images.append(image_path)
                except Exception:
                    # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –µ—Å–ª–∏ —Ñ–æ—Ä–º–∞—Ç –Ω–µ–æ–∂–∏–¥–∞–Ω–Ω—ã–π
                    continue
            
            # 2) –†–µ–∑–µ—Ä–≤–Ω—ã–π —Å–ø–æ—Å–æ–±: –ø—Ä–æ–π—Ç–∏ –ø–æ –≤—Å–µ–º media —á–∞—Å—Ç—è–º –¥–æ–∫—É–º–µ–Ω—Ç–∞ —Å —Ñ–∏–ª—å—Ç—Ä–æ–º –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
            try:
                seen_hashes = set()
                for rel in doc.part.rels.values():
                    if getattr(rel.target_part, 'content_type', '').startswith('image/'):
                        image_bytes = rel.target_part.blob
                        # –§–∏–ª—å—Ç—Ä –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –ø–æ —Ö—ç—à—É
                        try:
                            import hashlib
                            h = hashlib.sha256(image_bytes).hexdigest()
                            if h in seen_hashes:
                                continue
                            seen_hashes.add(h)
                        except Exception:
                            pass
                        image_name = f"{docx_path.stem}_media_{len(extracted_images) + 1}.png"
                        image_path = self.images_dir / image_name
                        with open(image_path, 'wb') as f:
                            f.write(image_bytes)
                        extracted_images.append(image_path)
            except Exception:
                pass
            
            if not extracted_images:
                return []
            
            # –§–∏–ª—å—Ç—Ä—É–µ–º –º—É—Å–æ—Ä–Ω—ã–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
            filtered_images = [p for p in extracted_images if not self._is_image_noise(p)]
            if not filtered_images:
                return []
            
            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —á–µ—Ä–µ–∑ VisionProcessor
            image_analyses: List[Dict] = []
            for image_path in filtered_images:
                analysis_result = self.vision_processor.analyze_image_with_gemini(image_path)
                if analysis_result.get('success'):
                    text_content = self.vision_processor.extract_text_from_image_gemini(image_path)
                    structure_result = self.vision_processor.analyze_document_structure(image_path)
                    image_analyses.append({
                        'image_path': str(image_path),
                        'image_name': image_path.name,
                        'description': (analysis_result.get('analysis', '')[:200] + "...") if len(analysis_result.get('analysis', '')) > 200 else analysis_result.get('analysis', ''),
                        'analysis': analysis_result.get('analysis', ''),
                        'text_content': text_content,
                        'structure': structure_result.get('structure') if structure_result.get('success') else None,
                        'model_used': analysis_result.get('model'),
                        'provider': analysis_result.get('provider', 'gemini')
                    })
                else:
                    image_analyses.append({
                        'image_path': str(image_path),
                        'image_name': image_path.name,
                        'description': '',
                        'analysis': '',
                        'error': analysis_result.get('error', 'analysis_failed')
                    })
            
            return image_analyses
        
        except ImportError:
            st.warning("python-docx –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install python-docx")
            return []
        except Exception as e:
            st.warning(f"–û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏–∑ DOCX: {e}")
            return []
    
    def _extract_images_from_pdf(self, pdf_path: Path) -> List[Path]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏–∑ PDF"""
        try:
            import fitz  # PyMuPDF
            
            doc = fitz.open(pdf_path)
            extracted_images = []
            
            for page_num in range(len(doc)):
                page = doc.load_page(page_num)
                image_list = page.get_images()
                
                for img_index, img in enumerate(image_list):
                    # –ü–æ–ª—É—á–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
                    xref = img[0]
                    pix = fitz.Pixmap(doc, xref)
                    
                    # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —Å –∞–ª—å—Ñ–∞-–∫–∞–Ω–∞–ª–æ–º
                    if pix.n - pix.alpha < 4:
                        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
                        image_name = f"{pdf_path.stem}_page_{page_num + 1}_img_{img_index + 1}.png"
                        image_path = self.images_dir / image_name
                        
                        if pix.alpha:
                            pix = fitz.Pixmap(fitz.csRGB, pix)
                        
                        pix.save(str(image_path))
                        extracted_images.append(image_path)
                        pix = None
            
            doc.close()
            return extracted_images
            
        except ImportError:
            st.warning("PyMuPDF –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install PyMuPDF")
            return []
        except Exception as e:
            st.warning(f"–û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: {e}")
            return []
    
    def _analyze_content(self, text: str, file_name: str = None) -> Dict:
        """–ê–Ω–∞–ª–∏–∑ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ —Ç–µ–∫—Å—Ç–∞"""
        text_lower = text.lower()
        
        # –°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ –ø—Ä–∞–≤–∏–ª–∞ –¥–ª—è –∏–∑–≤–µ—Å—Ç–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤ –ø–æ –Ω–∞–∑–≤–∞–Ω–∏—é
        filename_rules = {
            'billmaster_7.pdf': {
                'content_type': 'licenses',
                'category': '–õ–∏—Ü–µ–Ω–∑–∏–∏ –∏ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è',
                'description': '–õ–∏—Ü–µ–Ω–∑–∏—è –Ω–∞ –ø—Ä–æ–≥—Ä–∞–º–º–Ω–æ–µ –æ–±–µ—Å–ø–µ—á–µ–Ω–∏–µ BillMaster',
                'keywords': ['billmaster', '–ª–∏—Ü–µ–Ω–∑–∏—è', '–ø—Ä–æ–≥—Ä–∞–º–º–Ω–æ–µ –æ–±–µ—Å–ø–µ—á–µ–Ω–∏–µ'],
                'confidence': 0.9
            },
            'Iridium Short Burst Data Service Best Practices Guide Draft 091410_1.docx': {
                'content_type': 'technical_guides',
                'category': '–¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–∞',
                'description': '–†—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ –ø–æ –ª—É—á—à–∏–º –ø—Ä–∞–∫—Ç–∏–∫–∞–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —É—Å–ª—É–≥–∏ Iridium Short Burst Data Service',
                'keywords': ['iridium', 'sbd', 'short burst data', 'best practices', 'service guide', 'technical'],
                'confidence': 0.95
            }
        }
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ –ø—Ä–∞–≤–∏–ª–∞ –ø–æ –Ω–∞–∑–≤–∞–Ω–∏—é —Ñ–∞–π–ª–∞
        if file_name and file_name in filename_rules:
            rule_config = filename_rules[file_name]
            return {
                'content_type': rule_config['content_type'],
                'category': rule_config['category'],
                'description': rule_config['description'],
                'keywords': rule_config['keywords'],
                'confidence': rule_config['confidence']
            }
        
        # –°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ –ø—Ä–∞–≤–∏–ª–∞ –¥–ª—è –∏–∑–≤–µ—Å—Ç–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤ –ø–æ —Å–æ–¥–µ—Ä–∂–∏–º–æ–º—É
        special_rules = {
            'billmaster': {
                'content_type': 'licenses',
                'category': '–õ–∏—Ü–µ–Ω–∑–∏–∏ –∏ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è',
                'description': '–õ–∏—Ü–µ–Ω–∑–∏—è –Ω–∞ –ø—Ä–æ–≥—Ä–∞–º–º–Ω–æ–µ –æ–±–µ—Å–ø–µ—á–µ–Ω–∏–µ BillMaster',
                'keywords': ['billmaster', '–ª–∏—Ü–µ–Ω–∑–∏—è', '–ø—Ä–æ–≥—Ä–∞–º–º–Ω–æ–µ –æ–±–µ—Å–ø–µ—á–µ–Ω–∏–µ'],
                'confidence': 0.9
            },
            'iridium': {
                'content_type': 'technical_guides',
                'category': '–¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–∞',
                'description': '–¢–µ—Ö–Ω–∏—á–µ—Å–∫–æ–µ —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ –ø–æ —É—Å–ª—É–≥–∞–º Iridium',
                'keywords': ['iridium', 'sbd', 'short burst data', 'service', 'technical'],
                'confidence': 0.8
            }
        }
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ –ø—Ä–∞–≤–∏–ª–∞ –ø–æ —Å–æ–¥–µ—Ä–∂–∏–º–æ–º—É
        for rule_name, rule_config in special_rules.items():
            if rule_name in text_lower:
                return {
                    'content_type': rule_config['content_type'],
                    'category': rule_config['category'],
                    'description': rule_config['description'],
                    'keywords': rule_config['keywords'],
                    'confidence': rule_config['confidence']
                }
        
        # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è —Å –∫–ª—é—á–µ–≤—ã–º–∏ —Å–ª–æ–≤–∞–º–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–∏–ø–∞ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
        content_scores = {}
        for content_type, config in self.content_types.items():
            score = 0
            matched_keywords = []
            
            for keyword in config['keywords']:
                if keyword.lower() in text_lower:
                    score += 1
                    matched_keywords.append(keyword)
            
            content_scores[content_type] = {
                'score': score,
                'matched_keywords': matched_keywords,
                'category': config['category'],
                'description': config['description']
            }
        
        # –ù–∞—Ö–æ–¥–∏–º —Ç–∏–ø —Å –Ω–∞–∏–±–æ–ª—å—à–∏–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π
        best_match = max(content_scores.items(), key=lambda x: x[1]['score'])
        
        if best_match[1]['score'] > 0:
            return {
                'content_type': best_match[0],
                'category': best_match[1]['category'],
                'description': best_match[1]['description'],
                'keywords': best_match[1]['matched_keywords'],
                'confidence': min(best_match[1]['score'] / 5.0, 1.0)  # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –¥–æ 0-1
            }
        else:
            return {
                'content_type': 'unknown',
                'category': '–î—Ä—É–≥–æ–µ',
                'description': '–ù–µ —É–¥–∞–ª–æ—Å—å –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å —Ç–∏–ø –¥–æ–∫—É–º–µ–Ω—Ç–∞',
                'keywords': [],
                'confidence': 0.0
            }
    
    def _generate_recommendations(self, analysis: Dict) -> List[str]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –ø–æ –æ–±—Ä–∞–±–æ—Ç–∫–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        recommendations = []
        
        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ —Ñ–æ—Ä–º–∞—Ç—É
        if not analysis.get('format_supported', False):
            recommendations.append("‚ùå –§–æ—Ä–º–∞—Ç –¥–æ–∫—É–º–µ–Ω—Ç–∞ –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è")
            return recommendations
        
        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ —Ä–∞–∑–º–µ—Ä—É
        file_size_mb = analysis['file_size'] / (1024 * 1024)
        if file_size_mb > 50:
            recommendations.append("‚ö†Ô∏è –î–æ–∫—É–º–µ–Ω—Ç –æ—á–µ–Ω—å –±–æ–ª—å—à–æ–π (>50MB), –æ–±—Ä–∞–±–æ—Ç–∫–∞ –º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å –≤—Ä–µ–º—è")
        elif file_size_mb > 10:
            recommendations.append("‚ÑπÔ∏è –î–æ–∫—É–º–µ–Ω—Ç –±–æ–ª—å—à–æ–π (>10MB), —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è —Ä–∞–∑–±–∏—Ç—å –Ω–∞ —á–∞—Å—Ç–∏")
        
        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ —Å–æ–¥–µ—Ä–∂–∏–º–æ–º—É
        if analysis['confidence'] > 0.7:
            recommendations.append(f"‚úÖ –í—ã—Å–æ–∫–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –≤ —Ç–∏–ø–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞: {analysis['category']}")
        elif analysis['confidence'] > 0.3:
            recommendations.append(f"‚ö†Ô∏è –°—Ä–µ–¥–Ω—è—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –≤ —Ç–∏–ø–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞: {analysis['category']}")
        else:
            recommendations.append("‚ùì –ù–∏–∑–∫–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –≤ —Ç–∏–ø–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞, —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è —Ä—É—á–Ω–∞—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü–∏—è")
        
        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –æ–±—Ä–∞–±–æ—Ç–∫–µ
        if analysis['text_length'] > 10000:
            recommendations.append("üìÑ –î–æ–∫—É–º–µ–Ω—Ç —Å–æ–¥–µ—Ä–∂–∏—Ç –º–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞, —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è —Ä–∞–∑–±–∏—Ç—å –Ω–∞ —á–∞–Ω–∫–∏")
        
        if analysis['content_type'] == 'billing':
            recommendations.append("üí∞ –î–æ–∫—É–º–µ–Ω—Ç —Å–≤—è–∑–∞–Ω —Å –±–∏–ª–ª–∏–Ω–≥–æ–º, —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è —Å–æ–∑–¥–∞—Ç—å –æ—Ç–¥–µ–ª—å–Ω—É—é –ë–ó")
        
        if analysis['content_type'] == 'technical_regulations':
            recommendations.append("üìã –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–π —Ä–µ–≥–ª–∞–º–µ–Ω—Ç, —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è —Å–æ–∑–¥–∞—Ç—å –ë–ó –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –ø–æ–∏—Å–∫–∞")
        
        return recommendations
    
    def suggest_kb_strategy(self, documents: List[Dict]) -> Dict:
        """–ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ —Å–æ–∑–¥–∞–Ω–∏—è –ë–ó –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞–Ω–∞–ª–∏–∑–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
        if not documents:
            return {'type': 'no_documents', 'message': '–ù–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞'}
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –≤—Å–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã
        content_types = {}
        categories = {}
        
        for doc in documents:
            content_type = doc.get('content_type', 'unknown')
            category = doc.get('category', '–î—Ä—É–≥–æ–µ')
            
            content_types[content_type] = content_types.get(content_type, 0) + 1
            categories[category] = categories.get(category, 0) + 1
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Å—Ç—Ä–∞—Ç–µ–≥–∏—é
        total_docs = len(documents)
        unique_categories = len(categories)
        
        if unique_categories == 1:
            # –í—Å–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –æ–¥–Ω–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
            category = list(categories.keys())[0]
            strategy = {
                'type': 'single_kb',
                'category': category,
                'kb_name': f"–ë–ó: {category}",
                'description': f"–ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π –¥–ª—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ '{category}'",
                'documents': documents,
                'reasoning': f"–í—Å–µ {total_docs} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –æ—Ç–Ω–æ—Å—è—Ç—Å—è –∫ –æ–¥–Ω–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏: {category}"
            }
        elif unique_categories <= 3 and total_docs <= 10:
            # –ù–µ—Å–∫–æ–ª—å–∫–æ –∫–∞—Ç–µ–≥–æ—Ä–∏–π, –Ω–æ –º–∞–ª–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
            strategy = {
                'type': 'mixed_kb',
                'category': '–°–º–µ—à–∞–Ω–Ω–∞—è',
                'kb_name': f"–ë–ó: –°–º–µ—à–∞–Ω–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã ({total_docs} —Ñ–∞–π–ª–æ–≤)",
                'description': f"–ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π, —Å–æ–¥–µ—Ä–∂–∞—â–∞—è –¥–æ–∫—É–º–µ–Ω—Ç—ã —Ä–∞–∑–Ω—ã—Ö —Ç–∏–ø–æ–≤: {', '.join(categories.keys())}",
                'documents': documents,
                'reasoning': f"–ù–µ–±–æ–ª—å—à–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ ({total_docs}) —Ä–∞–∑–Ω—ã—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π, –º–æ–∂–Ω–æ –æ–±—ä–µ–¥–∏–Ω–∏—Ç—å –≤ –æ–¥–Ω—É –ë–ó"
            }
        else:
            # –ú–Ω–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Ä–∞–∑–Ω—ã—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π
            strategy = {
                'type': 'multiple_kb',
                'categories': categories,
                'kb_suggestions': [],
                'reasoning': f"–ú–Ω–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ ({total_docs}) —Ä–∞–∑–Ω—ã—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π, —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è —Å–æ–∑–¥–∞—Ç—å –æ—Ç–¥–µ–ª—å–Ω—ã–µ –ë–ó"
            }
            
            # –ü—Ä–µ–¥–ª–∞–≥–∞–µ–º –æ—Ç–¥–µ–ª—å–Ω—ã–µ –ë–ó –¥–ª—è –∫–∞–∂–¥–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
            for category, count in categories.items():
                if count > 0:
                    category_docs = [doc for doc in documents if doc.get('category') == category]
                    strategy['kb_suggestions'].append({
                        'category': category,
                        'kb_name': f"–ë–ó: {category}",
                        'description': f"–ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π –¥–ª—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ '{category}'",
                        'documents': category_docs,
                        'document_count': count
                    })
        
        return strategy
    
    def render_welcome(self):
        """–ü—Ä–∏–≤–µ—Ç—Å—Ç–≤–∏–µ –£–º–Ω–æ–≥–æ –±–∏–±–ª–∏–æ—Ç–µ–∫–∞—Ä—è"""
        st.markdown("""
        <div style="background: linear-gradient(90deg, #667eea 0%, #764ba2 100%); padding: 20px; border-radius: 10px; margin-bottom: 20px;">
            <h2 style="color: white; margin: 0; text-align: center;">üìö –£–º–Ω—ã–π –±–∏–±–ª–∏–æ—Ç–µ–∫–∞—Ä—å</h2>
            <p style="color: white; text-align: center; margin: 10px 0 0 0; font-size: 16px;">
                –í–∞—à –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–π –ø–æ–º–æ—â–Ω–∏–∫ –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏
            </p>
        </div>
        """, unsafe_allow_html=True)
        
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        self._render_document_statistics()
        
        col1, col2, col3 = st.columns(3)
        
        with col1:
            st.markdown("""
            **üîç –ê–Ω–∞–ª–∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤**
            - –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ñ–æ—Ä–º–∞—Ç–∞
            - –ê–Ω–∞–ª–∏–∑ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ
            - –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
            """)
        
        with col2:
            st.markdown("""
            **üìö –£–º–Ω–∞—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü–∏—è**
            - –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞
            - –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü–∏—è
            - –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ —Å—Ç—Ä—É–∫—Ç—É—Ä–µ –ë–ó
            """)
        
        with col3:
            st.markdown("""
            **üöÄ –ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞**
            - –°–æ–∑–¥–∞–Ω–∏–µ –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã—Ö –ë–ó
            - –†–∞—Å—à–∏—Ä–µ–Ω–∏–µ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –ë–ó
            - –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –ø–æ–∏—Å–∫–∞
            """)
    
    def _render_document_statistics(self):
        """–û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
        try:
            # –°–∫–∞–Ω–∏—Ä—É–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é uploads
            doc_status = self.document_manager.scan_upload_directory()
            
            col1, col2, col3, col4 = st.columns(4)
            
            with col1:
                st.metric(
                    label="üìÑ –ù–æ–≤—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã",
                    value=len(doc_status['new']),
                    delta=None
                )
            
            with col2:
                st.metric(
                    label="‚úÖ –û–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ",
                    value=len(doc_status['processed']),
                    delta=None
                )
            
            with col3:
                st.metric(
                    label="‚ùì –ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–µ",
                    value=len(doc_status['unknown']),
                    delta=None
                )
            
            # –ü–æ–ª—É—á–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ–± –∞—Ä—Ö–∏–≤–µ
            archive_info = self.document_manager.get_archive_info()
            with col4:
                st.metric(
                    label="üì¶ –í –∞—Ä—Ö–∏–≤–µ",
                    value=archive_info['total_files'],
                    delta=None
                )
            
            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —É–≤–µ–¥–æ–º–ª–µ–Ω–∏—è
            if len(doc_status['new']) > 0:
                st.info(f"üÜï –£ –≤–∞—Å –µ—Å—Ç—å {len(doc_status['new'])} –Ω–æ–≤—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏!")
            
            if len(doc_status['processed']) > 0:
                st.success(f"‚úÖ {len(doc_status['processed'])} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã –∏ –≥–æ—Ç–æ–≤—ã –∫ –∞—Ä—Ö–∏–≤–∏—Ä–æ–≤–∞–Ω–∏—é")
                
        except Exception as e:
            st.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {e}")
    
    @manual_transaction("kb_save_operation")
    def save_documents_to_kb_transactional(self, analyses: List[Dict], kb_id: int, 
                                         selected_doc_indices: List[int], 
                                         selected_image_indices: List[tuple],
                                         transaction_id: str = None) -> Dict:
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ KB —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π"""
        try:
            saved_count = 0
            
            for doc_idx in selected_doc_indices:
                analysis = analyses[doc_idx]
                
                # –ü–æ–ª—É—á–∞–µ–º –ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
                full_text = analysis.get('original_cleaned_text', analysis.get('full_cleaned_text', ''))
                if not full_text or (full_text.strip() == analysis.get('smart_summary', '').strip()):
                    full_text = analysis.get('raw_ocr_text', '')
                
                # –õ–æ–≥–∏—Ä—É–µ–º –æ–ø–µ—Ä–∞—Ü–∏—é –≤—Å—Ç–∞–≤–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞
                self.transaction_manager.log_database_change(
                    transaction_id, 'knowledge_documents', 'INSERT',
                    old_data=None, new_data={
                        'kb_id': kb_id,
                        'title': analysis.get('file_name', '–î–æ–∫—É–º–µ–Ω—Ç'),
                        'file_path': str(analysis.get('file_path', '')),
                        'content_type': analysis.get('content_type', 'text/plain'),
                        'file_size': analysis.get('file_size', 0),
                        'metadata': {
                            'ocr_cleaned': True,
                            'gemini_analyzed': bool(analysis.get('images')),
                            'text_length': len(full_text),
                            'summary_length': len(analysis.get('smart_summary', '')),
                            'content': full_text,
                            'summary': analysis.get('smart_summary', ''),
                            'processed': True,
                            'status': "processed"
                        }
                    }
                )
                
                # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π MIME type –ø–æ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—é
                ext_to_mime = {
                    '.pdf': 'application/pdf',
                    '.docx': 'application/vnd.openxmlformats-officedocument.wordprocessingml.document',
                    '.doc': 'application/msword',
                    '.txt': 'text/plain',
                    '.md': 'text/markdown',
                    '.rtf': 'application/rtf',
                    '.html': 'text/html',
                    '.xml': 'application/xml'
                }
                file_ext = analysis.get('file_extension', '').lower()
                mime_type = ext_to_mime.get(file_ext) or (mimetypes.guess_type(str(analysis.get('file_path', '')))[0] or 'application/octet-stream')

                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–æ–∫—É–º–µ–Ω—Ç
                doc_id = self.kb_manager.add_document(
                    kb_id=kb_id,
                    title=analysis.get('file_name', '–î–æ–∫—É–º–µ–Ω—Ç'),
                    file_path=str(analysis.get('file_path', '')),
                    content_type=mime_type,
                    file_size=analysis.get('file_size', 0),
                    metadata={
                        'ocr_cleaned': True,
                        'gemini_analyzed': bool(analysis.get('images')),
                        'text_length': len(full_text),
                        'summary_length': len(analysis.get('smart_summary', '')),
                        'content': full_text,
                        'summary': analysis.get('smart_summary', ''),
                        'processed': True,
                        'status': "processed",
                        'domain_content_type': analysis.get('content_type', 'unknown')
                        ,
                        'qa_count': int(analysis.get('qa_count', 0)),
                        'qa_sample': analysis.get('qa_pairs', [])[:20]
                    }
                )
                
                # –õ–æ–≥–∏—Ä—É–µ–º –æ–ø–µ—Ä–∞—Ü–∏—é –≤—Å—Ç–∞–≤–∫–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
                if analysis.get('images'):
                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–æ–ª—å–∫–æ –æ—Ç–º–µ—á–µ–Ω–Ω—ã–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
                    allowed = set(idx for (doc_idx_allowed, idx) in selected_image_indices if doc_idx_allowed == doc_idx)
                    for img_idx, image_info in enumerate(analysis['images']):
                        if img_idx not in allowed:
                            continue
                        self.transaction_manager.log_database_change(
                            transaction_id, 'knowledge_images', 'INSERT',
                            old_data=None, new_data={
                                'kb_id': kb_id,
                                'image_path': image_info.get('image_path', ''),
                                'image_name': image_info.get('image_name', ''),
                                'image_description': image_info.get('description', ''),
                                'llava_analysis': image_info.get('description', '')
                            }
                        )
                        
                        self.kb_manager.add_image(
                            kb_id=kb_id,
                            image_path=image_info.get('image_path', ''),
                            image_name=image_info.get('image_name', ''),
                            image_description=image_info.get('description', ''),
                            llava_analysis=image_info.get('description', ''),
                        )
                
                saved_count += 1
            
            # –õ–æ–≥–∏—Ä—É–µ–º –∞—Ä—Ö–∏–≤–∏—Ä–æ–≤–∞–Ω–∏–µ —Ñ–∞–π–ª–æ–≤
            for doc_idx in selected_doc_indices:
                analysis = analyses[doc_idx]
                file_path = Path(analysis.get('file_path', ''))
                if file_path.exists():
                    archive_path = self.document_manager.archive_dir / datetime.now().strftime('%Y-%m-%d') / file_path.name
                    archive_path.parent.mkdir(parents=True, exist_ok=True)
                    
                    self.transaction_manager.log_file_operation(
                        transaction_id, 'move',
                        source_path=str(file_path),
                        target_path=str(archive_path)
                    )
                    
                    # –ü–µ—Ä–µ–º–µ—â–∞–µ–º —Ñ–∞–π–ª –≤ –∞—Ä—Ö–∏–≤
                    shutil.move(str(file_path), str(archive_path))
            
            return {
                'success': True,
                'saved_count': saved_count,
                'transaction_id': transaction_id
            }
            
        except Exception as e:
            raise Exception(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≤ KB: {e}")
    
    def process_documents_smart(self, file_paths: List[Path]) -> Dict:
        """–£–º–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å –∞–Ω–∞–ª–∏–∑–æ–º –∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è–º–∏"""
        st.header("üß† –£–º–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –≤—Å–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã
        st.subheader("üìä –ê–Ω–∞–ª–∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
        analyses = []
        
        progress_bar = st.progress(0)
        status_text = st.empty()
        
        for i, file_path in enumerate(file_paths):
            status_text.text(f"–ê–Ω–∞–ª–∏–∑: {file_path.name}")
            analysis = self.analyze_document(file_path)
            analyses.append(analysis)
            progress_bar.progress((i + 1) / len(file_paths))
        
        status_text.text("")
        
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞ (—É—Å—Ç–æ–π—á–∏–≤–æ –∫ None)
        safe_analyses = [a for a in analyses if isinstance(a, dict)]
        self._display_analysis_results(safe_analyses)
        
        # –ü—Ä–µ–¥–ª–∞–≥–∞–µ–º —Å—Ç—Ä–∞—Ç–µ–≥–∏—é —Å–æ–∑–¥–∞–Ω–∏—è –ë–ó
        st.subheader("üéØ –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ —Å–æ–∑–¥–∞–Ω–∏—é –ë–ó")
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ –≤–∞–ª–∏–¥–Ω—ã–µ –∞–Ω–∞–ª–∏–∑—ã –¥–ª—è —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏
        strategy = self.suggest_kb_strategy(safe_analyses)
        
        # –û—Ç–ª–∞–¥–æ—á–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
        st.write(f"üîç DEBUG: strategy type = {strategy.get('type', 'unknown')}")
        st.write(f"üîç DEBUG: strategy keys = {list(strategy.keys())}")
        
        self._display_kb_strategy(strategy)
        
        return {
            'analyses': analyses,
            'strategy': strategy
        }
    
    def _display_analysis_results(self, analyses: List[Dict]):
        """–û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∞–Ω–∞–ª–∏–∑–∞"""
        for analysis in (analyses or []):
            if not isinstance(analysis, dict):
                continue
            # –°—Ç—Ä–∞—Ö–æ–≤–∫–∞ –æ—Ç –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏—Ö –∫–ª—é—á–µ–π
            analysis.setdefault('file_name', '–î–æ–∫—É–º–µ–Ω—Ç')
            analysis.setdefault('category', '–î—Ä—É–≥–æ–µ')
            analysis.setdefault('file_size', 0)
            analysis.setdefault('format_description', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')
            analysis.setdefault('content_type', 'unknown')
            analysis.setdefault('confidence', 0.0)
            analysis.setdefault('recommendations', [])
            with st.expander(f"üìÑ {analysis['file_name']} ({analysis['category']})"):
                col1, col2 = st.columns(2)
                
                with col1:
                    st.write(f"**–†–∞–∑–º–µ—Ä:** {analysis['file_size'] / 1024:.1f} KB")
                    st.write(f"**–§–æ—Ä–º–∞—Ç:** {analysis['format_description']}")
                    st.write(f"**–¢–∏–ø –∫–æ–Ω—Ç–µ–Ω—Ç–∞:** {analysis['content_type']}")
                    st.write(f"**–£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å:** {analysis['confidence']:.1%}")
                
                with col2:
                    st.write(f"**–ö–∞—Ç–µ–≥–æ—Ä–∏—è:** {analysis['category']}")
                    st.write(f"**–ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞:** {', '.join(analysis['keywords'][:5])}")
                    st.write(f"**–î–ª–∏–Ω–∞ —Ç–µ–∫—Å—Ç–∞:** {analysis.get('text_length', 0)} —Å–∏–º–≤–æ–ª–æ–≤")
                
                # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä–µ–≤—å—é –∏ –ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç
                if analysis.get('text_preview'):
                    st.write("**–ü—Ä–µ–≤—å—é —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ:**")
                    # –ò—Å–ø–æ–ª—å–∑—É–µ–º st.text –≤–º–µ—Å—Ç–æ st.text_area –¥–ª—è –∏–∑–±–µ–∂–∞–Ω–∏—è –º–∏–≥–∞–Ω–∏—è
                    st.text(analysis['text_preview'])
                    
                    # –ö–Ω–æ–ø–∫–∞ –¥–ª—è –ø—Ä–æ—Å–º–æ—Ç—Ä–∞ –ø–æ–ª–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
                    show_full_key = f"show_full_text_{analysis['file_name']}"
                    if st.button(f"üìÑ –ü–æ–∫–∞–∑–∞—Ç—å –ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç", key=f"full_text_{analysis['file_name']}"):
                        st.session_state[show_full_key] = True
                        st.rerun()
                    
                    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç –µ—Å–ª–∏ –∑–∞–ø—Ä–æ—à–µ–Ω
                    if st.session_state.get(f"show_full_text_{analysis['file_name']}", False):
                        st.write("**–ü–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç –¥–æ–∫—É–º–µ–Ω—Ç–∞:**")
                        
                        # –ö—ç—à–∏—Ä—É–µ–º –ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç –¥–ª—è –∏–∑–±–µ–∂–∞–Ω–∏—è –º–∏–≥–∞–Ω–∏—è
                        cache_key = f"full_text_cache_{analysis['file_name']}"
                        if cache_key not in st.session_state:
                            st.session_state[cache_key] = analysis.get('full_cleaned_text', analysis.get('text_preview', ''))
                        
                        full_text = st.session_state[cache_key]
                        
                        # –†–µ–¥–∞–∫—Ç–∏—Ä—É–µ–º–æ–µ –ø–æ–ª–µ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –≤—ã–∂–∏–º–∫–∏
                        edited_text = st.text_area(
                            "–†–µ–¥–∞–∫—Ç–∏—Ä—É–π—Ç–µ —Ç–µ–∫—Å—Ç –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –≤—ã–∂–∏–º–∫–∏:",
                            value=full_text,
                            height=300,
                            key=f"edit_text_{analysis['file_name']}",
                            help="–£–¥–∞–ª–∏—Ç–µ –Ω–µ–Ω—É–∂–Ω—ã–µ —á–∞—Å—Ç–∏, –æ—Å—Ç–∞–≤—å—Ç–µ —Ç–æ–ª—å–∫–æ –≤–∞–∂–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é"
                        )
                        
                        # –ö–Ω–æ–ø–∫–∏ –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å —Ç–µ–∫—Å—Ç–æ–º
                        col1, col2, col3 = st.columns(3)
                        
                        with col1:
                            if st.button(f"üíæ –°–æ—Ö—Ä–∞–Ω–∏—Ç—å –≤—ã–∂–∏–º–∫—É", key=f"save_{analysis['file_name']}"):
                                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ç—Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
                                st.session_state[f"saved_text_{analysis['file_name']}"] = edited_text
                                st.success("–í—ã–∂–∏–º–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞!")
                        
                        with col2:
                            if st.button(f"üìã –ö–æ–ø–∏—Ä–æ–≤–∞—Ç—å –≤ –±—É—Ñ–µ—Ä", key=f"copy_{analysis['file_name']}"):
                                st.code(edited_text, language=None)
                                st.info("–¢–µ–∫—Å—Ç —Å–∫–æ–ø–∏—Ä–æ–≤–∞–Ω –≤ –±—É—Ñ–µ—Ä –æ–±–º–µ–Ω–∞")
                        
                        with col3:
                            if st.button(f"‚ùå –ó–∞–∫—Ä—ã—Ç—å", key=f"close_{analysis['file_name']}"):
                                st.session_state[f"show_full_text_{analysis['file_name']}"] = False
                                st.rerun()
                        
                        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—É—é –≤—ã–∂–∏–º–∫—É –µ—Å–ª–∏ –µ—Å—Ç—å
                        if st.session_state.get(f"saved_text_{analysis['file_name']}"):
                            st.write("**–°–æ—Ö—Ä–∞–Ω–µ–Ω–Ω–∞—è –≤—ã–∂–∏–º–∫–∞:**")
                            st.text_area(
                                "–°–æ—Ö—Ä–∞–Ω–µ–Ω–Ω–∞—è –≤—ã–∂–∏–º–∫–∞",
                                st.session_state[f"saved_text_{analysis['file_name']}"],
                                height=150,
                                key=f"saved_{analysis['file_name']}",
                                label_visibility="collapsed"
                            )
                
                # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
                if analysis['recommendations']:
                    st.write("**–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏:**")
                    for rec in analysis['recommendations']:
                        st.write(f"‚Ä¢ {rec}")

            # –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ —á–∞–Ω–∫–∏–Ω–≥–∞
            try:
                text_preview = (analysis.get('original_cleaned_text') or '')[:4000]
                t = (text_preview or '').lower()
                qa_like = (t.count('?') >= 3 and ('‚Ä¢' in text_preview or '- ' in text_preview)) or ('–≤–æ–ø—Ä–æ—Å' in t and '–æ—Ç–≤–µ—Ç' in t) or ('question' in t and 'answer' in t)
                code_like = ('```' in text_preview) or (t.count('{') + t.count('}') > 10) or (t.count(';') > 20)
                st.write("**–°—Ç—Ä–∞—Ç–µ–≥–∏—è —á–∞–Ω–∫–∏–Ω–≥–∞ (–ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ):**")
                if qa_like:
                    st.write("- –†–∞–∑–±–∏–≤–∞—Ç—å –ø–æ –≤–æ–ø—Ä–æ—Å–∞–º: —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–∏ '?', –±—É–ª–ª–µ—Ç—ã ('‚Ä¢', '- '). –†–∞–∑–º–µ—Ä ~800, overlap ~120")
                elif code_like:
                    st.write("- –ö–æ–¥–æ–ø–æ–¥–æ–±–Ω—ã–π —Ç–µ–∫—Å—Ç: –º–µ–ª–∫–∏–µ —á–∞–Ω–∫–∏ ~600, —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–∏ –ø–æ —Å—Ç—Ä–æ–∫–∞–º/';', overlap ~80")
                else:
                    st.write("- –û–±—ã—á–Ω—ã–π —Ç–µ–∫—Å—Ç: —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ —á–∞–Ω–∫–∏ ~1000, overlap ~200")
            except Exception:
                pass

    def _display_kb_strategy(self, strategy: Dict):
        """–û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ —Å–æ–∑–¥–∞–Ω–∏—è –ë–ó"""
        st.write(f"üîç DEBUG: _display_kb_strategy called with type = {strategy.get('type', 'unknown')}")
        
        if strategy.get('type') == 'no_documents':
            st.warning(strategy.get('message', '–ù–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏'))
            return
        
        st.info(f"**–†–µ–∫–æ–º–µ–Ω–¥—É–µ–º–∞—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è:** {strategy['reasoning']}")
        
        if strategy['type'] == 'single_kb':
            st.success(f"‚úÖ –°–æ–∑–¥–∞—Ç—å –æ–¥–Ω—É –ë–ó: **{strategy['kb_name']}**")
            st.write(f"**–û–ø–∏—Å–∞–Ω–∏–µ:** {strategy['description']}")
            st.write(f"**–î–æ–∫—É–º–µ–Ω—Ç–æ–≤:** {len(strategy['documents'])}")
            
            if st.button("üöÄ –°–æ–∑–¥–∞—Ç—å –ë–ó", key="create_single_kb_btn"):
                self._create_kb_from_strategy(strategy)
            
            # –î–æ–±–∞–≤–∏—Ç—å –≤ —Å—É—â–µ—Å—Ç–≤—É—é—â—É—é –ë–ó
            try:
                existing = self.kb_manager.get_knowledge_bases(active_only=True) or []
                if existing:
                    kb_options = {f"ID {kb['id']}: {kb['name']} [{kb.get('category','')}]": kb['id'] for kb in existing}
                    selected_label = st.selectbox(
                        "–ò–ª–∏ –¥–æ–±–∞–≤–∏—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç—ã –≤ —Å—É—â–µ—Å—Ç–≤—É—é—â—É—é –ë–ó:",
                        list(kb_options.keys()),
                        key="single_kb_add_select"
                    )
                    if st.button("‚ûï –î–æ–±–∞–≤–∏—Ç—å –≤—ã–±—Ä–∞–Ω–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã", key="single_kb_add_btn"):
                        self._process_documents_to_kb(strategy.get('documents', []), kb_options[selected_label])
                        st.success("–î–æ–∫—É–º–µ–Ω—Ç—ã –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω—ã –Ω–∞ –æ–±—Ä–∞–±–æ—Ç–∫—É –≤ –≤—ã–±—Ä–∞–Ω–Ω—É—é –ë–ó")
                else:
                    st.info("–ê–∫—Ç–∏–≤–Ω—ã–µ –ë–ó –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
            except Exception:
                pass
            
            # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ—Å—Ç–æ–≤—ã—Ö –≤–æ–ø—Ä–æ—Å–æ–≤ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
            st.markdown("---")
            st.subheader("üß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏")
            
            if st.button("üéØ –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —Ç–µ—Å—Ç–æ–≤—ã–µ –≤–æ–ø—Ä–æ—Å—ã", key="generate_test_questions_btn"):
                with st.spinner("–ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ –≤–æ–ø—Ä–æ—Å—ã –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏..."):
                    try:
                        # –ü–æ–ª—É—á–∞–µ–º –∞–Ω–∞–ª–∏–∑ –ø–µ—Ä–≤–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–æ–ø—Ä–æ—Å–æ–≤
                        first_doc = strategy.get('documents', [{}])[0]
                        if first_doc and 'analysis' in first_doc:
                            test_questions = self.generate_relevance_test_questions(first_doc['analysis'])
                            
                            if test_questions:
                                st.success(f"‚úÖ –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ {len(test_questions)} —Ç–µ—Å—Ç–æ–≤—ã—Ö –≤–æ–ø—Ä–æ—Å–æ–≤")
                                
                                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤–æ–ø—Ä–æ—Å—ã –≤ session_state –¥–ª—è –ø–æ—Å–ª–µ–¥—É—é—â–µ–≥–æ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
                                st.session_state.generated_test_questions = test_questions
                                st.session_state.test_questions_kb_name = strategy['kb_name']
                                
                                # –û—Ç–æ–±—Ä–∞–∂–∞–µ–º –≤–æ–ø—Ä–æ—Å—ã
                                for i, question in enumerate(test_questions, 1):
                                    with st.expander(f"‚ùì –í–æ–ø—Ä–æ—Å {i}: {question['question']}"):
                                        st.write(f"**–ö–∞—Ç–µ–≥–æ—Ä–∏—è:** {question.get('category', '–ù–µ —É–∫–∞–∑–∞–Ω–∞')}")
                                        st.write(f"**–°–ª–æ–∂–Ω–æ—Å—Ç—å:** {question.get('difficulty', '–ù–µ —É–∫–∞–∑–∞–Ω–∞')}")
                                        st.write(f"**–û–∂–∏–¥–∞–µ–º—ã–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞:** {', '.join(question.get('expected_keywords', []))}")
                                        
                                st.info("üí° –í–æ–ø—Ä–æ—Å—ã –±—É–¥—É—Ç –¥–æ—Å—Ç—É–ø–Ω—ã –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –ø–æ—Å–ª–µ —Å–æ–∑–¥–∞–Ω–∏—è –ë–ó –∏–ª–∏ –¥–æ–±–∞–≤–ª–µ–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ —Å—É—â–µ—Å—Ç–≤—É—é—â—É—é –ë–ó")
                            else:
                                st.warning("–ù–µ —É–¥–∞–ª–æ—Å—å —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —Ç–µ—Å—Ç–æ–≤—ã–µ –≤–æ–ø—Ä–æ—Å—ã")
                        else:
                            st.error("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –∞–Ω–∞–ª–∏–∑–∞ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–æ–ø—Ä–æ—Å–æ–≤")
                    except Exception as e:
                        st.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ—Å—Ç–æ–≤—ã—Ö –≤–æ–ø—Ä–æ—Å–æ–≤: {e}")
        
        elif strategy['type'] == 'mixed_kb':
            st.warning(f"‚ö†Ô∏è –°–æ–∑–¥–∞—Ç—å —Å–º–µ—à–∞–Ω–Ω—É—é –ë–ó: **{strategy['kb_name']}**")
            st.write(f"**–û–ø–∏—Å–∞–Ω–∏–µ:** {strategy['description']}")
            st.write(f"**–î–æ–∫—É–º–µ–Ω—Ç–æ–≤:** {len(strategy['documents'])}")
            
            if st.button("üöÄ –°–æ–∑–¥–∞—Ç—å —Å–º–µ—à–∞–Ω–Ω—É—é –ë–ó", key="create_mixed_kb_btn"):
                self._create_kb_from_strategy(strategy)
            
            # –î–æ–±–∞–≤–∏—Ç—å –≤ —Å—É—â–µ—Å—Ç–≤—É—é—â—É—é –ë–ó (–≤—Å–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã)
            try:
                existing = self.kb_manager.get_knowledge_bases(active_only=True) or []
                if existing:
                    kb_options = {f"ID {kb['id']}: {kb['name']} [{kb.get('category','')}]": kb['id'] for kb in existing}
                    selected_label = st.selectbox(
                        "–ò–ª–∏ –¥–æ–±–∞–≤–∏—Ç—å —ç—Ç–∏ –¥–æ–∫—É–º–µ–Ω—Ç—ã –≤ —Å—É—â–µ—Å—Ç–≤—É—é—â—É—é –ë–ó:",
                        list(kb_options.keys()),
                        key="mixed_kb_add_select"
                    )
                    if st.button("‚ûï –î–æ–±–∞–≤–∏—Ç—å –≤—ã–±—Ä–∞–Ω–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã", key="mixed_kb_add_btn"):
                        self._process_documents_to_kb(strategy.get('documents', []), kb_options[selected_label])
                        st.success("–î–æ–∫—É–º–µ–Ω—Ç—ã –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω—ã –Ω–∞ –æ–±—Ä–∞–±–æ—Ç–∫—É –≤ –≤—ã–±—Ä–∞–Ω–Ω—É—é –ë–ó")
                else:
                    st.info("–ê–∫—Ç–∏–≤–Ω—ã–µ –ë–ó –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
            except Exception:
                pass
            
            # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ—Å—Ç–æ–≤—ã—Ö –≤–æ–ø—Ä–æ—Å–æ–≤ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
            st.markdown("---")
            st.subheader("üß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏")
            
            if st.button("üéØ –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —Ç–µ—Å—Ç–æ–≤—ã–µ –≤–æ–ø—Ä–æ—Å—ã", key="generate_test_questions_mixed_btn"):
                with st.spinner("–ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ –≤–æ–ø—Ä–æ—Å—ã –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏..."):
                    try:
                        # –ü–æ–ª—É—á–∞–µ–º –∞–Ω–∞–ª–∏–∑ –ø–µ—Ä–≤–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–æ–ø—Ä–æ—Å–æ–≤
                        first_doc = strategy.get('documents', [{}])[0]
                        if first_doc and 'analysis' in first_doc:
                            test_questions = self.generate_relevance_test_questions(first_doc['analysis'])
                            
                            if test_questions:
                                st.success(f"‚úÖ –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ {len(test_questions)} —Ç–µ—Å—Ç–æ–≤—ã—Ö –≤–æ–ø—Ä–æ—Å–æ–≤")
                                
                                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤–æ–ø—Ä–æ—Å—ã –≤ session_state –¥–ª—è –ø–æ—Å–ª–µ–¥—É—é—â–µ–≥–æ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
                                st.session_state.generated_test_questions = test_questions
                                st.session_state.test_questions_kb_name = strategy['kb_name']
                                
                                # –û—Ç–æ–±—Ä–∞–∂–∞–µ–º –≤–æ–ø—Ä–æ—Å—ã
                                for i, question in enumerate(test_questions, 1):
                                    with st.expander(f"‚ùì –í–æ–ø—Ä–æ—Å {i}: {question['question']}"):
                                        st.write(f"**–ö–∞—Ç–µ–≥–æ—Ä–∏—è:** {question.get('category', '–ù–µ —É–∫–∞–∑–∞–Ω–∞')}")
                                        st.write(f"**–°–ª–æ–∂–Ω–æ—Å—Ç—å:** {question.get('difficulty', '–ù–µ —É–∫–∞–∑–∞–Ω–∞')}")
                                        st.write(f"**–û–∂–∏–¥–∞–µ–º—ã–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞:** {', '.join(question.get('expected_keywords', []))}")
                                        
                                st.info("üí° –í–æ–ø—Ä–æ—Å—ã –±—É–¥—É—Ç –¥–æ—Å—Ç—É–ø–Ω—ã –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –ø–æ—Å–ª–µ —Å–æ–∑–¥–∞–Ω–∏—è –ë–ó –∏–ª–∏ –¥–æ–±–∞–≤–ª–µ–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ —Å—É—â–µ—Å—Ç–≤—É—é—â—É—é –ë–ó")
                            else:
                                st.warning("–ù–µ —É–¥–∞–ª–æ—Å—å —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —Ç–µ—Å—Ç–æ–≤—ã–µ –≤–æ–ø—Ä–æ—Å—ã")
                        else:
                            st.error("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –∞–Ω–∞–ª–∏–∑–∞ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–æ–ø—Ä–æ—Å–æ–≤")
                    except Exception as e:
                        st.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ—Å—Ç–æ–≤—ã—Ö –≤–æ–ø—Ä–æ—Å–æ–≤: {e}")
        
        elif strategy['type'] == 'multiple_kb':
            st.info("üìö –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è —Å–æ–∑–¥–∞—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ –ë–ó:")
            
            for kb_suggestion in strategy['kb_suggestions']:
                with st.expander(f"üìÅ {kb_suggestion['kb_name']} ({kb_suggestion['document_count']} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤)"):
                    st.write(f"**–û–ø–∏—Å–∞–Ω–∏–µ:** {kb_suggestion['description']}")
                    st.write(f"**–î–æ–∫—É–º–µ–Ω—Ç–æ–≤:** {kb_suggestion['document_count']}")
                    
                    if st.button(f"üöÄ –°–æ–∑–¥–∞—Ç—å –ë–ó: {kb_suggestion['category']}", key=f"create_multiple_{kb_suggestion['category']}_btn"):
                        self._create_kb_from_strategy(kb_suggestion)

                    # –î–æ–±–∞–≤–∏—Ç—å –∫–æ–Ω–∫—Ä–µ—Ç–Ω—É—é –∫–∞—Ç–µ–≥–æ—Ä–∏—é –≤ —Å—É—â–µ—Å—Ç–≤—É—é—â—É—é –ë–ó
                    try:
                        existing = self.kb_manager.get_knowledge_bases(active_only=True) or []
                        if existing:
                            kb_options = {f"ID {kb['id']}: {kb['name']} [{kb.get('category','')}]": kb['id'] for kb in existing}
                            selected_label = st.selectbox(
                                "–ò–ª–∏ –¥–æ–±–∞–≤–∏—Ç—å —ç—Ç–∏ –¥–æ–∫—É–º–µ–Ω—Ç—ã –≤ —Å—É—â–µ—Å—Ç–≤—É—é—â—É—é –ë–ó:",
                                list(kb_options.keys()),
                                key=f"multi_add_select_{kb_suggestion['category']}"
                            )
                            if st.button("‚ûï –î–æ–±–∞–≤–∏—Ç—å –≤—ã–±—Ä–∞–Ω–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã", key=f"multi_add_btn_{kb_suggestion['category']}"):
                                self._process_documents_to_kb(kb_suggestion.get('documents', []), kb_options[selected_label])
                                st.success("–î–æ–∫—É–º–µ–Ω—Ç—ã –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω—ã –Ω–∞ –æ–±—Ä–∞–±–æ—Ç–∫—É –≤ –≤—ã–±—Ä–∞–Ω–Ω—É—é –ë–ó")
                        else:
                            st.info("–ê–∫—Ç–∏–≤–Ω—ã–µ –ë–ó –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
                    except Exception:
                        pass
                    
                    # –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ –¥–ª—è —ç—Ç–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
                    if st.button(f"üß™ –¢–µ—Å—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ –¥–ª—è {kb_suggestion['category']}", key=f"test_relevance_{kb_suggestion['category']}"):
                        with st.spinner("–ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ –≤–æ–ø—Ä–æ—Å—ã –¥–ª—è —ç—Ç–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏..."):
                            try:
                                # –ü–æ–ª—É—á–∞–µ–º –∞–Ω–∞–ª–∏–∑ –ø–µ—Ä–≤–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞ —ç—Ç–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
                                first_doc = kb_suggestion.get('documents', [{}])[0]
                                if first_doc and 'analysis' in first_doc:
                                    test_questions = self.generate_relevance_test_questions(first_doc['analysis'])
                                    
                                    if test_questions:
                                        st.success(f"‚úÖ –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ {len(test_questions)} —Ç–µ—Å—Ç–æ–≤—ã—Ö –≤–æ–ø—Ä–æ—Å–æ–≤ –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ {kb_suggestion['category']}")
                                        
                                        # –û—Ç–æ–±—Ä–∞–∂–∞–µ–º –≤–æ–ø—Ä–æ—Å—ã
                                        for i, question in enumerate(test_questions, 1):
                                            with st.expander(f"‚ùì –í–æ–ø—Ä–æ—Å {i}: {question['question']}"):
                                                st.write(f"**–ö–∞—Ç–µ–≥–æ—Ä–∏—è:** {question.get('category', '–ù–µ —É–∫–∞–∑–∞–Ω–∞')}")
                                                st.write(f"**–°–ª–æ–∂–Ω–æ—Å—Ç—å:** {question.get('difficulty', '–ù–µ —É–∫–∞–∑–∞–Ω–∞')}")
                                                st.write(f"**–û–∂–∏–¥–∞–µ–º—ã–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞:** {', '.join(question.get('expected_keywords', []))}")
                                                
                                                # –ö–Ω–æ–ø–∫–∞ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è —ç—Ç–æ–≥–æ –≤–æ–ø—Ä–æ—Å–∞
                                                if st.button(f"üîç –ü—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å –≤–æ–ø—Ä–æ—Å {i}", key=f"test_question_multi_{kb_suggestion['category']}_{i}"):
                                                    self._test_single_question(question)
                                    else:
                                        st.warning("–ù–µ —É–¥–∞–ª–æ—Å—å —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —Ç–µ—Å—Ç–æ–≤—ã–µ –≤–æ–ø—Ä–æ—Å—ã")
                                else:
                                    st.error("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –∞–Ω–∞–ª–∏–∑–∞ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–æ–ø—Ä–æ—Å–æ–≤")
                            except Exception as e:
                                st.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ—Å—Ç–æ–≤—ã—Ö –≤–æ–ø—Ä–æ—Å–æ–≤: {e}")
    
    def _create_kb_from_strategy(self, strategy: Dict):
        """–°–æ–∑–¥–∞–Ω–∏–µ –ë–ó –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏"""
        try:
            # –°–æ–∑–¥–∞–µ–º –ë–ó
            kb_id = self.kb_manager.create_knowledge_base(
                name=strategy['kb_name'],
                description=strategy['description'],
                category=strategy.get('category', '–°–º–µ—à–∞–Ω–Ω–∞—è'),
                created_by=st.session_state.get('username', 'smart_agent')
            )
            
            st.success(f"‚úÖ –ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π '{strategy['kb_name']}' —Å–æ–∑–¥–∞–Ω–∞ —Å ID: {kb_id}")
            
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã
            documents = strategy.get('documents', [])
            if documents:
                self._process_documents_to_kb(documents, kb_id)
            
            # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Ñ–ª–∞–≥ —É—Å–ø–µ—à–Ω–æ–≥–æ —Å–æ–∑–¥–∞–Ω–∏—è –ë–ó
            st.session_state['kb_created_successfully'] = True
            st.session_state['created_kb_id'] = kb_id
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ –≤–æ–ø—Ä–æ—Å—ã –≤ –ë–ó –µ—Å–ª–∏ –æ–Ω–∏ –µ—Å—Ç—å
            if hasattr(st.session_state, 'generated_test_questions') and st.session_state.generated_test_questions:
                self._save_test_questions_to_kb(kb_id, st.session_state.generated_test_questions)
                self._show_relevance_testing_after_creation(kb_id, st.session_state.generated_test_questions)
            st.session_state['created_kb_name'] = strategy['kb_name']
            
        except Exception as e:
            st.error(f"–û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –ë–ó: {e}")
    
    def _process_documents_to_kb(self, documents: List[Dict], kb_id: int):
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ –ë–ó"""
        processed_count = 0
        progress_bar = st.progress(0)
        status_text = st.empty()
        
        for i, doc_analysis in enumerate(documents):
            try:
                file_path = Path(doc_analysis['file_path'])
                status_text.text(f"–û–±—Ä–∞–±–æ—Ç–∫–∞: {file_path.name}")
                
                # –ß–∏—Ç–∞–µ–º —Ñ–∞–π–ª
                with open(file_path, 'rb') as f:
                    file_content = f.read()
                
                # –°–æ–∑–¥–∞–µ–º mock –æ–±—ä–µ–∫—Ç –¥–ª—è PDFProcessor
                class MockUploadedFile:
                    def __init__(self, name, content):
                        self.name = name
                        self._content = content
                    
                    def getvalue(self):
                        return self._content
                
                mock_file = MockUploadedFile(file_path.name, file_content)
                
                # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç
                result = self.pdf_processor.process_pdf(
                    mock_file, 
                    kb_id, 
                    file_path.stem
                )
                
                if result['success']:
                    # –î–æ–±–∞–≤–ª—è–µ–º –¥–æ–∫—É–º–µ–Ω—Ç –≤ –ë–ó
                    doc_id = self.kb_manager.add_document(
                        kb_id,
                        result['title'],
                        result['file_path'],
                        result['content_type'],
                        result['file_size'],
                        result['metadata']
                    )
                    
                    # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç—É—Å
                    self.kb_manager.update_document_status(doc_id, True, 'completed')
                    processed_count += 1
                    
                    st.success(f"‚úÖ –û–±—Ä–∞–±–æ—Ç–∞–Ω: {file_path.name}")
                else:
                    st.error(f"‚ùå –û—à–∏–±–∫–∞: {file_path.name}")
                
            except Exception as e:
                st.error(f"‚ùå –û—à–∏–±–∫–∞: {file_path.name} - {e}")
            
            progress_bar.progress((i + 1) / len(documents))
        
        status_text.text("")
        st.success(f"üéâ –û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞! –£—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ {processed_count} –∏–∑ {len(documents)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ –≤–æ–ø—Ä–æ—Å—ã –≤ –ë–ó –µ—Å–ª–∏ –æ–Ω–∏ –µ—Å—Ç—å
        if hasattr(st.session_state, 'generated_test_questions') and st.session_state.generated_test_questions:
            self._save_test_questions_to_kb(kb_id, st.session_state.generated_test_questions)
            self._show_relevance_testing_after_creation(kb_id, st.session_state.generated_test_questions)
        
        # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É
        st.rerun()
