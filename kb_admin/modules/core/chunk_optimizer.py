"""
Chunk Optimization Module
–ú–æ–¥—É–ª—å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ —Ä–∞–∑–±–∏–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ –Ω–∞ —á–∞–Ω–∫–∏
"""

import re
from typing import List, Dict, Tuple, Any, Optional
from dataclasses import dataclass
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.schema import Document
from .text_analyzer import TextAnalyzer, TextStructure


@dataclass
class ChunkConfig:
    """–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Ä–∞–∑–±–∏–µ–Ω–∏—è –Ω–∞ —á–∞–Ω–∫–∏"""
    chunk_size: int
    chunk_overlap: int
    separators: List[str]
    length_function: str = "len"
    description: str = ""
    use_case: str = ""


@dataclass
class ChunkResult:
    """–†–µ–∑—É–ª—å—Ç–∞—Ç —Ä–∞–∑–±–∏–µ–Ω–∏—è –Ω–∞ —á–∞–Ω–∫–∏"""
    chunks: List[Document]
    config: ChunkConfig
    metrics: Dict[str, Any]
    quality_score: float


class ChunkOptimizer:
    """–û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä —Ä–∞–∑–±–∏–µ–Ω–∏—è –Ω–∞ —á–∞–Ω–∫–∏"""
    
    def __init__(self):
        self.text_analyzer = TextAnalyzer()
        
        # –ü—Ä–µ–¥—É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—ã–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
        self.preset_configs = {
            "regulations": ChunkConfig(
                chunk_size=600,
                chunk_overlap=100,
                separators=["\n\n", "\n", ". ", "! ", "? ", ", ", " ", ""],
                description="–†–µ–≥–ª–∞–º–µ–Ω—Ç—ã –∏ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∞—è –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è",
                use_case="technical_docs"
            ),
            "manuals": ChunkConfig(
                chunk_size=800,
                chunk_overlap=150,
                separators=["\n\n", "\n", ". ", "! ", "? ", "; ", ", ", " ", ""],
                description="–†—É–∫–æ–≤–æ–¥—Å—Ç–≤–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è",
                use_case="user_manuals"
            ),
            "faq": ChunkConfig(
                chunk_size=400,
                chunk_overlap=80,
                separators=["\n\n", "\n", "? ", ". ", " ", ""],
                description="–ß–∞—Å—Ç–æ –∑–∞–¥–∞–≤–∞–µ–º—ã–µ –≤–æ–ø—Ä–æ—Å—ã",
                use_case="faq_docs"
            ),
            "api_docs": ChunkConfig(
                chunk_size=700,
                chunk_overlap=120,
                separators=["\n\n", "\n", "```", "## ", "# ", ". ", " ", ""],
                description="–î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è API",
                use_case="api_documentation"
            ),
            "legal": ChunkConfig(
                chunk_size=1000,
                chunk_overlap=200,
                separators=["\n\n", "\n", "–°—Ç–∞—Ç—å—è ", "–ü—É–Ω–∫—Ç ", ". ", " ", ""],
                description="–ü—Ä–∞–≤–æ–≤—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã",
                use_case="legal_documents"
            )
        }

    # –ó–∞–≥–ª—É—à–∫–∏ –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å –º–µ–Ω–µ–¥–∂–µ—Ä–æ–º –º–æ–¥–µ–ª–µ–π UI
    def get_chat_backend_info(self) -> Dict[str, Any]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø—É—Å—Ç—É—é –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é, —Ç.–∫. –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç LLM –Ω–∞–ø—Ä—è–º—É—é."""
        return {"provider": "n/a", "model": "n/a"}

    def set_chat_backend(self, provider: str, model: str, **kwargs) -> None:
        """–ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º, —Ç.–∫. –º–æ–¥—É–ª—å –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç LLM –Ω–∞–ø—Ä—è–º—É—é."""
        return
    
    def optimize_chunking(self, text: str, document_type: str = "auto") -> ChunkResult:
        """–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è —Ä–∞–∑–±–∏–µ–Ω–∏—è –Ω–∞ —á–∞–Ω–∫–∏"""
        
        # –ê–Ω–∞–ª–∏–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã —Ç–µ–∫—Å—Ç–∞
        structure = self.text_analyzer.analyze_text_structure(text)
        
        # –í—ã–±–æ—Ä –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
        if document_type == "auto":
            config = self._auto_select_config(structure)
        elif document_type in self.preset_configs:
            config = self.preset_configs[document_type]
        else:
            config = self._create_custom_config(structure)
        
        # –†–∞–∑–±–∏–µ–Ω–∏–µ –Ω–∞ —á–∞–Ω–∫–∏
        chunks = self._split_text(text, config)
        
        # –ê–Ω–∞–ª–∏–∑ –∫–∞—á–µ—Å—Ç–≤–∞
        metrics = self._analyze_chunk_quality(chunks, structure)
        quality_score = self._calculate_quality_score(metrics, structure)
        
        return ChunkResult(
            chunks=chunks,
            config=config,
            metrics=metrics,
            quality_score=quality_score
        )
    
    def compare_configurations(self, text: str, configs: List[str] = None) -> Dict[str, ChunkResult]:
        """–°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–π —Ä–∞–∑–±–∏–µ–Ω–∏—è"""
        if configs is None:
            configs = list(self.preset_configs.keys())
        
        results = {}
        
        for config_name in configs:
            if config_name in self.preset_configs:
                config = self.preset_configs[config_name]
                chunks = self._split_text(text, config)
                metrics = self._analyze_chunk_quality(chunks, self.text_analyzer.analyze_text_structure(text))
                quality_score = self._calculate_quality_score(metrics, self.text_analyzer.analyze_text_structure(text))
                
                results[config_name] = ChunkResult(
                    chunks=chunks,
                    config=config,
                    metrics=metrics,
                    quality_score=quality_score
                )
        
        return results
    
    def _auto_select_config(self, structure: TextStructure) -> ChunkConfig:
        """–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –≤—ã–±–æ—Ä –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞–Ω–∞–ª–∏–∑–∞"""
        
        # –õ–æ–≥–∏–∫–∞ –≤—ã–±–æ—Ä–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã
        if structure.has_numbered_sections and structure.complexity_score > 0.6:
            return self.preset_configs["regulations"]
        elif structure.average_paragraph_length < 300 and structure.paragraph_count > 10:
            return self.preset_configs["faq"]
        elif structure.total_length > 20000 and structure.complexity_score > 0.5:
            return self.preset_configs["legal"]
        elif "API" in structure or "http" in structure.lower():
            return self.preset_configs["api_docs"]
        else:
            return self.preset_configs["manuals"]
    
    def _create_custom_config(self, structure: TextStructure) -> ChunkConfig:
        """–°–æ–∑–¥–∞–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏"""
        return ChunkConfig(
            chunk_size=structure.recommended_chunk_size,
            chunk_overlap=structure.recommended_overlap,
            separators=["\n\n", "\n", ". ", "! ", "? ", ", ", " ", ""],
            length_function="len",
            description="–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è",
            use_case="custom"
        )
    
    def _split_text(self, text: str, config: ChunkConfig) -> List[Document]:
        """–†–∞–∑–±–∏–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ —á–∞–Ω–∫–∏"""
        splitter = RecursiveCharacterTextSplitter(
            chunk_size=config.chunk_size,
            chunk_overlap=config.chunk_overlap,
            length_function=len,
            separators=config.separators
        )
        
        # –°–æ–∑–¥–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç
        doc = Document(page_content=text, metadata={"source": "text_analysis"})
        
        # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —á–∞–Ω–∫–∏
        chunks = splitter.split_documents([doc])
        
        return chunks
    
    def _analyze_chunk_quality(self, chunks: List[Document], structure: TextStructure) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ –∫–∞—á–µ—Å—Ç–≤–∞ —á–∞–Ω–∫–æ–≤"""
        if not chunks:
            return {"error": "No chunks created"}
        
        chunk_sizes = [len(chunk.page_content) for chunk in chunks]
        
        metrics = {
            "total_chunks": len(chunks),
            "avg_chunk_size": sum(chunk_sizes) / len(chunk_sizes),
            "min_chunk_size": min(chunk_sizes),
            "max_chunk_size": max(chunk_sizes),
            "size_variance": self._calculate_variance(chunk_sizes),
            "coverage_ratio": sum(chunk_sizes) / structure.total_length if structure.total_length > 0 else 0,
            "overlap_efficiency": self._calculate_overlap_efficiency(chunks),
            "context_preservation": self._calculate_context_preservation(chunks, structure)
        }
        
        return metrics
    
    def _calculate_variance(self, values: List[float]) -> float:
        """–†–∞—Å—á–µ—Ç –¥–∏—Å–ø–µ—Ä—Å–∏–∏"""
        if len(values) < 2:
            return 0.0
        
        mean = sum(values) / len(values)
        variance = sum((x - mean) ** 2 for x in values) / len(values)
        return variance
    
    def _calculate_overlap_efficiency(self, chunks: List[Document]) -> float:
        """–†–∞—Å—á–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏—è"""
        if len(chunks) < 2:
            return 1.0
        
        # –ü—Ä–æ—Å—Ç–∞—è –º–µ—Ç—Ä–∏–∫–∞: —á–µ–º –º–µ–Ω—å—à–µ —Ä–∞–∑–±—Ä–æ—Å —Ä–∞–∑–º–µ—Ä–æ–≤, —Ç–µ–º –ª—É—á—à–µ
        sizes = [len(chunk.page_content) for chunk in chunks]
        avg_size = sum(sizes) / len(sizes)
        variance = sum((s - avg_size) ** 2 for s in sizes) / len(sizes)
        
        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –∫ 0-1 (—á–µ–º –º–µ–Ω—å—à–µ –¥–∏—Å–ø–µ—Ä—Å–∏—è, —Ç–µ–º –ª—É—á—à–µ)
        max_variance = avg_size ** 2  # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –≤–æ–∑–º–æ–∂–Ω–∞—è –¥–∏—Å–ø–µ—Ä—Å–∏—è
        efficiency = 1.0 - (variance / max_variance) if max_variance > 0 else 1.0
        
        return max(0.0, min(1.0, efficiency))
    
    def _calculate_context_preservation(self, chunks: List[Document], structure: TextStructure) -> float:
        """–†–∞—Å—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞"""
        if not chunks:
            return 0.0
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å–æ—Ö—Ä–∞–Ω—è—é—Ç—Å—è –ª–∏ –≥—Ä–∞–Ω–∏—Ü—ã —Å–µ–∫—Ü–∏–π
        preserved_sections = 0
        total_sections = structure.section_count
        
        if total_sections == 0:
            return 1.0  # –ï—Å–ª–∏ –Ω–µ—Ç —Å–µ–∫—Ü–∏–π, –∫–æ–Ω—Ç–µ–∫—Å—Ç —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç—Å—è –ø–æ–ª–Ω–æ—Å—Ç—å—é
        
        for chunk in chunks:
            content = chunk.page_content
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å–æ–¥–µ—Ä–∂–∏—Ç –ª–∏ —á–∞–Ω–∫ –Ω–∞—á–∞–ª–æ —Å–µ–∫—Ü–∏–∏
            for pattern in self.text_analyzer.section_patterns:
                if re.search(pattern, content):
                    preserved_sections += 1
                    break
        
        return preserved_sections / total_sections if total_sections > 0 else 1.0
    
    def _calculate_quality_score(self, metrics: Dict[str, Any], structure: TextStructure) -> float:
        """–†–∞—Å—á–µ—Ç –æ–±—â–µ–≥–æ –±–∞–ª–ª–∞ –∫–∞—á–µ—Å—Ç–≤–∞"""
        if "error" in metrics:
            return 0.0
        
        # –í–µ—Å–∞ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö –º–µ—Ç—Ä–∏–∫
        weights = {
            "coverage_ratio": 0.3,      # –ü–æ–∫—Ä—ã—Ç–∏–µ —Ç–µ–∫—Å—Ç–∞
            "overlap_efficiency": 0.25,  # –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏—è
            "context_preservation": 0.25, # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
            "size_consistency": 0.2      # –ö–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—å —Ä–∞–∑–º–µ—Ä–æ–≤
        }
        
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –º–µ—Ç—Ä–∏–∫
        coverage_score = min(1.0, metrics["coverage_ratio"])
        overlap_score = metrics["overlap_efficiency"]
        context_score = metrics["context_preservation"]
        
        # –ö–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—å —Ä–∞–∑–º–µ—Ä–æ–≤ (–æ–±—Ä–∞—Ç–Ω–∞—è –¥–∏—Å–ø–µ—Ä—Å–∏–∏)
        size_consistency = 1.0 - (metrics["size_variance"] / (metrics["avg_chunk_size"] ** 2))
        size_consistency = max(0.0, min(1.0, size_consistency))
        
        # –í–∑–≤–µ—à–µ–Ω–Ω–∞—è —Å—É–º–º–∞
        quality_score = (
            weights["coverage_ratio"] * coverage_score +
            weights["overlap_efficiency"] * overlap_score +
            weights["context_preservation"] * context_score +
            weights["size_consistency"] * size_consistency
        )
        
        return quality_score
    
    def get_optimization_recommendations(self, result: ChunkResult) -> List[str]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –ø–æ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏"""
        recommendations = []
        
        metrics = result.metrics
        quality_score = result.quality_score
        
        if quality_score < 0.6:
            recommendations.append("‚ö†Ô∏è –ù–∏–∑–∫–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ —Ä–∞–∑–±–∏–µ–Ω–∏—è. –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –∏–∑–º–µ–Ω–∏—Ç—å –ø–∞—Ä–∞–º–µ—Ç—Ä—ã.")
        
        if metrics["coverage_ratio"] < 0.9:
            recommendations.append("üìÑ –ù–µ–ø–æ–ª–Ω–æ–µ –ø–æ–∫—Ä—ã—Ç–∏–µ —Ç–µ–∫—Å—Ç–∞. –£–≤–µ–ª–∏—á—å—Ç–µ —Ä–∞–∑–º–µ—Ä —á–∞–Ω–∫–æ–≤ –∏–ª–∏ –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ.")
        
        if metrics["overlap_efficiency"] < 0.7:
            recommendations.append("üîÑ –ù–∏–∑–∫–∞—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏—è. –û–ø—Ç–∏–º–∏–∑–∏—Ä—É–π—Ç–µ —Ä–∞–∑–º–µ—Ä –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏—è.")
        
        if metrics["context_preservation"] < 0.8:
            recommendations.append("üîó –ü–ª–æ—Ö–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞. –ò–∑–º–µ–Ω–∏—Ç–µ —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–∏ –∏–ª–∏ —Ä–∞–∑–º–µ—Ä —á–∞–Ω–∫–æ–≤.")
        
        if metrics["size_variance"] > metrics["avg_chunk_size"] * 0.5:
            recommendations.append("üìè –ë–æ–ª—å—à–æ–π —Ä–∞–∑–±—Ä–æ—Å —Ä–∞–∑–º–µ—Ä–æ–≤ —á–∞–Ω–∫–æ–≤. –£–ª—É—á—à–∏—Ç–µ —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–∏.")
        
        if not recommendations:
            recommendations.append("‚úÖ –ö–∞—á–µ—Å—Ç–≤–æ —Ä–∞–∑–±–∏–µ–Ω–∏—è —Ö–æ—Ä–æ—à–µ–µ!")
        
        return recommendations
