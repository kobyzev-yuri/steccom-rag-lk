import json
from typing import List, Dict, Tuple, Union
from pathlib import Path
import numpy as np
import os
import warnings
from sentence_transformers import SentenceTransformer
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_community.chat_models import ChatOllama
from langchain.prompts import ChatPromptTemplate
from langchain.schema.runnable import RunnablePassthrough
from langchain.schema import StrOutputParser
from langchain.schema import Document

class RAGHelper:
    def __init__(self):
        # Suppress deprecation warnings from legacy providers used only here
        try:
            from langchain_core._api.deprecation import LangChainDeprecationWarning
            warnings.filterwarnings("ignore", category=LangChainDeprecationWarning)
        except Exception:
            warnings.filterwarnings("ignore", category=DeprecationWarning)

        # Embeddings: use a local SentenceTransformer adapter to avoid torch_dtype issues
        embedding_model = os.getenv("EMBEDDING_MODEL", "intfloat/multilingual-e5-base")
        device = os.getenv("EMBEDDING_DEVICE", "cpu")

        class LocalHFEmbeddings:
            def __init__(self, model_name: str, device: str = "cpu", normalize: bool = True):
                self.model = SentenceTransformer(model_name, device=device)
                self.normalize = normalize

            def embed_documents(self, texts: List[str]) -> List[List[float]]:
                if not texts:
                    return []
                vectors = self.model.encode(
                    texts,
                    normalize_embeddings=self.normalize,
                    convert_to_numpy=True,
                    show_progress_bar=False,
                    batch_size=32,
                )
                return vectors.tolist()

            def embed_query(self, text: str) -> List[float]:
                vectors = self.model.encode(
                    [text],
                    normalize_embeddings=self.normalize,
                    convert_to_numpy=True,
                    show_progress_bar=False,
                    batch_size=1,
                )
                return vectors[0].tolist()

        self.embeddings = LocalHFEmbeddings(embedding_model, device=device, normalize=True)
        print(f"ðŸ”§ RAGHelper embeddings: provider=sentence-transformers model={embedding_model} device={device}")

        # Chat model (kept as Ollama unless overridden in the main app)
        self.chat_model = ChatOllama(model=os.getenv("OLLAMA_CHAT_MODEL", "qwen3:8b"))
        self.vectorstore = None
        self.documents = []
        self.initialize_vectorstore()

    def load_json_data(self) -> Union[List[Dict], List[str]]:
        """Load and parse steccom.json and optional docs/kb/*.json files"""
        try:
            aggregated: List[Union[Dict, str]] = []

            # Primary KB file at project root
            json_path = Path("steccom.json")
            if json_path.exists():
                with open(json_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                if isinstance(data, list):
                    aggregated.extend(data)
                elif isinstance(data, dict) or isinstance(data, str):
                    aggregated.append(data)
            else:
                print("Warning: steccom.json not found, relying on built-in docs and docs/kb/")

            # Additional KB files under docs/kb/*.json
            kb_dir = Path("docs/kb")
            if kb_dir.exists() and kb_dir.is_dir():
                for kb_file in sorted(kb_dir.glob("*.json")):
                    try:
                        with open(kb_file, 'r', encoding='utf-8') as f:
                            kb_data = json.load(f)
                        if isinstance(kb_data, list):
                            for it in kb_data:
                                if isinstance(it, dict):
                                    it = {**it, "_kb_file": str(kb_file)}
                                aggregated.append(it)
                        elif isinstance(kb_data, dict) or isinstance(kb_data, str):
                            if isinstance(kb_data, dict):
                                kb_data = {**kb_data, "_kb_file": str(kb_file)}
                            aggregated.append(kb_data)
                        print(f"Loaded KB file: {kb_file}")
                    except Exception as e:
                        print(f"Warning: failed to load KB file {kb_file}: {e}")

            return aggregated
        except json.JSONDecodeError as e:
            print(f"Warning: Error parsing steccom.json: {e}")
            return []
        except Exception as e:
            print(f"Warning: Error loading steccom.json: {e}")
            return []

    def process_json_to_documents(self, data: Union[List[Dict], List[str]]) -> List[Document]:
        """Convert JSON data into text documents with enhanced personal account focus.
        Honors optional metadata fields: audience (user/admin), scope (current_billing/legacy_billing), status.
        """
        documents = []
        
        # NOTE: Built-in guide/examples/troubleshooting/technical removed.
        # They must now live in docs/kb/*.json and will be loaded via load_json_data.
        
        # Process original documentation if available
        if isinstance(data, list) and data:
            for item in data:
                content = ""
                if isinstance(item, dict):
                    # Handle dictionary items
                    content += f"Title: {item.get('title', '')}\n"
                    content += f"Description: {item.get('description', '')}\n"
                    
                    if 'content' in item and isinstance(item['content'], list):
                        for section in item['content']:
                            if isinstance(section, dict):
                                content += f"\nSection: {section.get('title', '')}\n"
                                content += f"{section.get('text', '')}\n"
                                
                                if 'subsections' in section and isinstance(section['subsections'], list):
                                    for subsection in section['subsections']:
                                        if isinstance(subsection, dict):
                                            content += f"\nSubsection: {subsection.get('title', '')}\n"
                                            content += f"{subsection.get('text', '')}\n"
                elif isinstance(item, str):
                    # Handle string items
                    content = item
                
                if content.strip():
                    metadata: Dict = {"source": "steccom"}
                    if isinstance(item, dict):
                        # propagate role/scope metadata if present
                        if 'audience' in item:
                            metadata['audience'] = item['audience']
                        if 'scope' in item:
                            metadata['scope'] = item['scope']
                        if 'status' in item:
                            metadata['status'] = item['status']
                        if '_kb_file' in item:
                            metadata['kb_file'] = item['_kb_file']
                        if 'title' in item:
                            metadata['title'] = item['title']
                    documents.append(Document(page_content=content, metadata=metadata))
        
        return documents

    def initialize_vectorstore(self):
        """Initialize FAISS vectorstore with documents"""
        try:
            json_data = self.load_json_data()
            documents = self.process_json_to_documents(json_data)
            
            text_splitter = RecursiveCharacterTextSplitter(
                chunk_size=1000,
                chunk_overlap=200,
                length_function=len,
                separators=["\n\n", "\n", ".", "!", "?", ",", " ", ""]
            )
            
            chunks = text_splitter.split_documents(documents)
            self.vectorstore = FAISS.from_documents(chunks, self.embeddings)
            self.documents = documents
            print("RAG system initialized successfully with", len(documents), "documents")
            
        except Exception as e:
            print(f"Error initializing vectorstore: {e}")
            raise

    def format_docs(self, docs: List[Document]) -> str:
        """Format documents into a single string context"""
        return "\n\n".join(doc.page_content for doc in docs)

    def get_query_suggestion(self, user_question: str, company: str) -> Tuple[str, str]:
        """Generate SQL query suggestion based on user question and company"""
        template = """You are a SQL expert helping users create queries to analyze their company data.
        User can only see data for their company ({company}).
        
        Available tables and constraints:
        1. service_types - types of services
           - id
           - name (SBD, VSAT_DATA, VSAT_VOICE)
           - unit (KB, MB, minutes)
           - description
        
        2. tariffs - tariff plans
           - id
           - service_type_id
           - name
           - price_per_unit
           - monthly_fee
           - traffic_limit
           - is_active
        
        3. agreements - filter: user_id IN (SELECT id FROM users WHERE company = '{company}')
           - id
           - user_id
           - tariff_id
           - start_date
           - end_date
           - status
        
        4. devices - filter: user_id IN (SELECT id FROM users WHERE company = '{company}')
           - imei
           - user_id
           - device_type
           - model
           - activated_at
        
        5. sessions - usage sessions
           - id
           - imei
           - service_type_id
           - session_start
           - session_end
           - usage_amount
        
        6. billing_records - join through devices or agreements
           - id
           - agreement_id
           - imei
           - service_type_id
           - billing_date
           - usage_amount
           - amount
           - paid
           - payment_date
        
        7. users - used for company filtering
           - id
           - username
           - company
           - role
        
        Important rules:
        1. Use only English names for column aliases
        2. Use short table aliases (a for agreements, d for devices, etc.)
        3. Always qualify column names with table aliases
        4. Format dates as YYYY-MM-DD
        5. Use simple quotes for string values
        6. Never use Russian text in SQL
        7. For company filtering, always join with users table
        8. Return ONLY the SQL query, no explanations or thinking
        
        Example query for current agreement:
        SELECT 
            t.name as tariff_name,
            st.name as service_type,
            st.unit as unit,
            t.price_per_unit as price_per_unit,
            t.monthly_fee as monthly_fee,
            t.traffic_limit as traffic_limit,
            a.start_date as start_date,
            a.end_date as end_date,
            a.status as status
        FROM agreements a
        JOIN users u ON a.user_id = u.id
        JOIN tariffs t ON a.tariff_id = t.id
        JOIN service_types st ON t.service_type_id = st.id
        WHERE u.company = '{company}'
            AND date('now') BETWEEN date(a.start_date) AND date(a.end_date)
            AND a.status = 'active';
        
        User question: {question}
        
        Return ONLY the SQL query for SQLite, no explanations, no thinking, just the query:"""
        
        prompt = ChatPromptTemplate.from_template(template)
        
        chain = prompt | self.chat_model | StrOutputParser()
        
        try:
            response = chain.invoke({"question": user_question, "company": company})
            
            # Clean the response from thinking and explanations
            query = self.clean_sql_response(response)
            
            # Generate explanation separately
            explanation = self.generate_explanation(user_question, query, company)
            
            return query, explanation
        except Exception as e:
            return "", f"ÐžÑˆÐ¸Ð±ÐºÐ° Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ð¸ Ð·Ð°Ð¿Ñ€Ð¾ÑÐ°: {e}"

    def clean_sql_response(self, response: str) -> str:
        """Clean SQL response from thinking and explanations"""
        # Remove thinking blocks
        if "<think>" in response and "</think>" in response:
            start = response.find("</think>") + len("</think>")
            response = response[start:].strip()
        
        # Remove markdown code blocks
        if "```sql" in response:
            response = response.split("```sql")[1]
        if "```" in response:
            response = response.split("```")[0]
        
        # Remove common prefixes
        prefixes_to_remove = [
            "QUERY:", "SQL:", "SELECT", "WITH", "INSERT", "UPDATE", "DELETE"
        ]
        
        # Find the actual SQL query
        lines = response.split('\n')
        sql_lines = []
        in_sql = False
        
        for line in lines:
            line = line.strip()
            if not line:
                continue
                
            # Check if this line starts with SQL keywords
            if any(line.upper().startswith(prefix) for prefix in prefixes_to_remove):
                in_sql = True
                sql_lines.append(line)
            elif in_sql and (line.startswith(';') or line.endswith(';')):
                sql_lines.append(line)
                break
            elif in_sql:
                sql_lines.append(line)
        
        if sql_lines:
            return '\n'.join(sql_lines)
        
        # If no SQL found, return the original response cleaned
        return response.strip()

    def generate_explanation(self, question: str, query: str, company: str) -> str:
        """Generate explanation for the SQL query"""
        template = """ÐžÐ±ÑŠÑÑÐ½Ð¸ ÑÑ‚Ð¾Ñ‚ SQL Ð·Ð°Ð¿Ñ€Ð¾Ñ Ð½Ð° Ñ€ÑƒÑÑÐºÐ¾Ð¼ ÑÐ·Ñ‹ÐºÐµ:

Ð’ÐžÐŸÐ ÐžÐ¡: {question}
SQL Ð—ÐÐŸÐ ÐžÐ¡: {query}
ÐšÐžÐœÐŸÐÐÐ˜Ð¯: {company}

ÐžÐ±ÑŠÑÑÐ½ÐµÐ½Ð¸Ðµ Ð´Ð¾Ð»Ð¶Ð½Ð¾ Ð±Ñ‹Ñ‚ÑŒ ÐºÑ€Ð°Ñ‚ÐºÐ¸Ð¼ Ð¸ Ð¿Ð¾Ð½ÑÑ‚Ð½Ñ‹Ð¼, Ð¾Ð¿Ð¸ÑÑ‹Ð²Ð°Ñ‚ÑŒ Ñ‡Ñ‚Ð¾ Ð´ÐµÐ»Ð°ÐµÑ‚ Ð·Ð°Ð¿Ñ€Ð¾Ñ Ð¸ ÐºÐ°ÐºÐ¸Ðµ Ñ‚Ð°Ð±Ð»Ð¸Ñ†Ñ‹ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÑ‚."""
        
        prompt = ChatPromptTemplate.from_template(template)
        chain = prompt | self.chat_model | StrOutputParser()
        
        try:
            explanation = chain.invoke({
                "question": question,
                "query": query,
                "company": company
            })
            return explanation
        except Exception as e:
            return f"ÐžÐ±ÑŠÑÑÐ½ÐµÐ½Ð¸Ðµ Ð½ÐµÐ´Ð¾ÑÑ‚ÑƒÐ¿Ð½Ð¾: {e}"

    def _load_prompt(self, path: str, default: str) -> str:
        try:
            with open(path, 'r', encoding='utf-8') as f:
                return f.read()
        except Exception:
            return default

    def _filter_docs_by_role_and_scope(self, docs: List[Document], role: str) -> List[Document]:
        filtered: List[Document] = []
        for doc in docs:
            audience = doc.metadata.get('audience') if isinstance(doc.metadata, dict) else None
            scope = doc.metadata.get('scope') if isinstance(doc.metadata, dict) else None

            # role filter: if audience present, only include if role is allowed
            if audience:
                try:
                    allowed = set(audience)
                except Exception:
                    allowed = {str(audience)}
                if role == 'user' and 'user' not in allowed:
                    continue
                if role == 'admin' and 'admin' not in allowed and 'user' in allowed:
                    # admins can also read user docs
                    pass
            # else: no audience -> include for all

            # always include both scopes but mark legacy later
            filtered.append(doc)
        return filtered

    def _mark_legacy_in_context(self, docs: List[Document]) -> str:
        blocks: List[str] = []
        for doc in docs:
            is_legacy = False
            scope = doc.metadata.get('scope') if isinstance(doc.metadata, dict) else None
            if scope and ('legacy_billing' in scope if isinstance(scope, list) else scope == 'legacy_billing'):
                is_legacy = True
            prefix = "[LEGACY] " if is_legacy else ""
            blocks.append(prefix + doc.page_content)
        return "\n\n".join(blocks)

    def get_response(self, question: str, role: str = 'user') -> str:
        """Get response for a question using RAG with role-based filtering and legacy marking"""
        import time
        import logging
        
        logger = logging.getLogger(__name__)
        start_time = time.time()
        default_prompt = (
            "Ð¢Ñ‹ - ÑÐºÑÐ¿ÐµÑ€Ñ‚ Ð¿Ð¾ Ð»Ð¸Ñ‡Ð½Ð¾Ð¼Ñƒ ÐºÐ°Ð±Ð¸Ð½ÐµÑ‚Ñƒ ÑÐ¸ÑÑ‚ÐµÐ¼Ñ‹ ÑÐ¿ÑƒÑ‚Ð½Ð¸ÐºÐ¾Ð²Ð¾Ð¹ ÑÐ²ÑÐ·Ð¸ Ð¡Ð¢Ð­ÐšÐšÐžÐœ. Ð¢Ð²Ð¾Ñ Ð·Ð°Ð´Ð°Ñ‡Ð° - Ð¿Ð¾Ð¼Ð¾Ð³Ð°Ñ‚ÑŒ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»ÑÐ¼ Ñ€Ð°Ð·Ð¾Ð±Ñ€Ð°Ñ‚ÑŒÑÑ Ñ Ñ€Ð°Ð±Ð¾Ñ‚Ð¾Ð¹ ÑÐ¸ÑÑ‚ÐµÐ¼Ñ‹.\n\n"
            "ÐšÐžÐÐ¢Ð•ÐšÐ¡Ð¢:\n{context}\n\nÐ ÐžÐ›Ð¬ ÐŸÐžÐ›Ð¬Ð—ÐžÐ’ÐÐ¢Ð•Ð›Ð¯: {role}\n\nÐŸÐ ÐÐ’Ð˜Ð›Ð ÐžÐ¢Ð’Ð•Ð¢Ð:\n1. ÐžÑ‚Ð²ÐµÑ‡Ð°Ð¹ Ð¢ÐžÐ›Ð¬ÐšÐž Ð½Ð° Ñ€ÑƒÑÑÐºÐ¾Ð¼ ÑÐ·Ñ‹ÐºÐµ\n2. Ð‘ÑƒÐ´ÑŒ Ð¿Ð¾Ð´Ñ€Ð¾Ð±Ð½Ñ‹Ð¼, Ð½Ð¾ ÐºÑ€Ð°Ñ‚ÐºÐ¸Ð¼\n3. Ð•ÑÐ»Ð¸ Ð²Ð¾Ð¿Ñ€Ð¾Ñ Ð¾ Ñ‚Ð¾Ð¼, ÐºÐ°Ðº Ñ‡Ñ‚Ð¾-Ñ‚Ð¾ ÑÐ´ÐµÐ»Ð°Ñ‚ÑŒ - Ð´Ð°Ð¹ Ð¿Ð¾ÑˆÐ°Ð³Ð¾Ð²ÑƒÑŽ Ð¸Ð½ÑÑ‚Ñ€ÑƒÐºÑ†Ð¸ÑŽ\n"
            "4. Ð•ÑÐ»Ð¸ Ð²Ð¾Ð¿Ñ€Ð¾Ñ Ð¾ Ð²Ð¾Ð·Ð¼Ð¾Ð¶Ð½Ð¾ÑÑ‚ÑÑ… ÑÐ¸ÑÑ‚ÐµÐ¼Ñ‹ - Ð¿ÐµÑ€ÐµÑ‡Ð¸ÑÐ»Ð¸ Ð°ÐºÑ‚ÑƒÐ°Ð»ÑŒÐ½Ñ‹Ðµ Ñ„ÑƒÐ½ÐºÑ†Ð¸Ð¸ Ñ‚ÐµÐºÑƒÑ‰ÐµÐ³Ð¾ Ð»Ð¸Ñ‡Ð½Ð¾Ð³Ð¾ ÐºÐ°Ð±Ð¸Ð½ÐµÑ‚Ð°\n5. Ð•ÑÐ»Ð¸ Ð²Ð¾Ð¿Ñ€Ð¾Ñ Ð¾ Ð¿Ñ€Ð¸Ð¼ÐµÑ€Ð°Ñ… Ð·Ð°Ð¿Ñ€Ð¾ÑÐ¾Ð² - Ð¿Ñ€Ð¸Ð²ÐµÐ´Ð¸ ÐºÐ¾Ð½ÐºÑ€ÐµÑ‚Ð½Ñ‹Ðµ Ð¿Ñ€Ð¸Ð¼ÐµÑ€Ñ‹\n6. Ð•ÑÐ»Ð¸ Ð²Ð¾Ð¿Ñ€Ð¾Ñ Ð¾ Ð¿Ñ€Ð¾Ð±Ð»ÐµÐ¼Ð°Ñ… - Ð¿Ñ€ÐµÐ´Ð»Ð¾Ð¶Ð¸ Ñ€ÐµÑˆÐµÐ½Ð¸Ñ\n7. Ð•ÑÐ»Ð¸ Ð½Ðµ Ð·Ð½Ð°ÐµÑˆÑŒ Ð¾Ñ‚Ð²ÐµÑ‚Ð° - Ñ‡ÐµÑÑ‚Ð½Ð¾ ÑÐºÐ°Ð¶Ð¸ Ð¾Ð± ÑÑ‚Ð¾Ð¼\n"
            "8. Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐ¹ Ð¸Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸ÑŽ Ð¸Ð· ÐºÐ¾Ð½Ñ‚ÐµÐºÑÑ‚Ð°\n9. ÐÐµ Ð²Ñ‹Ð´ÑƒÐ¼Ñ‹Ð²Ð°Ð¹ Ð¸Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸ÑŽ\n10. ÐœÐ°Ñ‚ÐµÑ€Ð¸Ð°Ð»Ñ‹ Ð¿Ð¾ Ð½Ð°ÑÐ»ÐµÐ´Ð½Ð¾Ð¹/ÑÑ‚Ð°Ñ€Ð¾Ð¹ ÑÐ¸ÑÑ‚ÐµÐ¼Ðµ Ð¿Ð¾Ð¼ÐµÑ‡Ð°Ð¹ ÐºÐ°Ðº [LEGACY]. Ð•ÑÐ»Ð¸ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»ÑŒ ÑÐ²Ð½Ð¾ ÑÐ¿Ñ€Ð°ÑˆÐ¸Ð²Ð°ÐµÑ‚ Ð¿Ñ€Ð¾ ÑÑ‚Ð°Ñ€ÑƒÑŽ ÑÐ¸ÑÑ‚ÐµÐ¼Ñƒ/Ñ€ÐµÐ³Ð»Ð°Ð¼ÐµÐ½Ñ‚/Ð±Ð»Ð°Ð½Ðº Ð·Ð°ÐºÐ°Ð·Ð° â€” Ð¾Ñ‚Ð²ÐµÑ‡Ð°Ð¹ Ð½Ð° Ð¾ÑÐ½Ð¾Ð²Ðµ [LEGACY] Ð¼Ð°Ñ‚ÐµÑ€Ð¸Ð°Ð»Ð¾Ð² Ð¸ Ð¿Ñ€ÑÐ¼Ð¾ ÑƒÐºÐ°Ð¶Ð¸, Ñ‡Ñ‚Ð¾ Ð¾Ñ‚Ð²ÐµÑ‚ Ð¾Ñ‚Ð½Ð¾ÑÐ¸Ñ‚ÑÑ Ðº ÑÑ‚Ð°Ñ€Ð¾Ð¹ ÑÐ¸ÑÑ‚ÐµÐ¼Ðµ.\n\n"
            "Ð¡Ð¢Ð Ð£ÐšÐ¢Ð£Ð Ð ÐžÐ¢Ð’Ð•Ð¢Ð:\n- ÐšÑ€Ð°Ñ‚ÐºÐ¸Ð¹ Ð¾Ñ‚Ð²ÐµÑ‚ Ð½Ð° Ð²Ð¾Ð¿Ñ€Ð¾Ñ\n- ÐŸÐ¾ÑˆÐ°Ð³Ð¾Ð²Ñ‹Ðµ Ð¸Ð½ÑÑ‚Ñ€ÑƒÐºÑ†Ð¸Ð¸ (ÐµÑÐ»Ð¸ Ð¿Ñ€Ð¸Ð¼ÐµÐ½Ð¸Ð¼Ð¾)\n- ÐŸÑ€Ð¸Ð¼ÐµÑ€Ñ‹ (ÐµÑÐ»Ð¸ Ð¿Ñ€Ð¸Ð¼ÐµÐ½Ð¸Ð¼Ð¾)\n- Ð”Ð¾Ð¿Ð¾Ð»Ð½Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ñ‹Ðµ ÑÐ¾Ð²ÐµÑ‚Ñ‹ (ÐµÑÐ»Ð¸ Ð¿Ñ€Ð¸Ð¼ÐµÐ½Ð¸Ð¼Ð¾)\n\nÐ’ÐžÐŸÐ ÐžÐ¡ ÐŸÐžÐ›Ð¬Ð—ÐžÐ’ÐÐ¢Ð•Ð›Ð¯: {question}\n\nÐžÐ¢Ð’Ð•Ð¢:"
        )
        template = self._load_prompt("resources/prompts/assistant_prompt.txt", default_prompt)
        prompt = ChatPromptTemplate.from_template(template)

        try:
            # Detect legacy intent early to tune retrieval
            legacy_intent = False
            try:
                ql = question.lower()
                legacy_intent = any(k in ql for k in [
                    'ÑÑ‚Ð°Ñ€Ð¾Ð¹ ÑÐ¸ÑÑ‚ÐµÐ¼Ðµ', 'ÑÑ‚Ð°Ñ€Ð°Ñ ÑÐ¸ÑÑ‚ÐµÐ¼Ð°', 'legacy', 'Ñ€ÐµÐ³Ð»Ð°Ð¼ÐµÐ½Ñ‚', 'Ð±Ð»Ð°Ð½Ðº Ð·Ð°ÐºÐ°Ð·Ð°',
                    'ÑÑ‚Ð°Ñ€Ñ‹Ð¹ Ñ€ÐµÐ³Ð»Ð°Ð¼ÐµÐ½Ñ‚', 'ÑƒÑÑ‚Ð°Ñ€ÐµÐ²Ñˆ'
                ])
            except Exception:
                legacy_intent = False

            # Retrieve and filter documents by role (increase k for legacy intent)
            base_k = 12 if not legacy_intent else 20
            retriever = self.vectorstore.as_retriever(search_kwargs={"k": base_k})
            relevant_docs = retriever.get_relevant_documents(question)
            # If legacy intent and no legacy docs surfaced, do a targeted second pass
            if legacy_intent and not any(
                ('legacy_billing' in (d.metadata.get('scope') or [])) if isinstance(d.metadata.get('scope'), list)
                else (d.metadata.get('scope') == 'legacy_billing') for d in relevant_docs if isinstance(d.metadata, dict)
            ):
                probe_query = question + " legacy Ñ€ÐµÐ³Ð»Ð°Ð¼ÐµÐ½Ñ‚ Ð±Ð»Ð°Ð½Ðº Ð·Ð°ÐºÐ°Ð·Ð° common services"
                try:
                    extra = self.vectorstore.similarity_search(probe_query, k=10)
                    # merge without duplicates
                    seen_ids = set(id(doc) for doc in relevant_docs)
                    for doc in extra:
                        if id(doc) not in seen_ids:
                            relevant_docs.append(doc)
                            seen_ids.add(id(doc))
                except Exception:
                    pass

            # Ensure inclusion of specific legacy common services reg if user asks about Ð±Ð»Ð°Ð½Ðº Ð·Ð°ÐºÐ°Ð·Ð°/Ñ€ÐµÐ³Ð»Ð°Ð¼ÐµÐ½Ñ‚
            if legacy_intent:
                try:
                    seen_ids = set(id(doc) for doc in relevant_docs)
                    for doc in self.documents:
                        meta = doc.metadata if isinstance(doc.metadata, dict) else {}
                        kb_file = (meta.get('kb_file') or '').lower()
                        title = (meta.get('title') or '').lower()
                        if 'legacy_reglament_commonservices' in kb_file or 'Ð¾Ð±Ñ‰Ð¸Ðµ ÑƒÑÐ»ÑƒÐ³Ð¸' in title or 'common services' in title:
                            if id(doc) not in seen_ids:
                                relevant_docs.append(doc)
                                seen_ids.add(id(doc))
                except Exception:
                    pass
            relevant_docs = self._filter_docs_by_role_and_scope(relevant_docs, role)

            # If Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»ÑŒ ÑÐ²Ð½Ð¾ ÑÐ¿Ñ€Ð°ÑˆÐ¸Ð²Ð°ÐµÑ‚ Ð¿Ñ€Ð¾ "Ð±Ð»Ð°Ð½Ðº Ð·Ð°ÐºÐ°Ð·Ð°" â€” Ð¿Ñ€Ð¸Ð½ÑƒÐ´Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð¾ Ð´Ð¾Ð±Ð°Ð²Ð¸Ð¼ Ñ€ÐµÐ»ÐµÐ²Ð°Ð½Ñ‚Ð½Ñ‹Ðµ ÐºÑƒÑÐºÐ¸
            try:
                if legacy_intent and ('Ð±Ð»Ð°Ð½Ðº Ð·Ð°ÐºÐ°Ð·Ð°' in ql):
                    extra_snippets: List[Document] = []
                    for doc in self.documents:
                        meta = doc.metadata if isinstance(doc.metadata, dict) else {}
                        kb_file = (meta.get('kb_file') or '').lower()
                        if 'legacy_reglament_commonservices' in kb_file and 'Ð±Ð»Ð°Ð½Ðº Ð·Ð°ÐºÐ°Ð·Ð°' in doc.page_content.lower():
                            extra_snippets.append(doc)
                    if extra_snippets:
                        # ÐŸÑ€ÐµÐ¿ÐµÐ½Ð´Ð¸Ð¼, Ñ‡Ñ‚Ð¾Ð±Ñ‹ Ð¾Ð½Ð¸ Ð±Ñ‹Ð»Ð¸ Ð² ÐºÐ¾Ð½Ñ‚ÐµÐºÑÑ‚Ðµ Ð¿ÐµÑ€Ð²Ñ‹Ð¼Ð¸
                        relevant_docs = extra_snippets + relevant_docs
            except Exception:
                pass

            # Format documents with legacy marking
            context = self._mark_legacy_in_context(relevant_docs)

            if legacy_intent:
                # Augment instructions inline
                template = template + "\n\nÐ”ÐžÐŸÐžÐ›ÐÐ•ÐÐ˜Ð•: ÐŸÐ¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»ÑŒ ÑÐ¿Ñ€Ð°ÑˆÐ¸Ð²Ð°ÐµÑ‚ Ð¿Ñ€Ð¾ ÑÑ‚Ð°Ñ€ÑƒÑŽ/Ð½Ð°ÑÐ»ÐµÐ´Ð½ÑƒÑŽ ÑÐ¸ÑÑ‚ÐµÐ¼Ñƒ. Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐ¹ [LEGACY] Ð¼Ð°Ñ‚ÐµÑ€Ð¸Ð°Ð»Ñ‹ ÐºÐ°Ðº Ð¾ÑÐ½Ð¾Ð²Ð½Ð¾Ð¹ Ð¸ÑÑ‚Ð¾Ñ‡Ð½Ð¸Ðº Ð¾Ñ‚Ð²ÐµÑ‚Ð° Ð¸ ÑÐ²Ð½Ð¾ Ð¿Ð¾Ð¼ÐµÑ‚ÑŒ Ð¿Ñ€Ð¸Ð¼ÐµÐ½Ð¸Ð¼Ð¾ÑÑ‚ÑŒ."
                prompt = ChatPromptTemplate.from_template(template)

            chain = prompt | self.chat_model | StrOutputParser()
            
            # Add timeout protection for LLM call using threading
            import threading
            import queue
            
            result_queue = queue.Queue()
            exception_queue = queue.Queue()
            
            def run_chain():
                try:
                    response = chain.invoke({
                        "context": context,
                        "question": question,
                        "role": role
                    })
                    result_queue.put(response)
                except Exception as e:
                    exception_queue.put(e)
            
            # Start the chain in a separate thread
            thread = threading.Thread(target=run_chain)
            thread.daemon = True
            thread.start()
            
            try:
                # Wait for result with 30 second timeout
                response = result_queue.get(timeout=30)
                
                elapsed = time.time() - start_time
                if elapsed > 10:
                    logger.warning(f"Slow RAG request: {elapsed:.1f}s for question: {question[:50]}...")
                
            except queue.Empty:
                elapsed = time.time() - start_time
                logger.error(f"RAG request timeout after {elapsed:.1f}s for question: {question[:50]}...")
                return self._get_fallback_response(question, role)
            except Exception as e:
                # Check if there's an exception from the thread
                try:
                    thread_exception = exception_queue.get_nowait()
                    e = thread_exception
                except queue.Empty:
                    pass
                
                elapsed = time.time() - start_time
                logger.error(f"RAG request failed after {elapsed:.1f}s: {e}")
                return self._get_fallback_response(question, role)
            
            # Build citations from filtered documents
            citations: List[str] = []
            seen = set()
            for doc in relevant_docs:
                meta = doc.metadata if isinstance(doc.metadata, dict) else {}
                key = (meta.get('kb_file'), meta.get('title'), meta.get('source'))
                if key in seen:
                    continue
                seen.add(key)
                is_legacy = False
                scope = meta.get('scope')
                if scope and ('legacy_billing' in scope if isinstance(scope, list) else scope == 'legacy_billing'):
                    is_legacy = True
                prefix = "[LEGACY] " if is_legacy else ""
                title = meta.get('title') or "KB Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚"
                kb_file = meta.get('kb_file') or meta.get('source') or ""
                citations.append(f"- {prefix}{title} ({kb_file})")
                if len(citations) >= 5:
                    break

            if citations:
                response += "\n\n---\n**Ð˜ÑÑ‚Ð¾Ñ‡Ð½Ð¸ÐºÐ¸:**\n" + "\n".join(citations)
            return response
        except Exception as e:
            logger.error(f"RAG system error: {e}")
            return self._get_fallback_response(question, role)
    
    def _get_fallback_response(self, question: str, role: str) -> str:
        """Fallback response when RAG system fails or times out"""
        return f"""Ð˜Ð·Ð²Ð¸Ð½Ð¸Ñ‚Ðµ, ÑÐ¸ÑÑ‚ÐµÐ¼Ð° Ð²Ñ€ÐµÐ¼ÐµÐ½Ð½Ð¾ Ð½ÐµÐ´Ð¾ÑÑ‚ÑƒÐ¿Ð½Ð°. 

**ÐšÑ€Ð°Ñ‚ÐºÐ¸Ð¹ Ð¾Ñ‚Ð²ÐµÑ‚:**
Ð”Ð»Ñ Ð¿Ð¾Ð»ÑƒÑ‡ÐµÐ½Ð¸Ñ Ð¿Ð¾Ð¼Ð¾Ñ‰Ð¸ Ð¿Ð¾ Ð»Ð¸Ñ‡Ð½Ð¾Ð¼Ñƒ ÐºÐ°Ð±Ð¸Ð½ÐµÑ‚Ñƒ Ð¡Ð¢Ð­ÐšÐšÐžÐœ Ð¾Ð±Ñ€Ð°Ñ‚Ð¸Ñ‚ÐµÑÑŒ Ð² Ñ‚ÐµÑ…Ð½Ð¸Ñ‡ÐµÑÐºÑƒÑŽ Ð¿Ð¾Ð´Ð´ÐµÑ€Ð¶ÐºÑƒ.

**ÐšÐ¾Ð½Ñ‚Ð°ÐºÑ‚Ñ‹:**
- Ð¢ÐµÐ»ÐµÑ„Ð¾Ð½: +7 (495) 363-91-41
- Email: noc@steccom.ru
- Ð’Ñ€ÐµÐ¼Ñ Ñ€Ð°Ð±Ð¾Ñ‚Ñ‹: 24/7

**ÐžÑÐ½Ð¾Ð²Ð½Ñ‹Ðµ Ñ„ÑƒÐ½ÐºÑ†Ð¸Ð¸ Ð›Ðš:**
- Ð¡Ñ‚Ð°Ð½Ð´Ð°Ñ€Ñ‚Ð½Ñ‹Ðµ Ð¾Ñ‚Ñ‡ÐµÑ‚Ñ‹ Ð¿Ð¾ Ñ‚Ñ€Ð°Ñ„Ð¸ÐºÑƒ Ð¸ ÑƒÑÑ‚Ñ€Ð¾Ð¹ÑÑ‚Ð²Ð°Ð¼
- ÐŸÐ¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»ÑŒÑÐºÐ¸Ðµ SQL-Ð·Ð°Ð¿Ñ€Ð¾ÑÑ‹
- ÐŸÑ€Ð¾ÑÐ¼Ð¾Ñ‚Ñ€ Ð´Ð¾Ð³Ð¾Ð²Ð¾Ñ€Ð¾Ð² Ð¸ Ñ‚Ð°Ñ€Ð¸Ñ„Ð¾Ð²

ÐŸÐ¾Ð¿Ñ€Ð¾Ð±ÑƒÐ¹Ñ‚Ðµ Ð¿ÐµÑ€ÐµÑ„Ð¾Ñ€Ð¼ÑƒÐ»Ð¸Ñ€Ð¾Ð²Ð°Ñ‚ÑŒ Ð²Ð¾Ð¿Ñ€Ð¾Ñ Ð¸Ð»Ð¸ Ð¾Ð±Ñ€Ð°Ñ‚Ð¸Ñ‚ÐµÑÑŒ Ðº Ð°Ð´Ð¼Ð¸Ð½Ð¸ÑÑ‚Ñ€Ð°Ñ‚Ð¾Ñ€Ñƒ."""

    def search_similar(self, query: str, k: int = 3) -> List[str]:
        """Search for similar documents"""
        if not self.vectorstore:
            return ["Vectorstore not initialized"]
            
        docs = self.vectorstore.similarity_search(query, k=k)
        return [doc.page_content for doc in docs] 