#!/usr/bin/env python3
"""
Recreate Vectorstores - Create new FAISS vectorstores from database documents
–ü–µ—Ä–µ—Å–æ–∑–¥–∞–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–Ω—ã—Ö —Ö—Ä–∞–Ω–∏–ª–∏—â - —Å–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤—ã—Ö FAISS –≤–µ–∫—Ç–æ—Ä–Ω—ã—Ö —Ö—Ä–∞–Ω–∏–ª–∏—â –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ë–î
"""

import sys
from pathlib import Path
import sqlite3
import json

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ –º–æ–¥—É–ª—è–º
sys.path.append(str(Path(__file__).parent / "kb_admin"))

def recreate_vectorstores():
    """–ü–µ—Ä–µ—Å–æ–∑–¥–∞–µ–º –≤–µ–∫—Ç–æ—Ä–Ω—ã–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞ –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ –ë–î"""
    
    try:
        from langchain_community.vectorstores import FAISS
        from langchain.schema import Document
        from langchain_huggingface import HuggingFaceEmbeddings
        
        print("üîÑ –ü–µ—Ä–µ—Å–æ–∑–¥–∞–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–Ω—ã—Ö —Ö—Ä–∞–Ω–∏–ª–∏—â...")
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º embeddings
        embedding_model = HuggingFaceEmbeddings(
            model_name="intfloat/multilingual-e5-base",
            model_kwargs={'device': 'cpu'},
            encode_kwargs={'normalize_embeddings': True}
        )
        
        # –ü–æ–¥–∫–ª—é—á–∞–µ–º—Å—è –∫ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö
        db_path = "kb_admin/kbs.db"
        conn = sqlite3.connect(db_path)
        c = conn.cursor()
        
        # –ü–æ–ª—É—á–∞–µ–º –∞–∫—Ç–∏–≤–Ω—ã–µ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π
        c.execute("SELECT id, name, is_active FROM knowledge_bases WHERE is_active = 1")
        active_kbs = c.fetchall()
        
        print(f"üìö –ê–∫—Ç–∏–≤–Ω—ã–µ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π: {len(active_kbs)}")
        
        created_count = 0
        
        for kb_id, name, is_active in active_kbs:
            print(f"\nüîÑ –û–±—Ä–∞–±–æ—Ç–∫–∞ KB {kb_id}: {name}")
            
            # –ü–æ–ª—É—á–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã –¥–ª—è —ç—Ç–æ–π –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π
            c.execute("""
                SELECT dc.content, dc.metadata, kd.title, kd.file_path
                FROM document_chunks dc
                JOIN knowledge_documents kd ON dc.doc_id = kd.id
                WHERE kd.kb_id = ? AND kd.processed = 1
                ORDER BY dc.chunk_index
            """, (kb_id,))
            
            chunks = c.fetchall()
            
            if not chunks:
                print(f"   ‚ö†Ô∏è –ù–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è KB {kb_id}")
                continue
            
            print(f"   üìÑ –ù–∞–π–¥–µ–Ω–æ {len(chunks)} —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
            
            # –°–æ–∑–¥–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã LangChain
            documents = []
            for i, (content, metadata_json, title, file_path) in enumerate(chunks):
                try:
                    metadata = json.loads(metadata_json) if metadata_json else {}
                except:
                    metadata = {}
                
                # –î–æ–±–∞–≤–ª—è–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –≤ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
                metadata.update({
                    'kb_id': kb_id,
                    'kb_name': name,
                    'title': title,
                    'file_path': file_path,
                    'chunk_index': i
                })
                
                doc = Document(page_content=content, metadata=metadata)
                documents.append(doc)
            
            if documents:
                # –°–æ–∑–¥–∞–µ–º –≤–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ
                vectorstore = FAISS.from_documents(documents, embedding_model)
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ
                vectorstore_path = Path(f"kb_admin/data/knowledge_bases/vectorstore_{kb_id}")
                vectorstore_path.mkdir(parents=True, exist_ok=True)
                
                # –£–¥–∞–ª—è–µ–º —Å—Ç–∞—Ä—ã–µ —Ñ–∞–π–ª—ã
                for old_file in vectorstore_path.glob("*"):
                    old_file.unlink()
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –Ω–æ–≤—ã–µ —Ñ–∞–π–ª—ã
                vectorstore.save_local(str(vectorstore_path))
                
                print(f"   ‚úÖ –°–æ–∑–¥–∞–Ω–æ –≤–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ: {len(documents)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
                created_count += 1
            else:
                print(f"   ‚ö†Ô∏è –ù–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞")
        
        conn.close()
        
        print(f"\nüìä –†–µ–∑—É–ª—å—Ç–∞—Ç:")
        print(f"   - –°–æ–∑–¥–∞–Ω–æ –≤–µ–∫—Ç–æ—Ä–Ω—ã—Ö —Ö—Ä–∞–Ω–∏–ª–∏—â: {created_count}")
        
        if created_count > 0:
            print("‚úÖ –í–µ–∫—Ç–æ—Ä–Ω—ã–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞ –ø–µ—Ä–µ—Å–æ–∑–¥–∞–Ω—ã!")
            return True
        else:
            print("‚ùå –í–µ–∫—Ç–æ—Ä–Ω—ã–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞ –Ω–µ —Å–æ–∑–¥–∞–Ω—ã!")
            return False
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–µ—Ä–µ—Å–æ–∑–¥–∞–Ω–∏–∏ –≤–µ–∫—Ç–æ—Ä–Ω—ã—Ö —Ö—Ä–∞–Ω–∏–ª–∏—â: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    recreate_vectorstores()
