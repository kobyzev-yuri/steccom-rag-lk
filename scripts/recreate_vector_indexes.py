#!/usr/bin/env python3
"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è –ø–µ—Ä–µ—Å–æ–∑–¥–∞–Ω–∏—è –≤–µ–∫—Ç–æ—Ä–Ω—ã—Ö –∏–Ω–¥–µ–∫—Å–æ–≤ —Å –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–º–∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏
–¥–ª—è –∫–æ—Ä–æ—Ç–∫–∏—Ö –Ω–µ–∑–∞–≤–∏—Å–∏–º—ã—Ö –∞–±–∑–∞—Ü–µ–≤ –≤ —Ä–µ–≥–ª–∞–º–µ–Ω—Ç–∞—Ö
"""

import os
import sys
import json
import sqlite3
import shutil
from pathlib import Path
from typing import List, Dict
import re

# Add project root to path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from modules.rag.multi_kb_rag import MultiKBRAG
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.schema import Document


def analyze_text_structure(text: str) -> Dict:
    """–ê–Ω–∞–ª–∏–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã —Ç–µ–∫—Å—Ç–∞ –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã—Ö –Ω–∞—Å—Ç—Ä–æ–µ–∫"""
    analysis = {
        'total_length': len(text),
        'paragraphs_by_double_newline': len([p for p in text.split('\n\n') if p.strip()]),
        'lines_by_single_newline': len([l for l in text.split('\n') if l.strip()]),
        'sections_by_numbers': len(re.split(r'\n\s*(\d+\.\s)', text)),
        'avg_section_length': 0,
        'section_lengths': []
    }
    
    # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ä–∞–∑–¥–µ–ª—ã –ø–æ –Ω–æ–º–µ—Ä–∞–º
    sections = re.split(r'\n\s*(\d+\.\s)', text)
    section_lengths = []
    
    # –ë–µ—Ä–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —Ä–∞–∑–¥–µ–ª–æ–≤ (–∫–∞–∂–¥—ã–π –≤—Ç–æ—Ä–æ–π —ç–ª–µ–º–µ–Ω—Ç –ø–æ—Å–ª–µ —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—è)
    for i in range(1, len(sections), 2):
        if i < len(sections):
            section_content = sections[i]
            if section_content.strip():
                section_lengths.append(len(section_content.strip()))
    
    # –ï—Å–ª–∏ —Ä–∞–∑–¥–µ–ª—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã, –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –ø–æ –∞–±–∑–∞—Ü–∞–º
    if not section_lengths:
        paragraphs = [p.strip() for p in text.split('\n\n') if p.strip()]
        section_lengths = [len(p) for p in paragraphs if len(p) > 50]  # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º –æ—á–µ–Ω—å –∫–æ—Ä–æ—Ç–∫–∏–µ –∞–±–∑–∞—Ü—ã
    
    analysis['section_lengths'] = section_lengths
    analysis['avg_section_length'] = sum(section_lengths) / len(section_lengths) if section_lengths else 0
    
    return analysis


def create_optimal_text_splitter(text_analysis: Dict) -> RecursiveCharacterTextSplitter:
    """–°–æ–∑–¥–∞–Ω–∏–µ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ text splitter –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞–Ω–∞–ª–∏–∑–∞ —Ç–µ–∫—Å—Ç–∞"""
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞
    avg_section_length = text_analysis['avg_section_length']
    
    if avg_section_length < 500:
        chunk_size = 500
        chunk_overlap = 50
    elif avg_section_length < 1000:
        chunk_size = 800
        chunk_overlap = 100
    else:
        chunk_size = 1200
        chunk_overlap = 150
    
    print(f"üìä –ê–Ω–∞–ª–∏–∑ —Ç–µ–∫—Å—Ç–∞:")
    print(f"   –°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ —Ä–∞–∑–¥–µ–ª–∞: {avg_section_length:.0f} —Å–∏–º–≤–æ–ª–æ–≤")
    print(f"   –í—ã–±—Ä–∞–Ω–Ω—ã–π —Ä–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞: {chunk_size}")
    print(f"   –ü–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ: {chunk_overlap}")
    
    # –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–∏ –¥–ª—è —Ä–µ–≥–ª–∞–º–µ–Ω—Ç–æ–≤
    separators = [
        "\n\n",  # –î–≤–æ–π–Ω—ã–µ –ø–µ—Ä–µ–Ω–æ—Å—ã —Å—Ç—Ä–æ–∫ (–∞–±–∑–∞—Ü—ã)
        "\n",    # –û–¥–∏–Ω–∞—Ä–Ω—ã–µ –ø–µ—Ä–µ–Ω–æ—Å—ã —Å—Ç—Ä–æ–∫
        ". ",    # –¢–æ—á–∫–∏ —Å –ø—Ä–æ–±–µ–ª–æ–º
        "! ",    # –í–æ—Å–∫–ª–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–µ –∑–Ω–∞–∫–∏
        "? ",    # –í–æ–ø—Ä–æ—Å–∏—Ç–µ–ª—å–Ω—ã–µ –∑–Ω–∞–∫–∏
        "; ",    # –¢–æ—á–∫–∏ —Å –∑–∞–ø—è—Ç–æ–π
        ", ",    # –ó–∞–ø—è—Ç—ã–µ
        " ",     # –ü—Ä–æ–±–µ–ª—ã
        ""       # –ü–æ—Å–ª–µ–¥–Ω–∏–π —Ä–µ–∑–æ—Ä—Ç
    ]
    
    return RecursiveCharacterTextSplitter(
        chunk_size=chunk_size,
        chunk_overlap=chunk_overlap,
        length_function=len,
        separators=separators
    )


def recreate_kb_indexes(kb_ids: List[int] = None):
    """–ü–µ—Ä–µ—Å–æ–∑–¥–∞–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–Ω—ã—Ö –∏–Ω–¥–µ–∫—Å–æ–≤ –¥–ª—è —É–∫–∞–∑–∞–Ω–Ω—ã—Ö KB"""
    
    print("üöÄ –ü–µ—Ä–µ—Å–æ–∑–¥–∞–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–Ω—ã—Ö –∏–Ω–¥–µ–∫—Å–æ–≤ Knowledge Base")
    print("=" * 60)
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º RAG —Å–∏—Å—Ç–µ–º—É
    rag = MultiKBRAG()
    
    # –ï—Å–ª–∏ –Ω–µ —É–∫–∞–∑–∞–Ω—ã KB, –±–µ—Ä–µ–º –≤—Å–µ –∞–∫—Ç–∏–≤–Ω—ã–µ
    if kb_ids is None:
        available_kbs = rag.get_available_kbs()
        kb_ids = [kb.get('id', i) for i, kb in enumerate(available_kbs, 1)]
    
    print(f"üìö –ë—É–¥–µ—Ç –ø–µ—Ä–µ—Å–æ–∑–¥–∞–Ω–æ –∏–Ω–¥–µ–∫—Å–æ–≤: {len(kb_ids)}")
    
    for kb_id in kb_ids:
        print(f"\nüîÑ –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º KB ID: {kb_id}")
        
        try:
            # –£–¥–∞–ª—è–µ–º —Å—Ç–∞—Ä—ã–π –≤–µ–∫—Ç–æ—Ä–Ω—ã–π –∏–Ω–¥–µ–∫—Å
            vectorstore_path = f"data/knowledge_bases/vectorstore_{kb_id}"
            if Path(vectorstore_path).exists():
                shutil.rmtree(vectorstore_path)
                print(f"   üóëÔ∏è  –£–¥–∞–ª–µ–Ω —Å—Ç–∞—Ä—ã–π –∏–Ω–¥–µ–∫—Å: {vectorstore_path}")
            
            # –ü–æ–ª—É—á–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ KB –∏–∑ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö
            conn = sqlite3.connect(rag.db_path)
            c = conn.cursor()
            
            c.execute("SELECT * FROM knowledge_bases WHERE id = ? AND is_active = 1", (kb_id,))
            kb_info = c.fetchone()
            
            if not kb_info:
                print(f"   ‚ùå KB {kb_id} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –∏–ª–∏ –Ω–µ–∞–∫—Ç–∏–≤–Ω–∞")
                conn.close()
                continue
            
            print(f"   üìñ KB: {kb_info[1]} ({kb_info[3]})")
            
            # –ü–æ–ª—É—á–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã –¥–ª—è —ç—Ç–æ–π KB
            c.execute("SELECT * FROM knowledge_documents WHERE kb_id = ?", (kb_id,))
            documents = c.fetchall()
            
            if not documents:
                print(f"   ‚ö†Ô∏è  –ù–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ KB {kb_id}")
                conn.close()
                continue
            
            print(f"   üìÑ –î–æ–∫—É–º–µ–Ω—Ç–æ–≤: {len(documents)}")
            
            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É —Ç–µ–∫—Å—Ç–∞ –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã—Ö –Ω–∞—Å—Ç—Ä–æ–µ–∫
            all_text = ""
            for doc in documents:
                # –ö–æ–Ω—Ç–µ–Ω—Ç –º–æ–∂–µ—Ç –±—ã—Ç—å –≤ metadata –∏–ª–∏ –≤ file_path
                if doc[9]:  # metadata
                    try:
                        metadata = json.loads(doc[9])
                        if 'content' in metadata:
                            all_text += metadata['content'] + "\n\n"
                    except:
                        pass
                elif doc[3]:  # file_path - —á–∏—Ç–∞–µ–º —Ñ–∞–π–ª
                    try:
                        with open(doc[3], 'r', encoding='utf-8') as f:
                            file_content = f.read()
                            all_text += file_content + "\n\n"
                    except:
                        pass
            
            if all_text.strip():
                text_analysis = analyze_text_structure(all_text)
                text_splitter = create_optimal_text_splitter(text_analysis)
            else:
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
                text_splitter = RecursiveCharacterTextSplitter(
                    chunk_size=600,  # –£–º–µ–Ω—å—à–µ–Ω–Ω—ã–π —Ä–∞–∑–º–µ—Ä –¥–ª—è –∫–æ—Ä–æ—Ç–∫–∏—Ö –∞–±–∑–∞—Ü–µ–≤
                    chunk_overlap=100,
                    length_function=len,
                    separators=["\n\n", "\n", ". ", "! ", "? ", ", ", " ", ""]
                )
                print(f"   üìä –ò—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: chunk_size=600, overlap=100")
            
            # –°–æ–∑–¥–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
            documents_list = []
            for doc in documents:
                content = ""
                
                # –ü–æ–ª—É—á–∞–µ–º –∫–æ–Ω—Ç–µ–Ω—Ç –∏–∑ metadata –∏–ª–∏ —Ñ–∞–π–ª–∞
                if doc[9]:  # metadata
                    try:
                        metadata = json.loads(doc[9])
                        if 'content' in metadata:
                            content = metadata['content']
                    except:
                        pass
                
                if not content and doc[3]:  # file_path - —á–∏—Ç–∞–µ–º —Ñ–∞–π–ª
                    try:
                        with open(doc[3], 'r', encoding='utf-8') as f:
                            content = f.read()
                    except:
                        pass
                
                if content:
                    doc_metadata = {
                        'kb_id': kb_id,
                        'doc_id': doc[0],
                        'title': doc[1] or '–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è',
                        'source': 'db',
                        'category': kb_info[3]
                    }
                    
                    documents_list.append(Document(
                        page_content=content,
                        metadata=doc_metadata
                    ))
            
            if not documents_list:
                print(f"   ‚ùå –ù–µ—Ç –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤ KB {kb_id}")
                conn.close()
                continue
            
            # –†–∞–∑–±–∏–≤–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã –Ω–∞ —á–∞–Ω–∫–∏
            print(f"   ‚úÇÔ∏è  –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —á–∞–Ω–∫–∏...")
            chunks = text_splitter.split_documents(documents_list)
            print(f"   üì¶ –°–æ–∑–¥–∞–Ω–æ —á–∞–Ω–∫–æ–≤: {len(chunks)}")
            
            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä–∏–º–µ—Ä—ã —á–∞–Ω–∫–æ–≤
            for i, chunk in enumerate(chunks[:3]):
                print(f"      –ß–∞–Ω–∫ {i+1}: {len(chunk.page_content)} —Å–∏–º–≤–æ–ª–æ–≤ - {chunk.page_content[:100]}...")
            
            # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π –≤–µ–∫—Ç–æ—Ä–Ω—ã–π –∏–Ω–¥–µ–∫—Å
            print(f"   üîç –°–æ–∑–¥–∞–µ–º –≤–µ–∫—Ç–æ—Ä–Ω—ã–π –∏–Ω–¥–µ–∫—Å...")
            from langchain_community.vectorstores import FAISS
            vectorstore = FAISS.from_documents(chunks, rag.embeddings)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–Ω–¥–µ–∫—Å –Ω–∞ –¥–∏—Å–∫
            vectorstore.save_local(vectorstore_path)
            print(f"   üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω –Ω–æ–≤—ã–π –∏–Ω–¥–µ–∫—Å: {vectorstore_path}")
            
            # –û–±–Ω–æ–≤–ª—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –≤ –ø–∞–º—è—Ç–∏
            rag.vectorstores[kb_id] = vectorstore
            rag.kb_metadata[kb_id] = {
                'name': kb_info[1],
                'description': kb_info[2],
                'category': kb_info[3],
                'doc_count': len(documents),
                'chunk_count': len(chunks)
            }
            
            print(f"   ‚úÖ KB {kb_id} —É—Å–ø–µ—à–Ω–æ –ø–µ—Ä–µ—Å–æ–∑–¥–∞–Ω–∞")
            
            conn.close()
            
        except Exception as e:
            print(f"   ‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ KB {kb_id}: {e}")
            import traceback
            traceback.print_exc()
    
    print(f"\nüéâ –ü–µ—Ä–µ—Å–æ–∑–¥–∞–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–æ–≤ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
    print(f"üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
    for kb_id in kb_ids:
        if kb_id in rag.kb_metadata:
            metadata = rag.kb_metadata[kb_id]
            print(f"   ‚Ä¢ KB {kb_id} ({metadata['name']}): {metadata['chunk_count']} —á–∞–Ω–∫–æ–≤")


def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    import argparse
    
    parser = argparse.ArgumentParser(description='–ü–µ—Ä–µ—Å–æ–∑–¥–∞–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–Ω—ã—Ö –∏–Ω–¥–µ–∫—Å–æ–≤ KB')
    parser.add_argument('--kb-ids', nargs='+', type=int, help='ID KB –¥–ª—è –ø–µ—Ä–µ—Å–æ–∑–¥–∞–Ω–∏—è (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –≤—Å–µ)')
    parser.add_argument('--analyze-only', action='store_true', help='–¢–æ–ª—å–∫–æ –∞–Ω–∞–ª–∏–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã —Ç–µ–∫—Å—Ç–∞')
    
    args = parser.parse_args()
    
    if args.analyze_only:
        # –¢–æ–ª—å–∫–æ –∞–Ω–∞–ª–∏–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã
        print("üìä –ê–Ω–∞–ª–∏–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã —Ç–µ–∫—Å—Ç–∞ –≤ KB...")
        
        rag = MultiKBRAG()
        available_kbs = rag.get_available_kbs()
        
        for kb in available_kbs:
            kb_id = kb.get('id', 1)
            print(f"\nüìñ KB: {kb['name']} (ID: {kb_id})")
            
            # –ü–æ–ª—É—á–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã
            conn = sqlite3.connect(rag.db_path)
            c = conn.cursor()
            c.execute("SELECT metadata, file_path FROM knowledge_documents WHERE kb_id = ?", (kb_id,))
            documents = c.fetchall()
            conn.close()
            
            all_text = ""
            for doc in documents:
                content = ""
                if doc[0]:  # metadata
                    try:
                        metadata = json.loads(doc[0])
                        if 'content' in metadata:
                            content = metadata['content']
                    except:
                        pass
                
                if not content and doc[1]:  # file_path
                    try:
                        with open(doc[1], 'r', encoding='utf-8') as f:
                            content = f.read()
                    except:
                        pass
                
                if content:
                    all_text += content + "\n\n"
            if all_text:
                analysis = analyze_text_structure(all_text)
                print(f"   –û–±—â–∞—è –¥–ª–∏–Ω–∞: {analysis['total_length']} —Å–∏–º–≤–æ–ª–æ–≤")
                print(f"   –†–∞–∑–¥–µ–ª–æ–≤: {analysis['sections_by_numbers']}")
                print(f"   –°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ —Ä–∞–∑–¥–µ–ª–∞: {analysis['avg_section_length']:.0f} —Å–∏–º–≤–æ–ª–æ–≤")
                print(f"   –†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–π —Ä–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞: {min(800, max(400, int(analysis['avg_section_length'] * 0.8)))}")
    else:
        # –ü–µ—Ä–µ—Å–æ–∑–¥–∞–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–æ–≤
        recreate_kb_indexes(args.kb_ids)


if __name__ == "__main__":
    main()
